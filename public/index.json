[
{
	"uri": "http://example.org/data-modeling/4-1_data-modeling-basics/",
	"title": "Data Modeling Basics",
	"tags": [],
	"description": "",
	"content": "GridDB is a unique Key-Container data model that resembles Key-Value. It has the following features.\n A concept resembling a RDB table that is a container for grouping Key-Value. A schema to define the data type for the container can be set. An index can be set in a column. Transactions can be carried out on a row basis within the container. In addition, ACID is guaranteed on a container basis.  GridDB manages data on a block, container, partition, and partition group basis.\n Block\nA block is a data unit for data persistence processing in a disk (hereinafter known as a checkpoint) and is the smallest physical data management unit in GridDB. Multiple container data are arranged in a block. Before initial startup of GridDB, a size of either 64 KB or 1 MB can be selected for the block size to be set up in the definition file (cluster definition file). Specify 64 KB if the installed memory of the system is low, or if the frequency of data increase is low.  As a database file is created during initial startup of the system, the block size cannot be changed after initial startup of GridDB.\n Container\nA container consists of multiple blocks. A container is a data structure that serves as an interface with the user. There are 2 data types in a container, collection and time series. Table\nA table is a special container form that exists only in NewSQL products and SQL can be operated as an interface in NewSQL. Before registering data in an application, there is a need to make sure that a container or table is created beforehand. Data is registered in a container or table. Row\nA row refers to a line of data to be registered in a container or table. Multiple rows can be registered in a container or table, but this does not mean that data is arranged in the same block. Depending on the registration and update timing, data is arranged in suitable blocks within partitions. Normally, there are columns with multiple data types in a row. Partition\nA partition is a data management unit that includes one or more containers or tables.\n Partition Group\nA group of multiple partitions is known as a partition group.\n  A partition is a data arrangement unit between clusters for managing the data movement to adjust the load balance between nodes and data multiplexing (replica) in case of a failure. Data replica is arranged in a node to compose a cluster on a partition basis. A node that can be updated against a container inside a partition is known as an owner node and 1 node is allocated to each partition. A node that maintains replicas other than owner nodes is a backup node. Master data and multiple backup data exist in a partition, depending on the number of replicas set.\nData maintained by a partition group is saved in an OS disk as a physical database file. A partition group is created with a number that depends on the degree of parallelism of the database processing threads executed by the node.\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-1_griddb-technical-overview/",
	"title": "GridDB Technical Overview",
	"tags": [],
	"description": "",
	"content": " NoSQL databases are a relatively modern phenomenon that have permeated throughout the industry very quickly. The current data models being used most frequently in NoSQL databases are: Key-Value, Document, Graph, and Column-Oriented. With the advent of GridDB, a new structure is being introduced: Key-Container. Every method of organizing data has its own set of merits and features. In this chapter, we will address some of these merits.\nScaling In addition to supplying the 3 Vs (volume, variety, velocity) required in big data solutions, GridDB also provides data reliability/availability. GridDB also utilizes its autonomous node monitoring and load balancing functions to save on precious resources for cluster applications. As the scale of a system expands, the volume of data being handled also increases, requiring a need for more resources. System expansion can be broadly divided into 2 approaches - scale-up (vertical scalability) and scale-out (horizontal scalability).\n What is scale-up?\nThis approach upgrades the system by adding memory to the operating machines, using an SSD for the disks, adding processors, and so on. With the scale-up method, it is required to halt the machine during an upgrade. This reliance on a single machine opens up the application to a certain amount of risk \u0026ndash; if the machine fails, the entire system goes down. The benefit of scaling up is having a modular system in place and simply upgrading one portion of the machine at a time (ie. upgrading the disk to flash memory).\n What is scale-out?\nThis approach increases the number of nodes (machines) constituting a system to improve the processing capability. Generally, there is no need to completely stop service when a failure occurs or during maintenance as multiple nodes are linked and operating together. However, the application management time and effort increases as the number of nodes increases. This architecture is suitable for performing highly parallel processing.\n  With GridDB, in addition to the scale-up approach to increase the number of operating nodes and reinforce the system, new nodes can be added to expand the system with a scale-out approach to incorporate nodes into an operating cluster. As an in-memory processing database, GridDB can handle a large volume of data with its scale-out model. With GridDB, data is distributed throughout the nodes inside a cluster that is composed of multiple nodes. Therefore, a large-scale memory database can be provided as the memories of multiple nodes can be used as a single, large memory space. In addition, since data management of a hybrid composition that combines the use of disk with memory is also possible, data exceeding the memory size can be retained and accessed even when operating with a standalone node. A large capacity that is not limited by the memory size can also be realized.\nSystem expansion can be carried out online with a scale-out approach. As a result, a system in operation can be supported without having to stop it as it will support the increasing volume of data as the system grows. In the scale-out approach, data is arranged in an appropriate manner according to the load of the system in the nodes built into the system. As GridDB will optimize the load balance, the application administrator does not need to worry about the data arrangement. Operation is also easy because a structure to automate such operations has been built into the system.\n"
},
{
	"uri": "http://example.org/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Chapter 1 Introduction Introduction to GridDB\n"
},
{
	"uri": "http://example.org/drivers-and-integrators/7-1_jdbc-connector/",
	"title": "JDBC Connector",
	"tags": [],
	"description": "",
	"content": " Overview of GridDB AE JDBC driver An interface that can access GridDB data using SQL is provided in the GridDB Advanced Edition (GridDB AE). This chapter provides an overview and specifications of the Java API (JBDC) used to access databases supported by the GridDB AE.\nThis chapter also consists of a description of the specified format and data types that can be used in a program using JDBC parameters, and the points to note during use.\nConnection Method Operating Environment Requirements GridDB AE can be run on the 64-bit OS of RHEL 6.2\u0026frasl;6.3\u0026frasl;6.4\u0026frasl;6.5, and CentOS 62./6.3\u0026frasl;6.4\u0026frasl;6.5.\nJDBC driver can be used with the 64-bit OS of RHEL 6.2\u0026frasl;6.3\u0026frasl;6.4\u0026frasl;6.5, CentOS 62./6.3\u0026frasl;6.4\u0026frasl;6.5, and Windows 7. In addition, please check whether JDK6/7 has been installed as the development environment of the Java language.\nDriver Specification Add the JDBC driver file gridstore-jdbc.jar to the class path. When added, the driver will be registered automatically. The JDBC driver file has been installed under /usr/share/java by default. In addition, import the driver class as follows if necessary (normally not required).\nClass.forName(\u0026ldquo;com.toshiba.mwcloud.gs.sql.Driver\u0026rdquo;);\nURL Format When Connected The URL format is as follows. If the multicast method is used to create a cluster, it is normally connected using method (A). The load will be automatically distributed on the GridDB cluster side and the appropriate nodes will be connected. Connect using method (B) only if multicast communication with the GridDB cluster is not possible.\n(A) If connecting automatically to a suitable node in a GridDB cluster using the multicast method:\njdbc:gs//(multicastAddress):(portNo)/(clusterName)/(databaseName)\nmulticastAddress: Multicast address used in conecting with a GridDB cluster. (Default is 239.0.0.1)**\nPortNo:** Port number used in connecting with a GridDB cluster. (Default is 41999)**\nclusterName:** Cluster name of GridDB cluster**\ndatabaseName:** Database name. Connect to the default database if omitted.\n*multicastAddress, portNo can be amended by editing the gs_cluster.json file.\n(B) If connecting directly to a node in a GridDB cluster using the multicast method\njdbc:gs:///(nodeAddress):(portNo)/(clusterName)/(databaseName)\nnodeAddress: Address of node\nportNo: Port number used in connecting with a node Default value is 20001\nclusterName: Cluster name of GridDB cluster that the node belongs to\ndatabaseName: Database name. Connect to the default database if omitted.\n*Default values of nodeAddress, portNo can be amended by editing the gs_node.json file.\n*If the fixed list method is used to compose a cluster, use method \u0026copy; to connect.\n\u0026copy; If connecting to a GridDB cluster using the fixed list method\njdbc:gs:///(clusterName)/(databaseName)?notificationMember=(notificationMember)\nclusterName: Cluster name of GridDB cluster\ndatabaseName: Database name. Connect to the default database if omitted.\nnotificationMember: Address list of node(URL encoding required). Default port is 20001.\nExample: 192.168.0.10:20001, 192.168.0.11:20001, 192.168.0.12:20001\n*notificationMember can be amended by editing the gs_cluster.json file.\nPort used in the address list can be amended by editing the gs_node.json file\nIf the provider method is used to compose a cluster, use method (D) to connect.\n(D) if connecting to a GridDB cluster using the provider method.\njdbc:gs:///(clusterName)/(databaseName)?notificationProvider=(notificationProvider)\nclusterName: Cluster name of GridDB cluster\ndatabaseName: Database name. Connect to the default database if omitted.\nnotificationProvider: URL of address provider (URL encoding required)\n*notificationProvider can be amended by editing the gs_cluster.json file.\nIf the user name and password are going to be included in the URL in any of the cases, (A) to (D), add them at the end of the URL as shown below. ?user=(user name)\u0026amp;password=(password)\nConnection Timeout Settings The connection timeout can be set in either of the methods (1) and (2). Setting (2) is prioritized if both (1) and (2) are set. A defalut value of 300 seconds (5 minutes) is used if neither (1) or (2) has been set, or if there are no settings at all.\n(1) Specify with the DriverManager#setLoginTimeout (int seconds) The value in seconds is set as follows.\n If the value is 1 to Integer.MAX_VALUE\n Set by the specified number of seconds\n If the value is Integer.MIN_VALUE to 0\n Not set\n  After setting, connection timeout will be set in the connections to all the GridDB AE acquired by the DriverManager#getConnection or Driver#connect.\n(2) Specify with DriverManager#getConnection (String url, Properties info) or Driver#connect (String url, Properties info)\nAdd a property to argument info in the key \u0026ldquo;loginTimeout\u0026rdquo;. If the value corresponding to the key \u0026ldquo;loginTimeout\u0026rdquo; can be converted to a numerical value, the connection timeout will be set in the connection obtained as follows.\n If the converted value is 1 to Integer.MAX_VALUE\n Set by the specified number of seconds\n If the value is Integer.MIN_VALUE to 0\n Not Set\n  Observations  A container created by GridDB SE (Standard Edition) client can be referenced as a table by the GridDB AE JDBC driver but it cannot be updated. Besides updating the rows, changes in the schema and index of a container are also included in an update. Conversely, a table created with the GridDB AE JDBC driver can neither be referenced nor updated from a GridDB SE client.\n If a TimeSeries container created by a GridDB client is searched with a SQL command from the GridDB AE, the results will not be in chronological order if no ORDER BY phrase is specified for the main key. Specify an ORDER BY against the main key if a chronological series of the SQL results is required.\n Regarding consistency during a search, the way the results are viewed may differ between a search conducted from a GridDB SE client and a search conducted from a GridDB AE JDBC driver. As GridDB supports READ COMMITTED as an isolation level of the trasnaction, data committed at the point search is started will be read in a search. If a search is conducted from a GridDB SE client, since each search request is read once in the prcoess, only data committed at the point the search is started will be searched as per READ COMMMITTED. On the other hand, if a search is conducted with a SQL command from a GridDB AE JDBC driver, the requested SQL may be read and processed several times. In this case, the respective readings will become READ COMMITTED. As a result, when a transaction of another client is committed, in-between the readings the update details are ready by the next reading and the entire SQL may not become READ COMMITTED.\n When the number of SQL hits is large, the error \u0026ldquo;memory limit exceed\u0026rdquo; may occur. In this case, add the descriptions of parameter transaction:totalMemoryLimit and dataStore:resultSetMemoryLimit in the gs_node.json file as shown below, and expand the upper limit value of the memory used in the SQL process. However, the names of these parameters may be changed or deleted in future versions.\n  Example: when specifying the upper limit value\n\u0026ldquo;transaction\u0026rdquo;:{ \u0026ldquo;totalMemoryLimit\u0026rdquo;:\u0026ldquo;2048MB\u0026rdquo; }\n\u0026ldquo;dataStore\u0026rdquo;:{ \u0026ldquo;resultSetMemoryLimit\u0026rdquo;:\u0026ldquo;20480MB\u0026rdquo; }\n*Default value is 1024MB and 10240 MB respectively if there is no description.\nSpecifications of GridDB AE JDBC Driver The specifications of the GridDB AE JDBC Driver are shown in this section. This section explains mainly the support range of the driver as well as the differences with the JDBC standard. See the JDK API reference for the API specifications that conform to the JDBC standard unless otherwise stated.\nPlease note that the following could be revised in the future versions.\n Actions not conforming to the JDBC standard Support status of unsupported functions Error messages  Common Items Supported JDBC Version The following functions corresponding to some of the functions of JDBC 4.1 are not supported.\n Transaction control Stored procedure Error processing  Use of Unsupported Functions  Standard Functions\nA SQL FeatureNotSupportedException occurs if a function that ought to be but is currently not supported by a driver conforming to the JDBC specifications is used. This action differs from the original SQLFeatureNotSupportException specifications. Error name (to be described later) is JDBC_NOT_SUPPORTED.\n Optional Functions\nIf a function not supported by the driver that is positioned as an optional function in the JDBC specifications and for which a SQLFeatureNotSupportedException may occur is used, a SQLFeatureNotSupportedException will occur as per the JDBC specifications. Error name is JDBC_OPTIONAL_FEATURE_NOT_SUPPORTED.\n  Invoke A Method Against A Closed Object As per the JDBC specifications, when a method other than isClosed() is invoked for an object that has a close() method, e.g. a conection object, etc. a SQL Exception will occur. Error name is JDBC_ALREADY_CLOSED\nInvalid Null Argument If null is specified as the API method argument despite not being permitted, a SQLException due to a JDBC_EMPTY_PARAMETER error will occur. Null is not permitted except for arguments which explicitly accepts null in the JDBC specifications\nIf There Are Multiple Error Causes If there are multiple error causes, control will be returned to the application at the point either one of the errors is detected. In particular, if use of an unsupported function is attempted, it will be detected earlier than other errors. For example, if there is an attempt to create a stored procedure for a closed connection object an error indicating that the operation is \u0026ldquo;not supported\u0026rdquo; instead of \u0026ldquo;closed\u0026rdquo; will be returned.\nDescription Of Exception A check exception thrown from the driver is made up of a SQLException or a subclass instance of the SQLException\nUse of the following method to get the exception details.\n getErrorCode()\nFor errors detected by GridDB in either the server or client, an error number will be returned. See the list of error codes for the specifc number and cause. getSQLState()\nCorresponding SQLSTATE is returned if an error is detected in the SQL process or JDBC driver. null is returned otherwise. getMessage()\nReturn an error message containing the error number and error description as a set. The format is as follows.  [(Error number): (Error code name)] (Supplementary message)\nSample A JDBC sample program is given below:\nimport java.sql*\npublic class SampleJDBC{ public static void main (String[] args) throws SQLException { String url = null, user = \u0026ldquo;\u0026rdquo;, password = \u0026ldquo;\u0026rdquo;;\n if (args.length != 3) { System.err.println ( \u0026quot;usage: java SAmple JDBC (multicastAddress)(port)(clusterName)\u0026quot;); System.exit(1) } // url is \u0026quot;jdbc:gs:// (multicastAddress): (portNo)/(clusterName)\u0026quot; format url = \u0026quot;jdbc:gs://\u0026quot; + args\\[0\\] + \u0026quot;:\u0026quot; + args\\[1\\] + \u0026quot;/\u0026quot; + args\\[2\\]; user = \u0026quot;system\u0026quot;; password = \u0026quot;manager\u0026quot;; System.out.println(\u0026quot;DB Connection Start\u0026quot;); // Connection with GridDB Cluster Connection con = DriverManager.getConnection (url, user, password); try{ System.out.println(\u0026quot;Start\u0026quot;); Statement st = con.createStatement(); ResultSet rs = con.createStatement(); ResultSet rs = st.executeQuery(\u0026quot;SELECT * FROM table01\u0026quot;); ResultSetMetaData md = rs.getMetaData(); while (rs.next()){ for (int 1 = 0; i \u0026lt; md.getColumnCount(); i++) { System.out.print(rs.getStrng(i + 1) + \u0026quot;|\u0026quot;); } System.out.println(\u0026quot;\u0026quot;); } rs.close(); System.out.println(\u0026quot;End\u0026quot;); st.close (); } finally { System.out.println(\u0026quot;DB Connection Close\u0026quot;); con.close(); } }  }\n"
},
{
	"uri": "http://example.org/administration/6-1_supported-os-platforms-and-system-requirements/",
	"title": "Supported OS, Platforms and System Requirements",
	"tags": [],
	"description": "",
	"content": " Supported Environments The following Linux distributions are tested and will work with GridDB\n OS: Red Hat Enterprise Linux 6.2 / 6.3 / 6.4 / 6.5 (x86_64) CentOS 6.2 / 6.3 / 6.4 / 6.5 (x86_64)  Hardware Requirements As GridDB is a scale-out database, it is easy to add more machines to the cluster to increase both computing power and storage space simultaneously. Some basic hardware requirements look like this:\n Memory: 32GB+ Disk: 100GB+  Network Requirements There are no strict network requirements to use GridDB. The only caveat that exists is that the cluster must be able to communicate via Multicast networking for use with the Community Edition.\n_For a more detailed look at estimated requirements, please refer to chapter 2.1_\n"
},
{
	"uri": "http://example.org/getting-started/2-1_system-requirements/",
	"title": "System Requirements",
	"tags": [],
	"description": "",
	"content": " Checking the Required Resources GridDB is a scale-out database that \u0026ndash; unlike a conventional database \u0026ndash; pre-planning of the system design and sizing is not required in order to achieve non-stop operation. However, the following points should be considered as guidelines in the initial system design:\n Memory usage Number of nodes in a cluster Disk usage  The estimation method is explained in sequence below.\nFunctions to increase the capacity by using external storage devices such as SSDs, etc. have not been considered in calculating the memory size below. Please check with our service desk for estimation if these functions are used.\nTotal memory usage 1. Predict the amount of data to be stored in the application. Estimate the following:\n Data size of row Number of rows to be registered  2. Estimate the memory required to store those estimated data.\nMemory capacity used = row data size × no. of registered rows ÷ 0.75 + 8 × no. of registered rows × (assigned index number + 2) ÷ 0.66 (byte)\n3. Perform a similar estimation for all collections created and used in the application. The total sum becomes the amount of memory required in the GridDB cluster.\nTotal memory usage = the sum of memory usage in all collections\nNote: please consider this number as the minimum requirement, as the memory usage also depends on update frequency.\nNumber of Nodes Constituting a Cluster Estimate the required no. of nodes used in GridDB. In the example below, it is assumed that one node is executed in one machine.\nFirst, make an assumption of the memory required per machine.\nIn addition, make an assumption of the no. of replicas to create. The no. of replicas is given as a GridDB configuration value.\nDisk Usage Estimate the size of the file to be created in GridDB, and the disk capacity required for the machine to execute a node. Two types of files should be created; a checkpoint file and a transaction log file.\nThe memory usage of individual node is determined as follows:\nIndividual memory usage = (total memory usage * no. of replicas) ÷ no. of nodes (byte)\nThe size of the checkpoint file is estimated as follows based on this numerical value.\nFile size = Individual memory usage * 2 (byte)\nIn addition, as the transaction log life size is dependent on the update frequency of the application, the following data is thus predicted.\nRow update frequency (times/sec)\nFurthermore, the checkpoint interval is assumed. The checkpoint interval is given as a GridDB configuration value.\n Checkpoint interval  The default value of the checkpoint interval is 1200 sec (20 minutes).\nThe transaction log file size is estimated as follows based on these numerical values:\nFile size = row data size * row update frequency * checkpoint interval (byte)\nThe individual disk usage is estimated as follows.\nIndividual disk usage = Transaction log file size + checkpoint file size\n[Point to note]\nThe size of the checkpoint file expands depending on the data capacity. However, please note that once it has expanded, the file size will not be reduced, even if some of data in the container or row gets deleted. Empty space can be re-used after data is deleted.\n"
},
{
	"uri": "http://example.org/introduction/1-1_what-is-griddb/",
	"title": "What is GridDB",
	"tags": [],
	"description": "",
	"content": " Toshiba GridDB™ is a highly scalable NoSQL database best suited for IoT and Big Data We live in the era of the Internet of Things (IoT) where billions of devices are interconnected and are generating petabytes of data at an increasing rate. Gaining insights and information from that data and generating value out of it gives a tangible competitive advantage to businesses, organizations, governments, and even individuals.\nOrganizations should focus more on creating value from data that will enhance their core products, services or even operational processes rather than spend time in dealing with the complexity surrounding Big Data. Big data, in this case, means data in large quantities, high frequencies, and vast varieties.\nGridDB is an innovative solution built in Toshiba to solve these complex problems. The foundation of GridDB’s principles is based upon offering a versatile data store that is optimized for IoT, provides high scalability, is tuned for high performance, and ensures high reliability.\nFour Pillars of GridDB  Optimized for IoT High Performance High Scalability High Reliability/Availability  1. Optimized for IoT GridDB’s Key Container data model and Time Series functions are built for IoT The Key Container data model of GridDB extends the typical NoSQL Key-Value store. The Key Container model represents data in the form of collections that are referenced by keys. The key and container are rough equivalents of the table name and table data in Relational Databases (RDB). Data modeling in GridDB is easier than with other NoSQL databases as we can define the schema and design the data similar to that of an RDB.\nThe Key Container model allows high speed access to data through Java and C APIs. Data in GridDB is also queried through TQL, a custom SQL-like query language. Basic search through the WHERE command and high speed conditional search operations through indexing offer a great advantage for applications that rely on faster search. GridDB supports transactions, including those with plural records from the application. Transactions in GridDB guarantee ACID (Atomicity, Consistency, Isolation, and Durability) at the container level.\nTwo types of containers are prominent in GridDB: Collection-Container, a general-purpose container; and TimeSeries-Container which is for managing time series data.\nTimeSeries-Container is apt for IoT scenarios where the data is associated with a time-stamp. GridDB supports numerous time-series functions such as\n Data compression, for ever-increasing time series data. This functionality reduces memory usage significantly compared to other DBMS Term release, to automatically delete records that are no longer valid or needed Time series data aggregation and sampling functions  2. High Performance GridDB’s hybrid composition of In-Memory and Disk architecture is designed for maximum performance I/O is a common bottleneck in any DBMS that can cause the CPU to be under-utilized. GridDB overcomes this bottleneck with the ‘Memory first, Storage second’ structure where the ‘primary’ data that is frequently accessed resides in memory and the rest is passed on to disks (SSD and HDD). High performance is achieved in GridDB by:\nPrioritizing In-Memory processing – In scenarios with large amounts of data, GridDB localizes the data access needed by applications by placing as much ‘primary’ data in the same block as possible. Based on the application’s access pattern and frequency GridDB efficiently utilizes memory space by setting hint memory intensity function and thus reduces memory misses.\nReducing the Overhead – Operational and communication overhead occurs in multi-threaded operations due to lock and synchronization. GridDB eliminates this by allocating an exclusive memory and DB file to each CPU core / thread. As a result, execution time gets shortened and better performance is achieved.\nParallel Processing – GridDB achieves high performance through parallel processing within a node and across nodes. Parallel processing across nodes is done by distributing a large dataset among multiple nodes (partitioning). Parallelism is made possible by the event-driven engine which processes multiple requests using the least amount of resources.\n3. High Scalability GridDB scales linearly and horizontally on commodity hardware maintaining excellent performance Traditional RDBMS are built on Scale-Up architecture (add more capacity to existing server/node). Transactions and data consistency are excellent on RDBMS. On the other hand, NoSQL databases focus on Scale-Out architecture (add smaller nodes to form a large cluster) but fair poorly on transactions and data consistency.\nGridDB scales out horizontally with commodity hardware maintaining the same level of performance. Contrary to other scale-out NoSQL databases, GridDB offers strong data consistency at the container level and provides ACID transaction guarantees similar to that of an RDB. Proprietary algorithms of GridDB allow nodes to be added on the fly online without having to stop the service or operation. GridDB offers a dual advantage for businesses that need a scale-out database for large amounts of data but still want to maintain data consistency.\n4. High Reliability/Availability Hybrid cluster management and high fault-tolerant system of GridDB is exceptional for mission-critical applications Network partitions, node failures, and maintaining consistency are some of the major problems that arise when data is distributed across nodes. Typically, distributed systems adopt ‘Master-Slave’ or ‘Peer-to-Peer’ architectures. Master-Slave option is good at maintaining data consistency but a master node redundancy is required to avoid having a Single Point of Failure (SPOF). Peer-to-Peer, though avoids SPOF, has a huge problem of communication overhead among the nodes.\nGridDB’s autonomous control cluster architecture integrates the advantages of and overcomes the disadvantages of both Master-Slave and Peer-to-Peer styles. GridDB’s algorithms select the master node automatically among peers, and, in case of master node failure, operations remain intact as a new master is appointed automatically and immediately. GridDB’s proprietary algorithms avoid the classic distributed computing problem of Split-Brain, which occurs due to cluster partition during network failures. GridDB also offers various levels of replication based on the availability requirements of the application.\nOverall, GridDB offers multiple reliability features for mission-critical applications that require high availability and data retention.\nAnd finally\u0026hellip;\nWhen it comes to IoT and Big Data use-cases, GridDB clearly stands out among other databases in the Relational and NoSQL space. Toshiba’s customers in various industry verticals have successfully implemented IoT projects by harnessing the power of GridDB. For more detailed information on each topic covered above, business use-cases and implementation, benchmark performance results and many more please check the GridDB website www.griddb.org\n"
},
{
	"uri": "http://example.org/data-modeling/4-2_data-modeling-using-griddb/",
	"title": "Data Modeling Using GridDB",
	"tags": [],
	"description": "",
	"content": " Storing Data in Collection\nThe flow of storing data in a Collection is shown below. First, if there is no Collection to store data in, create a new Collection following the procedure below:\n Get a GridStore instance. Create a Collection.  You can store data in an existing Collection with the following procedure:\n Get a GridStore instance. Get a collection. Set operation parameters Create indexes. Create values to be stored. Store values in the Collection Perform a commit a proper intervals Release the GridStore instance.  Storing data in a TimeSeries container follows the exact same procedure.\nThis examples below are intended to provide an application which has the following capabilities:\n Storing facility information Storing alarm history Storing sensor data Searching for and displaying facility information and sensor data showing abnormality  The following sections describe client programs which implement each capability based on the schema definitions shown in the previous chapter.\n4.2.1 Storing Facility Information Technically, in the monitoring system, the information on facility configuration and specifications needs to be stored in a database. For simplicity, however, this section shows a sample program which loads facility information collectively from a CSV file storing the data. An outline of the processing flow is shown below.\n Connect to a server and get a GridStore instance. Create a facility information Collection name with a specified name (\u0026ldquo;equipment_col\u0026rdquo;) in GridStore. Create indexes to be used for search. Store a value repeatedly while reading a CSV file, as follows:  4-1. Analyze a read CSV-formatted line and create a facility information object to store. 4-2. Store (put) the created facility information object in the facility information Collection. 4-3. Perform a commit if repeated the predetermined number of times.  Release the GridStore instance if all CSV-formatted lines are processed.  A concrete sample program is shown below:\n1: package pvrms; 2:\n3: import java.io.FileReader; 4: import java.io.IOException; 5: import java.text.ParseException; 6: import java.util.Properties; 7:\n8: import au.com.bytecode.opencsv.CSVReader; 9:\n10: import com.toshiba.mwcloud.gs.Collection; 11: import com.toshiba.mwcloud.gs.GSException; 12: import com.toshiba.mwcloud.gs.GridStore; 13: import com.toshiba.mwcloud.gs.GridStoreFactory; 14: import com.toshiba.mwcloud.gs.RowKey; 15:\n16: // Facility information 17: class Equip { 18: @RowKey String id; 19: String name; 20: //Blob spec; // For simplicity, spec information is not used. 21: } 22:\n23: public class SimplePv0 { 24:\n25: /* 26: * Load facility information from a CSV file. 27: / 28: public static void main(String[] args) throws GSException, ParseException, IOException { 29:\n30: // Specify a server. 31: final String gsServer = \u0026ldquo;127.0.0.1\u0026rdquo;; 32: final String gsPort = \u0026ldquo;10001\u0026rdquo;; 33: final String user = \u0026ldquo;admin\u0026rdquo;; 34: final String password = \u0026ldquo;admin\u0026rdquo;; 35:\n36: final String equipColName = \u0026ldquo;equipment_col\u0026rdquo;; 37:\n38: // Get a GridStore instance. 39: final Properties prop = new Properties(); 40: prop.setProperty(\u0026ldquo;host\u0026rdquo;, gsServer); 41: prop.setProperty(\u0026ldquo;port\u0026rdquo;, gsPort); 42: prop.setProperty(\u0026ldquo;user\u0026rdquo;, user); 43: prop.setProperty(\u0026ldquo;password\u0026rdquo;, password); 44: final GridStore store = GridStoreFactory.getInstance().getGridStore(prop); 45:\n46:\n47: // Read a CSV file. 48: String dataFileName = \u0026ldquo;equipName.csv\u0026rdquo;; 49: CSVReader reader = new CSVReader(new FileReader(dataFileName)); 50: String[] nextLine; 51:\n52: / 53: * Create a Collection. 54: / 55: Collection equipCol = store.putCollection(equipColName, Equip.class); 56:\n57: / 58: * Create indexes for Columns. 59: / 60: equipCol.createIndex(\u0026ldquo;id\u0026rdquo;); 61: equipCol.createIndex(\u0026ldquo;name\u0026rdquo;); 62:\n63: / 64: * Set autocommit mde to OFF. 65: / 66: equipCol.setAutoCommit(false); 67:\n68: // Commit interval 69: Long commtInterval = (long) 1; 70:\n71: / 72: * Store a value. 73: */ 74: Equip equip = new Equip(); 75: Long cnt = (long) 0; 76: byte[] b = new byte[1]; 77: b[0] = 1; 78: while ((nextLine = reader.readNext()) != null) { 79:\n80: // Store facility information. 81: equip.id = nextLine[2]; 82: equip.name = nextLine[3]; 83:\n84: equipCol.put(equip); 85:\n86: cnt++; 87:\n88: if(0 == cnt%commtInterval) { 89: // Commit a transaction. 90: equipCol.commit(); 91: } 92:\n93: } 94:\n95: // Release a resource 96: store.close(); 97: reader.close(); 98:\n99: } 100:\n101: }\n4.2.2 Storing Alarm History Technically, in the monitoring system, a sensor or a facility directly sends an alarm to GridDB and stores it . For simplicity\u0026rsquo;s sake, however, this section shows a sample program which loads alarm history data collectively from a CSV file storing the data. An outline of the processing flow is shown below.\n Connect to a server and get a GridStore instance. Create an alert Collection with a specified name (\u0026ldquo;alert_col\u0026rdquo;) in GridStore and get it. Create indexes to be used for search. Store a value repeatedly while reading a CSV file, as follows:  4-1. Analyze a read CSV-formatted line and create an alert object to store. 4-2. Store (put) the created alert object in the alert Collection. 4-3. Perform a commit if repeated the predetermined number of times.  Release the GridStore instance if all CSV-formatted lines are processed.  A concrete sample program is shown below:\n1: package pvrms; 2:\n3: import java.io.FileReader; 4: import java.io.IOException; 5: import java.text.ParseException; 6: import java.text.SimpleDateFormat; 7: import java.util.Date; 8: import java.util.Properties; 9:\n10: import au.com.bytecode.opencsv.CSVReader; 11:\n12: import com.toshiba.mwcloud.gs.Collection; 13: import com.toshiba.mwcloud.gs.GSException; 14: import com.toshiba.mwcloud.gs.GridStore; 15: import com.toshiba.mwcloud.gs.GridStoreFactory; 16: import com.toshiba.mwcloud.gs.RowKey; 17:\n18: // Alert information 19: class Alert { 20: @RowKey Long id; 21: Date timestamp; 22: String sensorId; 23: int level; 24: String detail; 25: } 26:\n27: public class SimplePv1 { 28:\n29: /* 30: * Load alert data from a CSV file. 31: / 32: public static void main(String[] args) throws GSException, ParseException, IOException { 33:\n34: // Specify a server. 35: final String gsServer = \u0026ldquo;127.0.0.1\u0026rdquo;; 36: final String gsPort = \u0026ldquo;10001\u0026rdquo;; 37: final String user = \u0026ldquo;admin\u0026rdquo;; 38: final String password = \u0026ldquo;admin\u0026rdquo;; 39:\n40: final String alertColName = \u0026ldquo;alert_col\u0026rdquo;; 41:\n42: // Get a GridStore instance. 43: final Properties prop = new Properties(); 44: prop.setProperty(\u0026ldquo;host\u0026rdquo;, gsServer); 45: prop.setProperty(\u0026ldquo;port\u0026rdquo;, gsPort); 46: prop.setProperty(\u0026ldquo;user\u0026rdquo;, user); 47: prop.setProperty(\u0026ldquo;password\u0026rdquo;, password); 48: final GridStore store = GridStoreFactory.getInstance().getGridStore(prop); 49:\n50: // Read a CSV file. 51: String dataFileName = \u0026ldquo;alarmHistory.csv\u0026rdquo;; 52: CSVReader reader = new CSVReader(new FileReader(dataFileName)); 53: String[] nextLine; 54:\n55: / 56: * Create a Collection. 57: / 58: Collection alertCol = store.putCollection(alertColName, Alert.class); 59:\n60: / 61: * Create indexes for Columns. 62: / 63: alertCol.createIndex(\u0026ldquo;timestamp\u0026rdquo;); 64: alertCol.createIndex(\u0026ldquo;level\u0026rdquo;); 65:\n66: / 67: * Set autocommit mde to OFF. 68: / 69: alertCol.setAutoCommit(false); 70:\n71: // Commit interval 72: Long commtInterval = (long) 1; 73:\n74: / 75: * Store a value. 76: */ 77: SimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/ddHH:mm:ss\u0026rdquo;); 78: Alert alert = new Alert(); 79: Long cnt = (long) 0; 80: while ((nextLine = reader.readNext()) != null) { 81:\n82: String dateS = nextLine[0];//2011/1/1 83: String timeS = nextLine[1];//19 (hundred hours) 84: String datetimeS = dateS + \u0026ldquo; \u0026rdquo; + timeS + \u0026ldquo;:00:00\u0026rdquo;; 85: Date date = format.parse(datetimeS); 86: Long datetime = date.getTime(); 87:\n88: alert.id = ++cnt; 89: alert.timestamp = new Date(datetime); 90: alert.sensorId = nextLine[2]; 91: alert.level = Integer.valueOf(nextLine[3]); 92: alert.detail = nextLine[4]; 93:\n94: alertCol.put(alert); 95:\n96: if(0 == cnt%commtInterval) { 97: // Commit a transaction. 98: alertCol.commit(); 99: } 100:\n101: } 102:\n103: // Release a resource. 104: store.close(); 105: reader.close(); 106:\n107: } 108:\n109: }\n4.2.3 Storing Sensor Data Technically, in the monitoring system, a sensor directly sends a measured value to be stored in GridStore. For simplicity, however, this section shows a sample program which loads sensor data collectively from a CSV file storing the data. An outline of a processing flow is shown below.\n Connect to a server and get a GridStore instance. Read the first line of a CSV file and create a set of TimeSeries to be used beforehand, as follows:  2-1. Analyze the first CSV-formatted line and obtain multiple sensor IDs (= the names of TimeSeries to be created). 2-2. Create an alert Collection in GridStore for each obtained sensor ID.  Store a value repeatedly, while reading the rest of the CSV file, as follows:  3-1. Analyze a read CSV-formatted file and create a Point object to store. 3-2. Store (put) the created Point object in an appropriate TimeSeries.  Release the GridStore instance if all CSV-formatted lines are processed.  A concrete sample program is shown below:\n1: package pvrms; 2:\n3: import java.io.FileReader; 4: import java.io.IOException; 5: import java.text.ParseException; 6: import java.text.SimpleDateFormat; 7: import java.util.Date; 8: import java.util.Properties; 9:\n10: import au.com.bytecode.opencsv.CSVReader; 11:\n12: import com.toshiba.mwcloud.gs.GridStore; 13: import com.toshiba.mwcloud.gs.GridStoreFactory; 14: import com.toshiba.mwcloud.gs.RowKey; 15: import com.toshiba.mwcloud.gs.TimeSeries; 16:\n17: // Sensor data 18: class Point { 19: @RowKey Date time; 20: double value; 21: String status; 22: } 23:\n24: public class SimplePv2 { 25:\n26: /* 27: * Load time-series data form a CSV file. 28: / 29: public static void main(String[] args) throws ParseException, IOException { 30:\n31: // Specify a server. 32: final String gsServer = \u0026ldquo;127.0.0.1\u0026rdquo;; 33: final String gsPort = \u0026ldquo;10001\u0026rdquo;; 34: final String user = \u0026ldquo;admin\u0026rdquo;; 35: final String password = \u0026ldquo;admin\u0026rdquo;; 36:\n37: // Get a GridStore instance. 38: final Properties prop = new Properties(); 39: prop.setProperty(\u0026ldquo;host\u0026rdquo;, gsServer); 40: prop.setProperty(\u0026ldquo;port\u0026rdquo;, gsPort); 41: prop.setProperty(\u0026ldquo;user\u0026rdquo;, user); 42: prop.setProperty(\u0026ldquo;password\u0026rdquo;, password); 43: final GridStore store = GridStoreFactory.getInstance().getGridStore(prop); 44:\n45: // Read a CSV file. 46: String dataFileName = \u0026ldquo;sensorHistory.csv\u0026rdquo;; 47: CSVReader reader = new CSVReader(new FileReader(dataFileName)); 48: String[] nextLine; 49: nextLine = reader.readNext(); 50:\n51:\n52: // Presupposing that the 1st line contains sensor IDs and the rest of lines contain data. 53: // Presupposing that 0:date, 1:time, \u0026hellip; 54:\n55:\n56: / 57: * Read a sensor ID and create a TimeSeries. 58: / 59: String[] tsNameArray = new String[nextLine.length]; 60: for(int j = 2; j \u0026lt; nextLine.length; j++) { 61:\n62: tsNameArray[j] = nextLine[j]; 63: store.putTimeSeries(tsNameArray[j], Point.class); 64:\n65: } 66:\n67: / 68: * Store a value in each TimeSeries. 69: */ 70: SimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/ddHH:mm:ss\u0026rdquo;); 71: Point point = new Point(); 72: while ((nextLine = reader.readNext()) != null) { 73:\n74: String dateS = nextLine[0];//2011/1/1 75: String timeS = nextLine[1];//19 (hundred hours) 76: String datetimeS = dateS + \u0026ldquo; \u0026rdquo; + timeS + \u0026ldquo;:00:00\u0026rdquo;; 77: Date date = format.parse(datetimeS); 78: Long datetime = date.getTime(); 79:\n80: for(int i = 2, j = 2; j \u0026lt; nextLine.length; i++, j+=2) { 81:\n82: TimeSeries ts = store.getTimeSeries(tsNameArray[i], Point.class); 83:\n84: point.time = new Date(datetime); 85: point.value = Double.valueOf(nextLine[j]); 86: point.status = nextLine[j+1]; 87:\n88: ts.append(point); 89: } 90: } 91:\n92: // Release a resource. 93: store.close(); 94: reader.close(); 95:\n96: } 97:\n98: }\n"
},
{
	"uri": "http://example.org/getting-started/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Chapter 2 Getting Started Quickstart\n"
},
{
	"uri": "http://example.org/introduction/1-2_griddb-edition/",
	"title": "GridDB Editions",
	"tags": [],
	"description": "",
	"content": " There are currently two distinct versions of GridDB available. The information below is designed to help distinguish between the editions and help users arrive to the correct choice for their situation.\nGridDB Community Edition (GridDB CE) GridDB CE is available under the AGPLv3 as a high-performant, open-source NoSQL database built with scalability and fault tolerance in-mind. This is the only edition of GridDB that is open-source.\nGridDB Standard Edition (GridDB SE) A high-performant commerical NoSQL database. Included with the commercial license are tools, features, and the software support needed for critical big data applications.\nComparison of each edition are as follows:\n SOFTWARE SUPPORT\nFree Evaluation of SE does not include software support\nMaintenance Releases\nx\nBug fixes/Patches\nx\nUpdates\nx\nBASIC\nDistributed Data Management\nTransaction Management\nDATA TYPE SUPPORT\nKey-value data\nTime-series data\nGeometry data\nx\nQUERY LANGUAGE\nSQL Subset (TQL)\nSCALABILITY\nOffline expansion\nOnline expansion\nx\nPERSISTENCY\nIn-Memory and Disk\nAPI\nJava\nC\nADMINISTRATIVE TOOL\nOffline backup\nOnline backup\nx\nExport/import\nx\nDifferential backup function\nx\nManagement GUI\nx\nStatus Acquisition\nLicense Types\nSubscription License: A license that can only be used during the contract period service\n"
},
{
	"uri": "http://example.org/getting-started/2-2_installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " The following instructions are for the Standard Edition of GridDB. Instructions for the Community Edition can be found below\nSetup This chapter explains the installation of a node onto a single machine. See the chapter on operations for the cluster configuration.\n// The lsb_release command prints certain LSB (Linux Standard Base) and Distribution information. $ lsb_release -id Distributor ID: CentOS Description: CentOS release 6.3 (Final)   Installing a Node\nThe following 3 RPM packages are needed when installing a GridDB node. Place these packages anywhere in the machine.\nTODO: Table\nPackage name\nFile name\nDescription\ngriddb-server\ngriddb-server-X.X.X-linux.x86_64.rpm\nThe start and other commands for the GridDB node module and server are included.\ngriddb-client\ngriddb-client-X.X.X-linux.x86_64.rpm\nOne set of operating commands except start node is included.\ngriddb-docs\ngriddb-docs-X.X.X-linux.x86_64.rpm\nGridDB manual and program samples are included.\n  *X.X.X is the GridDB version\nInstall using the rpm command as a root user.\n$ su \\# rpm -Uvh griddb-server-X.X.X-linux.x86_64.rpm Under preparation... ########################################### \\[100%\\] User gsadm and group gridstore have been registered. GridStore uses new user and group. 1:griddb-server ########################################### \\[100%\\] \\# rpm -Uvh griddb-client-X.X.X-linux.x86_64.rpm Under preparation... ########################################### \\[100%\\] User and group has already been registered correctly. GridStore uses existing user and group. 1:griddb-client ########################################### \\[100%\\] \\# rpm -Uvh griddb-docs-X.X.X-linux.x86_64.rpm Under preparation... ########################################### \\[100%\\] 1:griddb-docs ########################################### \\[100%\\]  When you install the package, the following group and user are created in the OS. This OS user is set as the operator of GridDB.\nTODO: Table\nGroup\nUser\nHome directory\ngridstore\ngsadm\n/var/lib/gridstore\nThe following environment variables are defined in this gsadm user.\nTODO: Table Environment variables\nValue\nMeaning\nGS_HOME\n/var/lib/gridstore\ngsadm/GridDB home directory\nGS_LOG\n/var/lib/gridstore/log\nEvent log file output directory\n[Points to note]\n These environment variables are used in the operating commands described later. The password of the gsadm user has not been set. With the root user privilege, please set the password appropriately.  Some of the functions of the operation tools may be necessary.   In addition, when the GridDB node module is installed, services that are executed automatically upon startup of the OS will be registered.\nTODO: Table Service Name\nRun Level\ngridstore\n3, 4, 5\nThe service registration data can be checked with the following command.\n# /sbin/chkconfig \u0026ndash;list | grep gridstore gridstore 0:off 1:off 2:off 3:on 4:on 5:on 6:off\nThe GridDB node will be started automatically by this service during OS startup.\n[Points to note]\n Services will not start automatically immediately after installation.  To Stop auto startup of a service, use the command below.\n# /sbin/chkconfig gridstore off\nSee the chapter on services in \u0026lsquo;GridDB Operational Management Guide\u0026rsquo; (GridDB_OperationGuide.html) for details of the services.\nPost-Installation Checks\nCheck the directory configuration of the installed GridDB node.\nFirst, check that the GridDB home directory, and related directory and files have been created.\nGridDB home directory /var/lib/gridstore/ # GridDB home directory admin/ # integrated operational management GUI home directory backup/ # backup directory conf/ # definition file directory gs_cluster.json # cluster definition file gs_node.json # node definition file password # user definition file data/ # database file directory log/ # log directory  Confirm the following command.\n$ ls /var/lib/gridstore/ admin backup conf data log  Next, check that the installation directory has been created.\nInstallation directory /usr/gridstore-X.X.X/ # installation directory Fixlist.pdf # revision record Readme.txt # release instructions bin/ # operation command, module directory conf/ # definition file sample directory docs/ # document directory etc/ lib/ # library directory license/ # license directory prop/ # configuration file directory web/ # integrated operational management GUI file directory  Confirm with the following command.\n$ ls /usr/gridstore-X.X.X/ Fixlist.pdf Readme.txt bin conf etc lib license prop web  All documents have been compressed into a single ZIP file. Decompress and refer to the documents where appropriate as shown below.\n$ cd /usr/gridstore-X.X.X/docs $ unzip griddb-documents-X.X.X.zip  In addition, the following symbolic links are created as shown below in a few directories under the installation directory for greater convenience.\n$ ls /usr/gridstore/ conf lib prop web  Lastly, confirm the version of the server module installed with the following command.\n$ gsserver --version GridDB version X.X.X build XXXXX  [Points to note]\nThe following files are created when GridDB is operated according to the following procedure.\n[Database file]\n/var/lib/gridstore/ # GridDB home directory data/ # database file directory gs\\_log\\_n_m.log # log file to record the transaction logs (n, m are numbers) gs\\_cp\\_n_p.dat # checkpoint file to record data regularly (n, p are numbers)  [Log file]\n/var/lib/gridstore/ # GridDB home directory log/ # log directory gridstore-%Y%m%d-n.log # event log file gs_XXXX.log # operation tool log file  The directory where these files are created can be changed by the parameter settings in the node definition file.\n*gs_XXXX is an operation tool name. (Example: gs_startnode.log)\nSetting Up an Administrator User\nAdministrator privilege is used for authentication related matter within the nodes and clusters. Creation date of administrator user is saved in the user definition file. The default file is as shown below.\n /var/lib/gridstore/conf/password  The default user below exists immediately after installation.\nTODO: Table User\nPassword\nExample of proper use\nadmin\nadmin\nFor authentication of operation administrator user, operation commands\nsystem\nmanager\nFor authentication of application user, client execution\nAdministrator user information including the above-mentioned default users can be changed using the user administration command in the operating commands.\nTODO: Table Command\nFunction\ngs_adduser\nAdd an administrator user\ngs_deluser\nDelete an administrator user\ngs_passwd\nChange the password of administrator user\nChange the password as shown below when using a default user. The password is encrypted during registration.\n$ gs_passwd admin Password: (enter password) Retype password: (re-enter password)  [Points to note]\n A GridDB administrator user is different from the OS user gsadm created during installation. A change in the administrator user information using a user administration command becomes valid when a node is restarted. In order to use it for authentication purposes in the client, the same user data needs to be registered in all the nodes. Copy the user definition file and make sure the same user data can be referred to in all the nodes. Execute the operating command as a gsadm user.  [Memo]\n See “GridDB Operational Management Guide” (GridDB_OperationGuide.html) for details of the user management commands.  Installation (Community Editon) on a Singular Node This section will layout the steps to installing GridDB. These instructions have been confirmed to work on CentOS version 6.7. First, download the GridDB RPM: here. Switch to the root user and install the RPM by using the “rpm” command:\n$ su \\# rpm -ivh griddb\\_nosql-X.X.X-linux.x86\\_64.rpm Preparing... ########################################### \\[100%\\] Information: User gsadm and group gridstore have been registered. GridDB uses new user and group. 1:griddb_nosql ########################################### \\[100%\\] ```\t### Confirmation After Installation After installing GridDB's node module, the user “gsadm” and the group “gridstore” are created. Use the user and group for running a node module and operational commands. And please note that a password for the new user gsadm is not automatically set upon creation, so please take the time to set one. If installation completed normally, the following directories and files are created as well as necessary modules and commands.  /var/lib/gridstore/ # GridDB home directory backup/ # Backup directory(unused) conf/ # Directory storing definition files gs_cluster.json # Cluster definition file gs_node.json # Node definition file password # User definition file data/ # Directory storing database files log/ # Directory storing event log files\n We should also confirm that the following files and directories exist:  $ ls /var/lib/gridstore/* /var/lib/gridstore/backup:\n/var/lib/gridstore/conf: gs_cluster.json gs_node.json password\n/var/lib/gridstore/data:\n/var/lib/gridstore/log:\n Confirm the directory structure of the installed GridDB client libraries. If installation completed normally, the following files are created.  $ ls -l /usr/share/java/*gridstore* lrwxrwxrwx 1 gsadm gridstore 46 Apr 11 20:43 /usr/share/java/gridstore-conf.jar -\u0026gt; /usr/griddb-X.X.X/bin/gridstore-conf-X.X.X.jar lrwxrwxrwx 1 gsadm gridstore 41 Apr 11 20:43 /usr/share/java/gridstore.jar -\u0026gt; /usr/griddb-X.X.X/bin/gridstore-X.X.X.jar\n If you start a GridDB node and then access and run the node from a client, the following files are created in the directories to store database files and event log files.  /var/lib/gridstore/ # GridDB home directory data/ # Directory storing database files gs_log_n_m.log # File recording transaction logs (n, m: positive number) gs_cp_n_p.dat # Checkpoint file recording data regularly (n, p: positive number)\n The event log file will be in the following location  /var/lib/gridstore/ # GridDB home directory log/ # Directory storing event logs gridstore-%Y%m%d-n.log # Event log file\n ### Setting up an Administrator User After all of this, you must create an administrator user to use GridDB. The administrator user information is stored in the User Definition file. The default file is as shown here: $GS\\_HOME/conf/password. After installation a default user called “admin” will be created. The operating commands used to change the default users’ information are shown below. TODO: Table Command Function gs\\_adduser Add an administrator user gs\\_deluser Delete an administrator user gs\\_passwd Change the password of an administrator user It is recommended that you add a password to the user “admin” as one is not set upon creation.  $ gs_passwd admin Password:（Input password） Retype password:（Input password again）\nIf you’d like to add another user beyond the default “admin” user, it must start with “gs#”  $ gs_adduser gs#newuser Password:（Input password） Retype password:（Input password again） ``` Once the changes you wanted to make are done, you will need to restart the node for the changes to have effect.\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-2_key-container-model/",
	"title": "Key Container Model",
	"tags": [],
	"description": "",
	"content": " GridDB data adopts a Key-Container data model that is expanded from a Key-Value model. Data is stored in a \u0026ldquo;container\u0026rdquo;, which acts similarly to a RDB table. In fact, a container can be considered a RDB table for easier understanding and to help conceptualizing. When accessing data in GridDB, the model allows data to be short-listed with a key thanks to its Key-Value database structure, allowing processing to be carried out at the highest speed. A design that prepares a container serving as a key is required to support the entity under management.\n3.2.1 Collection Container A type of container storing and managing ROWS. A ROW can have a key, but a key is not mandatory. A key can be assigned to a single string or integer (INTEGER or LONG type only), or time-type data. The data housed in this container is typically thought of as more \u0026lsquo;traditional\u0026rsquo; (ie. STRING, BOOLEAN, ARRAY, etc). An example of both calling the data and how it is viewed can be seen below\nList.1 Data retrieving process (WeatherStationLogic.java)\npackage sample;\nimport com.toshiba.mwcloud.gs.Collection; import com.toshiba.mwcloud.gs.GSException; import com.toshiba.mwcloud.gs.GridStore;\nimport sample.logic.GridDBLogic; import sample.logic.WeatherStationLogic; import sample.row.WeatherStation;\npublic class CollectionDeleteRow {\npublic static void main(String\\[\\] args) throws GSException { GridStore store = null; try { WeatherStationLogic wsLogic = new WeatherStationLogic(); // Create Connection store = wsLogic.createGridStore(); // Get Collection Collection weatherStationCol = store.getCollection(\u0026quot;weather_station\u0026quot;, WeatherStation.class);  List.2 Call data retrieving operation(CollectionRetrieve.java)\ntry { System.out.println(\u0026ldquo;ID \\tName \\t \\t \\tLongitude \\tLatitude \\tCamera\u0026rdquo;); for (int i=0; i \u0026lt; WeatherStationLogic.JP_PREFECTURE; i ++) { // Retrieve row by key WeatherStation weatherStation=weatherStationCol.get (String.valueOf (i + 1)); System.out.println (String.format(\u0026ldquo;% - 3s \\t% -20s \\t% -10s \\t% -10s \\t% -5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); } } Finally { // Close Connection weatherStationCol.close (); }\nExecution results are as follows.\nList.3 data acquisition result\nID Name Longitude Latitude Camera 1 Hokkaido-Sapporo 43.06417 141.34694 true 2 Aomori-Aomori 40.82444 140.74 false 3 Iwate-Morioka 39.70361 141.1525 true 4 Miyagi-Sendai 38.26889 140.87194 false 5 Akita-Akita 39.71861 140.1025 true (Snip)\n3.2.2 TimeSeries Container A type of container storing and managing ROWS with a time-type key, provided with a special function to operate TimeSeries data. A key corresponds to the time of a TimeSeries row. Suitable for handling large volumes of TimeSeries data that is generated by sensors. Other values paired with the time of occurrence, space data (such as position information, etc.) can also be registered and space specific operations (space intersection) can also be carried out in a container. A variety of data can be handled as the system supports non-standard data such as array data, BLOB and other data as well. A unique compression function and a function to release data that has expired and so on are provided in a TimeSeries container, making it suitable for the management of data which is generated in large volumes.\nA small example of calling a TimeSeries container and viewing the information can be seen below\nList.1 process of acquiring the specified time (TimeSeriesRetrieve.java)\n// Specify Time InstrumentLog log=logTs.get (format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;)); System.out.println(\u0026ldquo;get by Time\u0026rdquo;); System.out.println (String.format(\u0026ldquo;% s \\ t% -20s \\ t% -10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\nExecution results are as follows.\nList.2 specified time of acquisition results\nget by Time Sat Jul 02 12:00:00 EDT 2016 weather_station_1 80.0\n"
},
{
	"uri": "http://example.org/drivers-and-integrators/7-2_odbc-connector/",
	"title": "ODBC Connector",
	"tags": [],
	"description": "",
	"content": " Operating Environment The ODBC driver used in GridDB AE (Advanced Edition) can be used with the following operating systems:\n Windows Server 2012 R2 Windows Server 2008 R2 Windows 8 Windows 7  Overview of ODBC\nODBC (Open Database Connectivity) is a standard interface for accessing database management systems (DBMS) from Windows-compatible applications advocated by Microsoft\n ODBC-compatible application (Windows) For ODBC functions invoked from an ODBC-compatible application, the OS will control the connection to a specific database of the specified ODBC data source. In addition, management of the data source is also carried out.\n ODBC driver for GridDB AE (Windows) This is a unique driver that can be used to connect to GridDB AE. 32-bit and 64-bit versions are available and should be chosen carefully based on the application.\n GridDB Cluster (Linux) This is a GridDB AE Cluster operating in Linux OS on another machine\n  Installation Method The ODBC driver for GridDB AE is included in the \u0026ldquo;/Windows/ODBC\u0026rdquo; directory of the installation media.\nThe installation method is as follows.\n When installing a 32-bit driver   Select the \u0026ldquo;GridStoreODBC_32bit_setup.bat\u0026rdquo;, right-click, and select \u0026ldquo;Run as administrator\u0026rdquo; Check that GridStoreODBC32.dll has been copied to the program c://Program Files/TOSHIBA/GridStore/bin directory  Registration of ODBC data source In order to access the GridDB AE database using GridDB ODBC, it is necessary to register the ODBC data source in advance. Registration of the ODBC data source is performed with the following procedure:\n For 64-bit   For Windows 7/Windows 2008, start [Data source] (ODBC) from the [Management tools].\nFor Windows 8/Windows 2012, start [ODBS dats source (64-bit)] from the [Management tools]\n Create a system data source (system DSN)\n Set up the connection data to the GridDB cluster when the GridStore ODBC setup screen appears.\n  Input Items  \u0026ldquo;Data source\u0026rdquo;: specify the data source name ot be registered to the ODBC driver manager. The application will specify the data source name specified here to connect to the GridDB cluster. \u0026ldquo;Multicase Address\u0026rdquo;: Specify the multicast address used to connect to the GridDB cluster. Default is 239.0.0.1. Value of \u0026ldquo;sql/notificationAddress\u0026rdquo; in the cluster definition file (gs_cluster.json) of th eGridDB node needs to be specified. \u0026ldquo;Port number\u0026rdquo;: Refers to the port number used to connect to GridDB cluster. Default is 41999. Value of \u0026ldquo;sql/notificationPort\u0026rdquo; in the cluster definittion file (gs_cluster.json) of the GridDB node needs to be specified. \u0026ldquo;Cluster name\u0026rdquo;: Refers to the cluster name of the GridDB cluster. Value of \u0026ldquo;cluster/clusterName\u0026rdquo; in the cluster definition file (gs_cluster.json) of the GridDB node needs to be specified. \u0026ldquo;User Name\u0026rdquo;: Refers to the user name connected to GridDB cluster. \u0026ldquo;Password\u0026rdquo;: Refers to the password of the user mentioned above  Buttons  \u0026ldquo;Test connection\u0026rdquo;: Check the connection to the GridDB cluster. \u0026ldquo;Save\u0026rdquo;: Save the configurations to the registry and close the dialog. \u0026ldquo;Cancel\u0026rdquo;: Discard any changes made.\n For 32-bit\n   For Windows 7/Windows 2008, star the command prompt as adminstrator, and run the 32-bit version of ODBC adminstrator (odbcad32.exe).  Storage location: /windir/SysWOW64/odbcad32.exe\nExample: c:/WINDOWS/SysWOW64.odbcad32.exe\n[Attention]\nIf [Data source (ODBC) is started from the [Management tools] in a 64-bit OS, the 64-bit ODBC adminstrator will be started and it will not be possible to configure the 32-bit settings.\nFor Windows 8/Windows 2012, start [ODBS data source (32-bit)] from the [Management tools].\n Create a system data source (system DSN)   *Select \u0026ldquo;GridStore ODBC (x86)\u0026rdquo; when specifying the driver.\n Set up the connection data to the GridDB server in the GridStore ODBC setup screen. The configuration method is the same as the 64-bit version  *The database at the connection destination will serve as the default public database. A database cannot be specified by the ODBC driver.\n*Only GridDB clusters configured with the multicast method can be connected.\nConnection Method If a business intelligence/ETL tool is used, please specify the data source name set in the \u0026ldquo;GridStore ODBC setup screen\u0026rdquo;.\nIf the ODBC API is used, specify the data source name set in the \u0026ldquo;GridStore ODBC setup screen\u0026rdquo; as well.\nExample: Specify in the second argument for SQLConnect functions.\nSQLConnect(hdbc, (SQLTCHAR *) TEXT(\u0026ldquo;GridStoreODBC-DB1\u0026rdquo;), SQL_NTS, (SQLTCHAR ) TEXT(\u0026ldquo;\u0026rdquo;), SQL_NTS, (SQLTCHAR) TEXT(\u0026ldquo;\u0026rdquo;) SQL_NTS);\nSamples Sample programs in the C language using ODBC API and VisualStudio project files are included in the file \u0026ldquo;gridstore-odbc-sample.zip\u0026rdquo;. (These are stored under Windows in the installation media.)\nThe samples are written and tested (operation check in VisualStudio 2005 SP1)\n GridStoreODBC-sample.sln: VisualStudio solution file ODBC-sample/GridStoreODBC-sample.cpp: Sample Source GridStoreODBC-sample.vcproj: Project file  Try to perform table creation, data registration, and search. The project settings are configured for 64-bit and 32-bit use.\nThe following ODBC data source settings are required to execute a program.\nFor 64-bit\nData source name: GridStoreODBC-test64bit\nFor 32-bit\nData source name: GridStoreODBC-test32bit\n"
},
{
	"uri": "http://example.org/administration/6-2_sizing-and-scaling/",
	"title": "Sizing and Scaling",
	"tags": [],
	"description": "",
	"content": " GridDB is a scale-out database that unlike a conventional DB, careful system design and sizing is not required in order to achieve non-stop operation. However, the following points should be considered as guidelines in the initial system design.\n Memory usage Number of nodes in a cluster Disk usage  The estimation method is explained in sequence below.\nFunctions to increase the capacity by using external storage devices such as SSDs etc. have not been considered in calculating the memory size below. Please check with our service desk for estimation if these functions are used.\nTotal memory usage Predict the amount of data to be stored in the container and then estimate the memory usage.\nFirst, predict the amount of data to be stored in the application. Estimate the following:\n Data size of row Number of rows to be registered  Next, estimate the memory required to store those estimated data.\nMemory capacity used = row data size × no. of registered rows ÷ 0.75 + 8 × no. of registered rows × (assigned index number + 2) ÷ 0.66 (byte)\nPerform a similar estimation for all collections created and used in the application. The total sum becomes the amount of memory required in the GridDB cluster.\n Total memory usage = the sum of memory usage in all collections  However, please consider this number as the minimum requirement, as the memory usage also depends on update frequency.\nNo. of Nodes Constituting a Cluster Estimate the required no. of nodes used in GridDB. In the example below, it is assumed that one node is executed in one machine.\nFirst, make an assumption of the memory required per machine.\n Memory size of machine  In addition, make an assumption of the no. of replicas to create. The no. of replicas is given as a GridDB configuration value.\n No. of replicas  Default no. of replicas is 2.\nNo. of nodes = (Total memory usage ÷ machine memory size) × no. of replicas (units)\nHowever, please consider this as the minimum requirement as it is preferable to have a greater number of spare units to take into account the load distribution and availability improvement.\n"
},
{
	"uri": "http://example.org/drivers-and-integrators/7-3_hadoop-and-spark-connector/",
	"title": "Hadoop and Spark Connector",
	"tags": [],
	"description": "",
	"content": " Overview The Hadoop MapReduce GridDB connector is a Java library for using GridDB as an input source and output destination for Hadoop MapReduce jobs. This library allows the GridDB performance to be used directly by MapReduce jobs through in-memory processing.\nOperating Environment Building of the library and execution of the sample programs are checked in the environment below.\nOS: CentOS6.7(x64) Java: JDK 1.8.0_60 Maven: apache-maven-3.3.9 Hadoop: CDH5.7.1(YARN)\nQuickStart Preparations Build a GridDB Java client and place the created gridstore.jar under the lib directory.\nBuild Run the mvn command like the following: $ mvn package and create the following jar files.\ngs-hadoop-mapreduce-client/target/gs-hadoop-maprduce-client-1.0.0.jar gs-hadoop-mapreduce-examples/target/gs-hadoop-maprduce-examples-1.0.0.jar\nRunning The Sample Program An operating example to run the WordCount program using GridDB is shown below. GridDB and Hadoop (HDFS and YARN) need to be started in advance. Run the following in an environment in which these and hadoop commands can be used.\n$ cd gs-hadoop-mapreduce-examples $ ./exec-example.sh \u0026gt; \u0026ndash;job wordcount \u0026gt; \u0026ndash;define notificationAddress= \u0026gt; \u0026ndash;define notificationPort= \u0026gt; \u0026ndash;define clusterName= \u0026gt; \u0026ndash;define user= \u0026gt; \u0026ndash;define password= \u0026gt; pom.xml 2\u0026gt; /dev/null | sort -r\n5 5 3 org.apache.hadoop 3 com.toshiba.mwcloud.gs.hadoop \u0026hellip;\nThe first number is the number of occurrences while the right side is a word in the file (pom.xml) specified as a processing target. See gs-hadoop-mapreduce-examples/README.md for details about the sample programs.\nSpark Connector The Spark Connector can be downloaded from our downloads page.\nFull list of dependencies:\n OS: CentOS6.7(x64) Maven: apache-maven-3.3.9 Java: JDK 1.8.0_101 Apache Hadoop: Version 2.6.5 Apache Spark: Version 2.1.0 Scala: Version 2.11.8 GridDB server and Java client: 3.0 CE GridDB connector for Apache Hadoop MapReduce: 1.0  If beginning from scratch, I recommend ensuring all of these items are installed and configured. This tutorial also assumes that your Hadoop, Spark, and Connector are all installed in the [INSTALL_FOLDER] directory (I used /opt).\nInstallation Once verified, please proceed with the steps outlined below:\nWe start this process off with adding the following environment variables to .bashrc\n$ nano ~/.bashrc\nexport JAVA_HOME=/usr/lib/jvm/[JDK folder] export HADOOP_HOME=[INSTALL_FOLDER]/hadoop-2.6.5 export SPARK_HOME=[INSTALL_FOLDER]/spark-2.1.0-bin-hadoop2.6 export GRIDDB_SPARK=[INSTALL_FOLDER]/griddb_spark export GRIDDB_SPARK_PROPERTIES=$GRIDDB_SPARK/gd-config.xml\nexport PATH=$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native export HADOOP_OPTS=\u0026ldquo;$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native\u0026rdquo;\n$ source ~/.bashrc\nOnce those are added, modify the gd-config.xml file.\n$ cd $GRIDDB_SPARK $ nano gd-config.xml\n\u0026lt;!-- GridDB properties \u0026ndash;\u0026gt;  gs.user  [GridDB user]    gs.password  [GridDB password]    gs.cluster.name  [GridDB cluster name]   \u0026lt;!-- Define address and port for multicast method, leave it blank if using other method \u0026ndash;\u0026gt;  gs.notification.address  [GridDB notification address(default is 239.0.0.1)]    gs.notification.port  [GridDB notification port(default is 31999)]  \nBuild The Connector + An Example Next up, refer to this configuration page for a quick definition of each of the GridDB properties.\nTo build a GridDB Java client and a GridDB connector for Hadoop MapReduce, place the following files under the $GRIDDB_SPARK/gs-spark-datasource/lib directory.\ngridstore.jar gs-hadoop-mapreduce-client-1.0.0.jar\n(Note: these .jar files should have been created when you built your GridDB client and the GridDB Mapreduce Connector. You can find gridstore.jar in /usr/griddb-X.X.X/bin, for example)\nOnce that\u0026rsquo;s complete, add the SPARK_CLASSPATH to \u0026ldquo;spark-env.sh\u0026rdquo;\n$ cd $SPARK_HOME $ nano conf/spark-env.sh\nSPARK_CLASSPATH=.:$GRIDDB_SPARK/gs-spark-datasource/target/gs-spark-datasource.jar:$GRIDDB_SPARK/gs-spark-datasource/lib/gridstore.jar:$GRIDDB_SPARK/gs-spark-datasource/lib/gs-hadoop-mapreduce-client-1.0.0.jar\nNow that we\u0026rsquo;ve got the prerequisites out of the way, we can continue on to build the connector and an example to ensure everything is working properly.\nTo begin, we will need to edit our Init.java file to add the correct authentication credientials.\n$ cd $SPARK_HOME/gs-spark-datasource-example/src/ $ nano Init.java\nAnd add in your credentials:\nProperties props = new Properties(); props.setProperty(\u0026ldquo;notificationAddress\u0026rdquo;, \u0026ldquo;239.0.0.1\u0026rdquo;); props.setProperty(\u0026ldquo;notificationPort\u0026rdquo;, \u0026ldquo;31999\u0026rdquo;); props.setProperty(\u0026ldquo;clusterName\u0026rdquo;, \u0026ldquo;Spark-Cluster\u0026rdquo;); props.setProperty(\u0026ldquo;user\u0026rdquo;, \u0026ldquo;admin\u0026rdquo;); props.setProperty(\u0026ldquo;password\u0026rdquo;, \u0026ldquo;hunter2\u0026rdquo;); GridStore store = GridStoreFactory.getInstance().getGridStore(props);\nAnd now we can run the mvn command like so:\n$ cd $GRIDDB_SPARK $ mvn package\nwhich will create the following .jar files:\ngs-spark-datasource/target/gs-spark-datasource.jar gs-spark-datasource-example/target/example.jar\nNow proceed with running the example program. First start your GridDB cluster. And then:\nPut some data into the server with the GridDB Java client\n$ cd $GRIDDB_SPARK $ java -cp ./gs-spark-datasource-example/target/example.jar:gs-spark-datasource/lib/gridstore.jar Init\nQueries Now you can run queries with your GridDB connector for Spark:\n$ spark-submit \u0026ndash;class Query ./gs-spark-datasource-example/target/example.jar\nWe will go over some brief examples of Apache Spark\u0026rsquo;s API. Examples are pulled from the official page.\nSpark\u0026rsquo;s defining feature is its RDD (Resilient Distributed Datasets) and the accompanying API. RDDs are immutable data structures that can be run in parallel on commodity hardware \u0026ndash; essentially it is exactly what allows Spark to run its queries in parallel and outperform MapReduce. Here\u0026rsquo;s a very basic example; it will showcase how to build an RDD of the numbers 1 - 5\nList data = Arrays.asList(1, 2, 3, 4, 5); JavaRDD distData = sc.parallelize(data);\nWith this, you can now run that small array in parallel. Pretty cool, huh?\nCommand Line Query A \u0026ldquo;must-run\u0026rdquo; query in the Big Data scene is running a word count, so here\u0026rsquo;s what it looks like on Spark. For this example, let\u0026rsquo;s try using the shell (example taken from: here). To run this, please be sure you place a text file input.txt into your $GRIDDB_SPARK directory. Fill it with whatever text you like; I used the opening chapter of Moby Dick . Now fire up the spark shell:\n$ spark-shell\n\nscala\u0026gt; val inputfile = sc.textFile (\u0026ldquo;input.txt\u0026rdquo;) inputfile: org.apache.spark.rdd.RDD[String] = input.txt MapPartitionsRDD[1] at textFile at :24\nscala\u0026gt; val counts = inputfile.flatMap (line =\u0026gt; line.split (\u0026rdquo; \u0026ldquo; )).map (word =\u0026gt; (word, 1)).reduceByKey(+) counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at :26\nscala\u0026gt; counts.saveAsTextFile (\u0026ldquo;output\u0026rdquo;)\nAnd now if you head back into $GRIDDB_SPARK, you should find the output dir. Now just run a simple cat on the file in there to retrieve the word count results of your text file.\n$ cd $GRIDDB_SPARK $ cd output $ cat part-00000 (Ah!,1) (Let,1) (dreamiest,,1) (dotings,1) (cooled,1) (spar,1) (previous,2) (street,,1) (old,6) (left,,1) (order,2) (told,1) (marvellous,,1) (Now,,1) (virtue,1) (Take,1)\nTS Query Of course, Spark is also capable of handling much more complex queries. Because GridDB ideally deals mostly in TimeSeries (TS) data, how about we take a look into a TS query? Here\u0026rsquo;s a sample query taken from here:\nval tsRdd: TimeSeriesRDD = \u0026hellip;\n// Find a sub-slice between two dates val zone = ZoneId.systemDefault() val subslice = tsRdd.slice( ZonedDateTime.of(LocalDateTime.parse(\u0026ldquo;2015-04-10T00:00:00\u0026rdquo;), zone) ZonedDateTime.of(LocalDateTime.parse(\u0026ldquo;2015-04-14T00:00:00\u0026rdquo;), zone))\n// Fill in missing values based on linear interpolation val filled = subslice.fill(\u0026ldquo;linear\u0026rdquo;)\n// Use an AR(1) model to remove serial correlations val residuals = filled.mapSeries(series =\u0026gt; ar(series, 1).removeTimeDependentEffects(series))\nLicense The Hadoop MapReduce GridDB connector source license is Apache License, version 2.0.\n"
},
{
	"uri": "http://example.org/administration/6-3_installing/",
	"title": "Installing",
	"tags": [],
	"description": "",
	"content": " The following chapter will discuss how to install GridDB (Standard Edition). Instructions for installing on the cloud can be found below\nOn Premises The following 3 RPM packages are needed when installing a GridDB node. Place these packages anywhere in the machine.\nPackage name\nFile name\nDescription\ngriddb-server\ngriddb-server-X.X.X-linux.x86_64.rpm\nThe start and other commands for the GridDB node module and server are included.\ngriddb-client\ngriddb-client-X.X.X-linux.x86_64.rpm\nOne set of operating commands except start node is included.\ngriddb-docs\ngriddb-docs-X.X.X-linux.x86_64.rpm\nGridDB manual and program samples are included.\n*: X.X.X is the GridDB version\nInstall using the rpm command as a root user.\n$ su # rpm -Uvh griddb-server-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] User gsadm and group gridstore have been registered. GridStore uses new user and group. 1:griddb-server ########################################### [100%] # rpm -Uvh griddb-client-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] User and group has already been registered correctly. GridStore uses existing user and group. 1:griddb-client ########################################### [100%] # rpm -Uvh griddb-docs-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-docs ########################################### [100%]\nWhen you install the package, the following group and user are created in the OS. This OS user is set as the operator of GridDB.\nGroup\nUser\nHome directory\ngridstore\ngsadm\n/var/lib/gridstore\nThe following environment variables are defined in this gsadm user.\nEnvironment variables\nValue\nMeaning\nGS_HOME\n/var/lib/gridstore\ngsadm/GridDB home directory\nGS_LOG\n/var/lib/gridstore/log\nEvent log file output directory\n[Points to note]\n These environment variables are used in the operating commands described later. The password of the gsadm user has not been set. With the root user privilege, please set the password appropriately.  Some of the functions of the operation tools may be necessary.   In addition, when the GridDB node module is installed, services that are executed automatically upon startup of the OS will be registered.\nService name\nRun level\ngridstore\n3,4,5\nThe service registration data can be checked with the following command.\n# /sbin/chkconfig \u0026ndash;list | grep gridstore gridstore 0:off 1:off 2:off 3:on 4:on 5:on 6:off\nThe GridDB node will be started automatically by this service during OS startup.\n[Points to note]\n Services will not start automatically immediately after installation.  To stop auto startup of a service, use the command below.\n# /sbin/chkconfig gridstore off\nSee the chapter on services in “GridDB Operational Management Guide” (GridDB_OperationGuide.html) for details of the services.\nPost-installation checks Check the directory configuration of the installed GridDB node.\nFirst, check that the GridDB home directory, and related directory and files have been created.\nGridDB home directory\n/var/lib/gridstore/ # GridDB home directory admin/ # integrated operational management GUI home directory backup/ # backup directory conf/ # definition file directory gs_cluster.json # cluster definition file gs_node.json # node definition file password # user definition file data/ # database file directory log/ # log directory\nConfirm with the following command.\n$ ls /var/lib/gridstore/ admin backup conf data log\nNext, check that the installation directory has been created\nInstallation directory\n/usr/gridstore-X.X.X/ # installation directory Fixlist.pdf # revision record Readme.txt # release instructions bin/ # operation command, module directory conf/ # definition file sample directory docs/ # document directory etc/ lib/ # library directory license/ # license directory prop/ # configuration file directory web/ # integrated operational management GUI file directory\nConfirm with the following command.\n$ ls /usr/gridstore-X.X.X/ Fixlist.pdf Readme.txt bin conf etc lib license prop web\nAll documents have been compressed into a single ZIP file. Decompress and refer to the documents where appropriate as shown below.\n$ cd /usr/gridstore-X.X.X/docs $ unzip griddb-documents-X.X.X.zip\nIn addition, the following symbolic links are created as shown below in a few directories under the installation directory for greater convenience.\n$ ls /usr/gridstore/ conf lib prop web\nLastly, confirm the version of the server module installed with the following command.\n$ gsserver \u0026ndash;version GridDB version X.X.X build XXXXX\nPoints to note\nThe following files are created when GridDB is operated according to the following procedure.\n[Database file]\n/var/lib/gridstore/ # GridDB home directory data/ # database file directory gs_log_n_m.log # log file to record the transaction logs (n, m are numbers) gs_cp_n_p.dat # checkpoint file to record data regularly (n, p are numbers)\n[Log file]\n/var/lib/gridstore/ # GridDB home directory log/ # log directory gridstore-%Y%m%d-n.log # event log file gs_XXXX.log # operation tool log file\nThe directory where these files are created can be changed by the parameter settings in the node definition file.\n*: gs_XXXX is an operation tool name. (Example: gs_startnode.log)\nSetting up an Administrator User Administrator privilege is used for authentication related matter within the nodes and clusters. Creation date of administrator user is saved in the user definition file. The default file is as shown below.\n /var/lib/gridstore/conf/password  The default user below exists immediately after installation.\nUser\nPassword\nExample of proper use\nadmin\nadmin\nFor authentication of operation administrator user, operation commands\nsystem\nmanager\nFor authentication of application user, client execution\nAdministrator user information including the above-mentioned default users can be changed using the user administration command in the operating commands.\nCommand\nFunction\ngs_adduser\nAdd an administrator user\ngs_deluser\nDelete an administrator user\ngs_passwd\nChange the password of administrator user\nChange the password as shown below when using a default user. The password is encrypted during registration.\n$ gs_passwd admin Password: (enter password) Retype password: (re-enter password)\nWhen adding a new administrator user except a default user, the user name has to start with gs#.\nOne or more ASCII alphanumeric characters and the underscore sign “_” can be used after gs#.\nAn example on adding a new administrator user is shown below.\n$ gs_adduser gs#newuser Password: (enter password) Retype password: (re-enter password)\n[Points to note]\n A GridDB administrator user is different from the OS user gsadm created during installation. A change in the administrator user information using a user administration command becomes valid when a node is restarted. In order to use it for authentication purposes in the client, the same user data needs to be registered in all the nodes. Copy the user definition file and make sure the same user data can be referred to in all the nodes. Execute the operating command as a gsadm user.  [Memo]\n See “GridDB Operational Management Guide” (GridDB_OperationGuide.html) for details of the user management commands.  Setting the environment-dependent parameters After installation, the following settings are necessary in order to operate GridDB.\n Network environment settings Cluster name settings  GridDB settings are configured by editing 2 types of definition files.\n Cluster definition file (gs_cluster.json) Node definition file (gs_node.json)  The cluster definition file defines the parameters that are common in the entire clusters.\nThe node definition files define the parameters for the different settings in each node.\nThese definition file samples are installed as follows.\n/usr/gridstore/ # installation directory conf/ # definition file directory gs_cluster.json # cluster definition file sample gs_node.json # node definition file sample\nIn a new installation, the same files are also placed in the conf directory under the GridDB home directory.\n/var/lib/gridstore/ # GridDB home directory conf/ # definition file directory gs_cluster.json # (edited) cluster definition file gs_node.json # (edited) node definition file\nDuring operations, edit these definition files.\n[Points to note]\n When the GridDB version is upgraded, compare the newly installed sample with these definition files to adequately reflect the parameters added. A cluster definition file defines the parameters that are common in the entire clusters. As a result, the settings must be the same in all of the nodes in the cluster. Nodes with different settings will get an error upon joining the cluster and prevented from participating in the cluster. Further details will be explained in the later chapter.  Network environment settings (essential) First, set up the network environment.\nAn explanation of the recommended configuration method in an environment that allows a multicast to be used is given below. In an environment which does not allow a multicast to be used, or an environment in which communications between fellow nodes cannot be established in a multicast, a cluster configuration method other than the multicast method has to be used. See Other cluster configuration method settings for the details.\nThe configuration items can be broadly divided as follows.\n (1) Address information serving as an interface with the client (2) Address information for cluster administration and processing (3) Address information serving as an interface with the JDBC client (GridDB Advanced Edition only)  Although these settings need to be set to match the environment, basically default settings will also work.\nHowever, an IP address derived in reverse from the host name of the machine needs to be an address that allows it to be connected from the outside regardless of whether the GridDB cluster has a multiple node configuration or a single node configuration.\nNormally, this can be set by stating the host name and the corresponding IP address in the /etc/hosts file.\n/etc/hosts setting\nFirst, check with the following command to see whether the setting has been configured. If the IP address appears, it means that the setting has already been configured.\n$ hostname -i 192.168.11.10\nThe setting has not been configured in the following cases.\n$ hostname -i hostname: no address corresponding to name\nIn addition, a loopback address that cannot be connected from the outside may appear.\n$ hostname -i 127.0.0.1\nIf the setting has not been configured or if a loopback address appears, use the following example as a reference to configure /etc/hosts. The host name and IP address, and the appropriate network interface card (NIC) differ depending on the environment.\n Check the host name and IP address.\n$ hostname GS_HOST $ ip route | grep eth0 | cut -f 12 -d \u0026ldquo; \u0026rdquo; | tr -d \u0026ldquo;\\n\u0026rdquo; 192.168.11.10\n Add the IP address and corresponding host name checked by the root user to the /etc/hosts file.\n192.168.11.10 GS_HOST\n Check that the settings have been configured correctly.\n$ hostname -i 192.168.11.10\n  *If the displayed setting remains the same as before, it means that a setting higher in priority is given in the /etc/hosts file. Change the priority order appropriately.\nProceed to the next setting after you have confirmed that /etc/hosts has been configured correctly.\n(1) Address information serving as an interface with the client\nIn the address data serving as an interface with the client, there are configuration items in the node definition file and cluster definition file.\nNode definition file\nParameters\nData type\nMeaning\n/transaction/serviceAddress\nstring\nReception address of transaction process\n/transaction/servicePort\nstring\nReception port of transaction process\n/system/serviceAddress\nstring\nConnection address of operation command\n/system/servicePort\nstring\nConnection port of operation command\nThe reception address and port of transaction processes are used to connect individual client to the nodes in the cluster, and to request for the transaction process from the cluster. This address is used when configuring a cluster with a single node, but in the case where multiple nodes are present through API, the address is not used explicitly.\nThe connection address and port of the operating command are used to specify the process request destination of the operation command, as well as the repository information of the integrated operation control GUI.\nThese reception/connection addresses need not be set so long as there is no need to use/separate the use of multiple interfaces.\nCluster definition file\nParameters\nData type\nMeaning\n/transaction/notificationAddress\nstring\nInterface address between client and cluster\n/transaction/notificationPort\nstring\nInterface port between client and cluster\nA multi-cast address and port are specified in the interface address between a client and cluster. This is used by a GridDB cluster to send cluster information to its clients and for the clients to send processing requests via the API to the cluster. See the description of GridStoreFactory class/method in “GridDB API reference” (GridDB_API_Reference.html) for details.\nIt is also used as a connection destination address of the export/import tool, or as repository data of the integrated operation control GUI.\n(2) Address information for cluster administration and processing\nIn the address data for a cluster to autonomously perform cluster administration and processing, there are configuration items in the node definition file and cluster definition file. These addresses are used internally by GridDB to exchange the heart beat (live check among clusters) and information among the clusters. These settings are not necessary so long as the address used is not duplicated with other systems on the same network or when using multiple network interface cards.\nNode definition file\nParameters\nData type\nMeaning\n/cluster/serviceAddress\nstring\nReception address used for cluster administration\n/cluster/servicePort\nstring\nReception port used for cluster administration\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationAddress\nstring\nMulticast address for cluster administration\n/cluster/notificationPort\nstring\nMulticast port for cluster administration\n Although a synchronization process is carried out with a replica when the cluster configuration is changed, a timeout time can be set for the process.  /sync/timeoutInterval   [Points to note]\n An address or port that is not in use except in GridDB has to be set. The same address can be set for the node definition file gs_node.json /transaction/serviceAddress, /system/serviceAddress, and /cluster/serviceAddress for operations to be performed. If a machine has multiple network interfaces, the bandwidth can be increased by assigning a separate address to each respective interface.  The following settings are applicable in the GridDB Advanced Edition only.\n(3) Address information serving as an interface with the JDBC client\nIn the address data serving as an interface with the JDBC/ODBC client, there are configuration items in the node definition file and cluster definition file.\nNode definition file\nParameters\nData type\nMeaning\n/sql/serviceAddress\nstring\nReception address for JDBC/ODBC client connection\n/sql/servicePort\nint\nReception port for JDBC/ODBC client connection\nThe reception address and port of JDBC/ODBC client connection are used to connect JDBC/ODBC individual client to the nodes in the cluster, and to access the cluster data in SQL. This address is used when configuring a cluster with a single node, but in the case where multiple nodes are present through API, the address is not used explicitly.\nCluster definition file\nParameters\nData type\nMeaning\n/sql/notificationAddress\nstring\nAddress for multi-cast distribution to JDBC/ODBC client\n/sql/notificationPort\nint\nMulticast port to JDBC/ODBC client\nThe address and port used for multicast distribution to a JDBC/ODBC client are used for the GridDB cluster to notify the JDBC/ODBC client of cluster data, and to access the cluster data in SQL with the JDBC/ODBC client.\nRefer to Annex Parameter List for the other parameters and default values.\nCluster name settings (essential) Set the name of the cluster to be composed by the target nodes in advance. The name set will be checked to see if it matches the value specified in the command to compose the cluster. As a result, this prevents a different node and cluster from being composed when there is an error in specifying the command.\nThe cluster name is specified in the following configuration items of the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/clusterName\nstring\nName of cluster to create\n[Points to note]\n Node failed to start with default value (\u0026ldquo;\u0026rdquo;). A unique name on the sub-network is recommended. A cluster name is a string composed of 1 or more ASCII alphanumeric characters and the underscore “_”. However, the first character cannot be a number. The name is also not case-sensitive. In addition, it has to be specified within 64 characters.  Settings of other cluster configuration methods In an environment which does not allow the multicast method to be used, configure the cluster using the fixed list method or provider method. An explanation of the respective network settings in the fixed list method and provider method is given below.\nWhen using the multicast method, proceed to Setting the tuning parameters.\n(1) Fixed list method\nWhen a fixed address list is given to start a node, the list is used to compose the cluster.\nWhen composing a cluster using the fixed list method, configure the parameters in the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationMember\nstring\nSpecify the address list when using the fixed list method as the cluster configuration method.\nA configuration example of a cluster definition file is shown below.\n{ : : \u0026ldquo;cluster\u0026rdquo;:{ \u0026ldquo;clusterName\u0026rdquo;:\u0026ldquo;yourClusterName\u0026rdquo;, \u0026ldquo;replicationNum\u0026rdquo;:2, \u0026ldquo;heartbeatInterval\u0026rdquo;:\u0026ldquo;5s\u0026rdquo;, \u0026ldquo;loadbalanceCheckInterval\u0026rdquo;:\u0026ldquo;180s\u0026rdquo;, \u0026ldquo;notificationMember\u0026rdquo;: [ { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} } ] }, : : }\n(2) Provider method\nGet the address list supplied by the address provider to perform cluster configuration.\nWhen composing a cluster using the provider method, configure the parameters in the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationProvider/url\nstring\nSpecify the URL of the address provider when using the provider method as the cluster configuration method.\n/cluster/notificationProvider/updateInterval\nstring\nSpecify the interval to get the list from the address provider. Specify a value that is 1s or higher and less than 2^31s.\nA configuration example of a cluster definition file is shown below.\n{ : : \u0026ldquo;cluster\u0026rdquo;:{ \u0026ldquo;clusterName\u0026rdquo;:\u0026ldquo;yourClusterName\u0026rdquo;, \u0026ldquo;replicationNum\u0026rdquo;:2, \u0026ldquo;heartbeatInterval\u0026rdquo;:\u0026ldquo;5s\u0026rdquo;, \u0026ldquo;loadbalanceCheckInterval\u0026rdquo;:\u0026ldquo;180s\u0026rdquo;, \u0026ldquo;notificationProvider\u0026rdquo;:{ \u0026ldquo;url\u0026rdquo;:\u0026ldquo;http://example.com/notification/provider\u0026quot;, \u0026ldquo;updateInterval\u0026rdquo;:\u0026ldquo;30s\u0026rdquo; } }, : : }\nThe address provider can be configured as a Web service or as a static content. The specifications below need to be satisfied.\n Compatible with the GET method. When accessing the URL, the node address list of the cluster containing the cluster definition file in which the URL is written is returned as a response.  Response body: Same JSON as the contents of the node list specified in the fixed list method Response header: Including Content-Type:application/json   An example of a response sent from the address provider is as follows.\n$ curl http://example.com/notification/provider [ { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} } ]\n[Memo]\n Specify the serviceAddress and servicePort of the node definition file in each module (cluster,sync etc.) for each address and port. sql items are required in the GridDB Advanced Edition only. Set either the /cluster/notificationAddress, /cluster/notificationMember, /cluster/notificationProvider in the cluster definition file to match the cluster configuration method used. See “GridDB technical reference” (GridDB_TechnicalReference.html) for details on the cluster configuration method.  Installation and setup (client) This chapter explains the installation procedure of the client library. There are 2 types of client library in GridDB, Java and C. Only the Java version supports the GridDB Advanced Edition NewSQL interface.\n[Points to note]\n When choosing the OS package group during OS installation, please choose the minimum version or lower.  Software Development WorkStation   The following needs to be installed as a Java language development environment.\n Oracle Java 6/7/8 Only 64-bit Java is supported by the GridDB Advanced Edition NewSQL interface.  $ java -version java version \u0026ldquo;1.7.0_79\u0026rdquo; Java\u0026trade; SE Runtime Environment (build 1.7.0_79-b15) Java HotSpot\u0026trade; 64-Bit Server VM (build 24.79-b02, mixed mode)\nInstalling the client library The following 4 RPM packages are required for the installation of the GridDB client library. Place the packages anywhere in the machine.\nThe griddb-newsql package is only available in GridDB Advanced Edition.\nPackage name\nFile name\nDescription\ngriddb-java_lib\ngriddb-java_lib-X.X.X-linux.x86_64.rpm\nJava library is included.\n(/usr/share/java/gridstore.jar)\ngriddb-c_lib\ngriddb-c_lib-X.X.X-linux.x86_64.rpm\nC header file and library are included.\n(/usr/include/gridstore.h and /usr/lib64/libgridstore.so)\ngriddb-docs\ngriddb-docs-X.X.X-linux.x86_64.rpm\nGridDB manual and program samples are included.\ngriddb-newsql\ngriddb-newsql-X.X.X-linux.x86_64.rpm\nNewSQL interface library is included.\nInstall using the rpm command as a root user.\n$ su # rpm -ivh griddb-c_lib-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-c_lib ########################################### [100%] # rpm -ivh griddb-java_lib-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-java_lib ########################################### [100%] # rpm -ivh griddb-docs-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-docs ########################################### [100%] # rpm -ivh griddb-newsql-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-newsql ########################################### [100%]\nSetting up a library When the Java version of the client is used, add the client library to CLASSPATH.\n$ export CLASSPATH=${CLASSPATH}:/usr/share/java/gridstore.jar\nWhen the C version is used instead, add the client library to LD_LIBRARY_PATH.\n$ export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/lib64\n"
},
{
	"uri": "http://example.org/getting-started/2-3_run-your-first-griddb-node-cluster/",
	"title": "Run your First GridDB Node/Cluster",
	"tags": [],
	"description": "",
	"content": " Quickstart We will be taking a quick look at running your first GridDB node/cluster. Please take a look at the example below.\nStart a Server $ export GS_HOME=$PWD $ export GS_LOG=$PWD/log  $ bin/gs_passwd admin #input your_password $ vi conf/gs_cluster.json #\u0026quot;clusterName\u0026quot;:\u0026quot;your\\_clustername\u0026quot; #\u0026lt;-- input your\\_clustername $ export no_proxy=127.0.0.1 $ bin/gs_startnode $ bin/gs\\_joincluster -c your\\_clustername -u admin/your_password  "
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-3_tql/",
	"title": "TQL",
	"tags": [],
	"description": "",
	"content": "TQL in NoSQL products and SQL-92 compliant SQL in NewSQL products are supported as database access languages.\n What is TQL?\nA simplified version of SQL prepared for NoSQL products. The support range is limited to functions such as search, aggregation, etc., using a container as a unit. TQL is employed by using the client API (Java, C language) of NoSQL products.\n What is SQL?\nSQL stands for \u0026ldquo;Structured Query Language\u0026rdquo;, and it has been the standard query language for RDMS systems for years. Standardization of the language specifications is carried out in ISO to support the interface for defining and performing data operations in conformance with SQL-92 in GridDB. SQL uses the ODBC/JDBC of the new SQL product.\n  GridDB has a NoSQL interface to access containers and a NewSQL interface to access tables. The GridDB Standard Edition supports the NoSQL interface while the GridDB Advanced Edition supports both NoSQL and NewSQL interfaces. The range of access to tables and containers using these interfaces are shown below.\n The NoSQL interface provides access to containers but not tables. The NewSQL interface provides access to tables but not containers.  More Information on TQL\nTQL supports a query corresponding to the SQL SELECT statement which is required to select data to be fetched, deleted or updated. It does not support management of data structure and transaction processing other than a selection query (such as manipulation of selected data).\nAll queries are expressed by the syntax below:\n[EXPLAIN [ANALYZE]] SELECT (select expression) [FROM (Collection or TimeSeries name)] [WHERE (conditional expression)] ORDER BY (Column name) [ASC|DESC] [, (Column name) [ASC|DESC]]* [LIMIT (number) [OFFSET (number)]]\nA SELECT statement is used to narrow down Rows in a Collection or TimeSeries specified in the FROM clause according to the conditional expression in the WHERE clause and process the result set according to the select expression specifing target Column(s), a calculation formula, etc.\nIf a target Collection or TimeSeries is already specified, you need to omit the FROM clause or specify the same name as the target in the FROM clause. You should note that the FROM clause is case-insensitive. If the WHERE clause is omitted, all the Rows of a target Collection or TimeSeries are selected.\nYou can place EXPLAIN or EXPLAIN ANALYZE before a SELECT statement to obtain execution plan information and analysis information on execution results in relation to the SELECT statement. See the later section for more information.\nUnlike SQL, you cannot extract only specific Column(s) except for aggregation operations. Additionally, clauses corresponding to the following are not available.\n GROUP BY HAVING DISTINCT FOR UPDATE ※can be carried out with the API JOIN  ASCII characters in the keywords of the basic syntax and the names of functions, operators and enumeration constants described in the later sections can be written in lower case.\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/",
	"title": "Technical Architecture and Guide",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Technical Architecture and Guide A technical guide to GridDB\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-4_crud-operations/",
	"title": "CRUD Operations",
	"tags": [],
	"description": "",
	"content": " CRUD operations are present in all databases as the foundational actions that allow the most basic of actions. GridDB\u0026rsquo;s CRUD operations are most easily executed by using its very own Java-based API, though it does also accept TQL.\nWhat is CRUD?\n Create: writing new data (containers, rows, etc) Read: viewing, or \u0026ldquo;pulling up\u0026rdquo; any data Update: to modify already existing data (as opposed to writing NEW data) to reflect changes Delete: erasing or removing data from a container or row  Native API GridDB can be accessed using TQL or through its native API. The native API willl be looked at first, followed by some examples of use with TQL.\nCreating a Container\nContainers can be made easily by defining the data as a class.\n// Create Collection Collection weatherStationCol = store.putCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class); return weatherStationCol;\n34-35 line: create the collection by using the GridStore.putCollection (String, Class) method. The String Specifies the name of the container. The Class specifies the WeatherStation class that was created in the schema definition.\n**\nRegister the Data\n**\nReady access to GridDB in the above process is now ready. Let\u0026rsquo;s try to register the data in GridDB.\n// Set the value to Row data WeatherStation weatherStation1 = new WeatherStation (); weatherStation1.id = \u0026ldquo;1\u0026rdquo;; weatherStation1.name = \u0026ldquo;WeatherStation 01\u0026rdquo;; weatherStation1.latitude = 35.68944; weatherStation1.longitude = 139.69167; weatherStation1.hasCamera = true;\nWeatherStation weatherStation2 = new WeatherStation (); weatherStation2.id = \u0026ldquo;2\u0026rdquo;; weatherStation2.name = \u0026ldquo;WeatherStation 02\u0026rdquo;; weatherStation2.latitude = 35.02139; weatherStation2.longitude = 135.75556; weatherStation2.hasCamera = false;\n// Register Collection weatherStationCol.put (weatherStation1); weatherStationCol.put (weatherStation2);\n* 30-42 line: Set the values of the data to be registered.\n* 45-46 line: Pack data and register it in the container\nRead The Data (Retrieval)\nNow the registered data can be retrieved from the GridDB server.\nList.6 data acquisition process (FirstGridDB.java)\n// Retrieve Collection System.out.println(\u0026ldquo;get by key\u0026rdquo;); System.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); weatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\nfor (int i = 0; i \u0026lt;2; i ++) { WeatherStation weatherStation = weatherStationCol.get (String.valueOf (i + 1)); System.out.println (String.format(\u0026ldquo;% - 3s\\t% -20s\\t% s\\t% s\\t% -5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); }\n* 51 line: First get the container by specifying the container name and class.\n* 54 line: Then get row data by specifying the key.\nHere is the output:\nList.7 data acquisition result\nget by key ID Name Longitude Latitude Camera 1 WeatherStation 01 35.68944 139.69167 true 2 WeatherStation 02 35.02139 135.75556 false\n**\nDeleting A Container\n**\nThe command to delete a container looks like this:\nContainer.dropCollection (String)\nTQL Put simply, TQL is a simplified SQL prepared for NoSQL products. The support range is limited to functions such as search, aggregation, etc. TQL is employed by using the client API (Java, C language). To register and search for data in GridDB, a container or table (NewSQL products only) needs to be created to store the data. This section describes the data types that can be registered in a container or table, data size, index and data management functions.\nThe naming rules for containers and tables are the same as those for databases.\n A string consisting of alphanumeric characters and the underscore mark can be specified. However, the first character cannot be a number. Although the case sensitivity of the name is maintained, a container (table) which has the same name when it is not case-sensitive cannot be created.  Container Creation\nContainer (collection)\ncreatecollection Container name Column name Type [Column name Type …] Container (time series container)\ncreatetimeseries Container name Compression method Column name type [Column name Type …]\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the name of the container to be created. If the name is omitted in the createcontainer command, a container with the name given in the container data file will be created.\nColumn name\nSpecify the column name.\nType\nSpecify the column type.\nCompression method\nFor TimeSeries data, specify the data compression method.\nContainer definition file\nSpecify the file storing the container data in JSON format.\nDetailed version\nSpecify the container definition data in the json file to create a container.\n The container definition data has the same definition as the metadata file output by the export tool. See Metadata files with the container data file format for the column type and data compression method, container definition format, etc. However, the following data will be invalid in this command even though it is defined in the metadata file of the export command.  version Export tool version database Database name containerFileType Export data file type containerFile Export file name partitionNo Partition no.  Describe a single container definition in a single container definition file. If the container name is omitted in the argument, create the container with the name described in the container definition file. If the container name is omitted in the argument, ignore the container name in the container definition file and create the container with the name described in the argument. An error will not result even if the database name is described in the container definition file but the name will be ignored and the container will be created in the database currently being connected. When using the container definition file, the metadata file will be output when the \u0026ndash;out option is specified in the export function. The output metadata file can be edited and used as a container definition file.   　Example: When using the output metadata file as a container definition file\n{ \u0026ldquo;version\u0026rdquo;:\u0026ldquo;2.1.00\u0026rdquo;,　←invalid \u0026ldquo;container\u0026rdquo;:\u0026ldquo;container_354\u0026rdquo;, \u0026ldquo;database\u0026rdquo;:\u0026ldquo;db2\u0026rdquo;,　←invalid \u0026ldquo;containerType\u0026rdquo;:\u0026ldquo;TIME_SERIES\u0026rdquo;, \u0026ldquo;containerFileType\u0026rdquo;:\u0026ldquo;binary\u0026rdquo;,　←invalid \u0026ldquo;containerFile\u0026rdquo;:\u0026ldquo;20141219_114232_098_div1.mc\u0026rdquo;, ←invalid \u0026ldquo;rowKeyAssigned\u0026rdquo;:true, \u0026ldquo;partitionNo\u0026rdquo;:0,　←invalid \u0026ldquo;columnSet\u0026rdquo;:[ { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;timestamp\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;timestamp\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;active\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;boolean\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;voltage\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;double\u0026rdquo; } ], \u0026ldquo;timeSeriesProperties\u0026rdquo;:{ \u0026ldquo;compressionMethod\u0026rdquo;:\u0026ldquo;NO\u0026rdquo;, \u0026ldquo;compressionWindowSize\u0026rdquo;:-1, \u0026ldquo;compressionWindowSizeUnit\u0026rdquo;:\u0026ldquo;null\u0026rdquo;, \u0026ldquo;expirationDivisionCount\u0026rdquo;:8, \u0026ldquo;rowExpirationElapsedTime\u0026rdquo;:-1, \u0026ldquo;rowExpirationTimeUnit\u0026rdquo;:\u0026ldquo;null\u0026rdquo; }, \u0026ldquo;compressionInfoSet\u0026rdquo;:[ ]\nContainer Deletion\n Example:\ngs[public]\u0026gt; dropcontainer　Con001\n  Container Indication\nThe following command is used to display the container data.\n Sub-command\nshowcontainer\nContainer name\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the container name to be displayed. Display a list of all containers if omitted.\n Example:\n// display container list gs[userDB]\u0026gt; showcontainer Database : userDB Name Type PartitionId ------------------------------------------------ cont001 COLLECTION 10 col00a COLLECTION 3 time02 TIME_SERIES 5 cont003 COLLECTION 15 cont005 TIME_SERIES 17\n// display data of specified container gs[public]\u0026gt; showcontainer cont003 Database : userDB Name : cont003 Type : COLLECTION Partition ID: 15 DataAffinity: -\nColumns: No Name Type Index ------------------------------------------------------------ 0 col1 INTEGER [TREE] (RowKey) 1 col2 STRING [] 2 col3 TIMESTAMP []\n  [Memo]\n The data displayed in a container list are the “Container name”, “Container type” and “Partition ID”. The data displayed in the specified container are the “Container name”, “Container type”, “Partition ID”, “Defined column name”, “Column data type” and “Column index setting”. Container data of the current DB will be displayed.  Container ROWKEY\nA ROWKEY is the data set in the row of a container. The uniqueness of a row with a set ROWKEY is guaranteed. A ROWKEY can be set in the first column of the row. (This is set in Column No. 0 since columns start from 0 in GridDB.)\n For a timeseries container\n ROWKEY is a TIMESTAMP\n Must be specified.\n For a collection container\n A ROWKEY is either a STRING, INTEGER, LONG, or TIMESTAMP column.\n Need not be specified\n  A default index prescribed in advance according to the column data type can be set in a column set in ROWKEY. In the current version, the default index of all STRING, INTEGER, LONG or TIMESTAMP data that can be specified in a ROWKEY is the TREE index.\nSample of Collection Operations (Java) package test;\nimport java.util.Arrays; import java.util.Properties;\nimport com.toshiba.mwcloud.gs.Collection; import com.toshiba.mwcloud.gs.GSException; import com.toshiba.mwcloud.gs.GridStore; import com.toshiba.mwcloud.gs.GridStoreFactory; import com.toshiba.mwcloud.gs.Query; import com.toshiba.mwcloud.gs.RowKey; import com.toshiba.mwcloud.gs.RowSet;\n// Operaton on Collection data public class Sample1 {\nstatic class Person { @RowKey String name; boolean status; long count; byte\\[\\] lob; } public static void main(String\\[\\] args) throws GSException { // Get a GridStore instance Properties props = new Properties(); props.setProperty(\u0026quot;notificationAddress\u0026quot;, args\\[0\\]); props.setProperty(\u0026quot;notificationPort\u0026quot;, args\\[1\\]); props.setProperty(\u0026quot;clusterName\u0026quot;, args\\[2\\]); props.setProperty(\u0026quot;user\u0026quot;, \u0026quot;system\u0026quot;); props.setProperty(\u0026quot;password\u0026quot;, \u0026quot;manager\u0026quot;); GridStore store = GridStoreFactory.getInstance().getGridStore(props); // Create a Collection (Delete if schema setting is NULL) Collection\u0026lt;String, Person\u0026gt; col = store.putCollection(\u0026quot;col01\u0026quot;, Person.class); // Set an index on the Row-key Column col.createIndex(\u0026quot;name\u0026quot;); // Set an index on the Column col.createIndex(\u0026quot;count\u0026quot;); // Set the autocommit mode to OFF col.setAutoCommit(false); // Prepare data for a Row Person person = new Person(); person.name = \u0026quot;name01\u0026quot;; person.status = false; person.count = 1; person.lob = new byte\\[\\] { 65, 66, 67, 68, 69, 70, 71, 72, 73, 74 }; // Operate a Row on a K-V basis: RowKey = \u0026quot;name01\u0026quot; boolean update = true; col.put(person); // Add a Row person = col.get(person.name, update); // Obtain the Row (acquiring a lock for update) col.remove(person.name); // Delete the Row // Operate a Row on a K-V basis: RowKey = \u0026quot;name02\u0026quot; col.put(\u0026quot;name02\u0026quot;, person); // Add a Row (specifying RowKey) // Commit the transaction (Release the lock) col.commit(); // Search the Collection for a Row Query\u0026lt;Person\u0026gt; query = col.query(\u0026quot;select * where name = 'name02'\u0026quot;); // Fetch and update the searched Row RowSet\u0026lt;Person\u0026gt; rs = query.fetch(update); while (rs.hasNext()) { // Update the searched Row Person person1 = rs.next(); person1.count += 1; rs.update(person1); System.out.println(\u0026quot;Person: \u0026quot; + \u0026quot; name=\u0026quot; + person1.name + \u0026quot; status=\u0026quot; + person1.status + \u0026quot; count=\u0026quot; + person1.count + \u0026quot; lob=\u0026quot; + Arrays.toString(person1.lob)); } // Commit the transaction col.commit(); // Release the resource store.close(); }  }\nSample of TimeSeries Operations 0 Storage and Extraction of Specific Range (Java) package test;\nimport java.util.Date; import java.util.Properties;\nimport com.toshiba.mwcloud.gs.GSException; import com.toshiba.mwcloud.gs.GridStore; import com.toshiba.mwcloud.gs.GridStoreFactory; import com.toshiba.mwcloud.gs.RowKey; import com.toshiba.mwcloud.gs.RowSet; import com.toshiba.mwcloud.gs.TimeSeries; import com.toshiba.mwcloud.gs.TimestampUtils; import com.toshiba.mwcloud.gs.TimeUnit;\n// Storage and extraction of a specific range of time-series data public class Sample2 {\nstatic class Point { @RowKey Date timestamp; boolean active; double voltage; } public static void main(String\\[\\] args) throws GSException { // Get a GridStore instance Properties props = new Properties(); props.setProperty(\u0026quot;notificationAddress\u0026quot;, args\\[0\\]); props.setProperty(\u0026quot;notificationPort\u0026quot;, args\\[1\\]); props.setProperty(\u0026quot;clusterName\u0026quot;, args\\[2\\]); props.setProperty(\u0026quot;user\u0026quot;, \u0026quot;system\u0026quot;); props.setProperty(\u0026quot;password\u0026quot;, \u0026quot;manager\u0026quot;); GridStore store = GridStoreFactory.getInstance().getGridStore(props); // Create a TimeSeries (Only obtain the specified TimeSeries if it already exists) TimeSeries\u0026lt;Point\u0026gt; ts = store.putTimeSeries(\u0026quot;point01\u0026quot;, Point.class); // Prepare time-series element data Point point = new Point(); point.active = false; point.voltage = 100; // Store the time-series element (GridStore sets its timestamp) ts.append(point); // Extract the specified range of time-series elements: last six hours Date now = TimestampUtils.current(); Date before = TimestampUtils.add(now, -6, TimeUnit.HOUR); RowSet\u0026lt;Point\u0026gt; rs = ts.query(before, now).fetch(); while (rs.hasNext()) { point = rs.next(); System.out.println( \u0026quot;Time=\u0026quot; + TimestampUtils.format(point.timestamp) + \u0026quot; Active=\u0026quot; + point.active + \u0026quot; Voltage=\u0026quot; + point.voltage); } // Release the resource store.close(); }  }\n"
},
{
	"uri": "http://example.org/data-modeling/",
	"title": "Data Modeling",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Data Modeling An introduction to GridDB\u0026rsquo;s data modeling\n"
},
{
	"uri": "http://example.org/getting-started/2-4_run-a-sample-java-client-app/",
	"title": "Run a Sample Java Client App",
	"tags": [],
	"description": "",
	"content": " Build a Client $ ant -f java_client/build.xml  Execute a sample program $ export CLASSPATH=${CLASSPATH}:$GS_HOME/bin/gridstore.jar $ mkdir gsSample $ cp $GS_HOME/docs/sample/program/Sample1.java gsSample/. $ javac gsSample/Sample1.java $ java gsSample/Sample1 239.0.0.1 31999 your_clustername admin your_password --\u0026gt; Person: name=name02 status=false count=2 lob=[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]  "
},
{
	"uri": "http://example.org/administration/6-4_upgrading-the-editions/",
	"title": "Upgrading Editions",
	"tags": [],
	"description": "",
	"content": "Follow the procedure below to update the software.\n Stop the cluster Stop the node Make a backup copy of the definition file, database file and event log file Update the software Start the node Configure the cluster  An example of the command execution in a machine in which the nodes have been started is shown below.\n[Command execution example]\n$ gs_stopcluster -u admin/admin $ gs_stopnode -u admin/admin $ cp -rp /var/lib/gridstore/data /xxx/shelter # copy just in case $ cp -rp /var/lib/gridstore/log /xxx/shelter # copy just in case $ cp -rp /var/lib/gridstore/conf /xxx/shelter # copy just in case $ su # rpm -Uvh griddb-server-Y.Y.Y-linux.x86_64.rpm # rpm -Uvh griddb-client-Y.Y.Y-linux.x86_64.rpm # rpm -Uvh griddb-docs-Y.Y.Y-linux.x86_64.rpm # exit $ gs_startnode $ gs_joincluster -c configured cluster name -u admin/admin\n*Y.Y.Y: Version of GridDB to update\n"
},
{
	"uri": "http://example.org/administration/6-5_migration-from-other-databases/",
	"title": "Migration from Other Databases",
	"tags": [],
	"description": "",
	"content": " In the GridDB export/import tools, to recover a database from local damages or the database migration process, save/recovery functions are provided in the database and container unit.\nIn addition, there is also a function to link up with RDBs, and RDB data can also be collected and registered in GridDB.\nOverview In a GridDB cluster, container data is automatically arranged in a node within a cluster. The user does not need to know how the data is arranged in the node (data position transmission).\nThere is also no need to be aware of the arrangement position in data extraction and registration during export/import as well. The export/import configuration is as follows.\nExport/import configuration\n[Export]\n(1) Save the container and row data of a GridDB cluster in the file below. A specific container can also be exported by specifying its name.\n Container data file  Save GridDB container data and row data. There are 2 types of format available, one for saving data in a container unit and the other for consolidating and saving data in multiple containers.  Export execution data file  Save the data during export execution. This is required to directly recover exported data in a GridDB cluster.   　*See the various sections in “What is a container data file” and “What is an export execution data file” for details.\n[Import]\n(2) Import the container and export execution data file, and recover the container and row data in GridDB. A specific container data can also be imported as well.\n(3) Import container data files created by the user, and register the container and row data.\n(4) Import RDB (Oracle) data, then correlate and register data from an RDB table to a GridDB container.\n[Memo]\n An exported container data file has the same format as the container data file created by a user.  What is a container data file? A container data file is composed of a metadata file and a row data file.\nA metadata file is a file in the json format which contains the container type and schema, the index set up, and the trigger data.\nThere are 2 types of row data files, one of which is the CSV data file, in which container data is stored in the CSV format, and the other is the binary data file in which data is stored in a zip format.\n CSV data file:  Stores container row data as CSV data. Readability is high, and the file can be imported and edited with generic tools. If the row data is a specific data type such as BLOB, spatial data, array etc., the data is stored in an external object file while only the external object file name is stored in a CSV data file. An external object file is created for each row data.  Binary data file:  Stores container row data in the zip format. Can be created with the command gs_export only. The file size is smaller when compared to a CSV data file. In addition, the number of files can be reduced as there is no need to create external object files. However, binary data files are not readable and cannot be edited.   See Format of a container data file for details of the contents described in each file.\nIn addition, there are 2 types of container data files as shown below depending on the number of containers to be listed.\n Single container configuration: holds 1 container data file for each container. Multi-container configuration: consolidates multiple containers into a single container data file.  Hereinafter, container data files of various configurations will be written as single container data file and multi-container data file.\nContainer data file\nWhen a large container is specified as a single container data file and export is executed, management becomes troublesome as a large amount of metadata files and row data files are created. On the other hand, even if a large container is specified as a multi-container data file, only 1 metadata file and row data file is output.\nTherefore, it is recommended that these 2 configurations be used differently depending on the application.\nA single container data file is used in the following cases.\n When you want to output the current data of a specific container to perform data analysis. When you want to create many containers with the same schema as existing containers to register data.  A multi-container data file is used in the following cases.\n You want to backup a specific container group. You want to move a database to a different GridDB cluster.  \u0026gt; What is an export execution data file? Data such as the export date and time, the number of containers, the container name etc. are saved in the export execution data file. This file is required to directly recover exported data in a GridDB cluster.\n[Memo]\n The file name of an export execution data file is gs_export.json. Delete the export execution data if an exported container data file is edited manually. A registration error may occur due to discrepancies in the data. When importing without any export execution data file, it is essential that the container metadata file be specified. If not, the import will fail. When importing from an RDB, the export execution data file is not required.  Configuration of export/import execution environment The following settings are required to execute an export/import command.\nRPM package installation The client package containing the export/import functions and Java library package need to be installed.\n[Example]\n# rpm -Uvh griddb-client-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] User and group has already been registered correctly. GridDB uses existing user and group. 1:griddb-client ########################################### [100%]\n# rpm -Uvh griddb-java_lib-X.X.X-linux.x86_64.rpm Under preparation\u0026hellip; ########################################### [100%] 1:griddb-java_lib ########################################### [100%]\nProperty file settings Configuration file is /usr/gridstore/prop/gs_expimp.properties. Set together with the GridDB cluster configuration used as a gsadm user.\ngs_expimp.properties contains the following settings.\nParameters　Default\nDescription\nmode\nMULTICAST\nSpecify the type of connection method. If the method is not specified, the method used will be the multicast method.\nMULTICAST ・・ multicast method\nFIXED_LIST・・ fixed list method\nPROVIDER ・・ provider method\nhostAddress\n239.0.0.1\nSpecify the /transaction/notificationAddress in the GridDB cluster definition file (gs_cluster.json). Multicast address used by the export/import tool to access a cluster.\nhostPort\n31999\nSpecify the /transaction/notificationPort in the GridDB cluster definition file (gs_cluster.json). Port of multicast address used by the export/import tool to access a cluster.\nnotificationProvider.url\n-\nSpecify /cluster/notificationProvide/url of the cluster definition file (gs_cluster.json) when using the provider method to connect.\nnotificationMember\n-\nSpecify /cluster/notificationMember/transaction of the cluster definition file (gs_cluster.json) when using the fixed list method to connect. Connect address and port with a “:” in the description. For multiple nodes, link them up using commas.\nExample)192.168.0.100:10001,192.168.0.101:10001\nrestAddress\n127.0.0.1\nSpecify /system/listenerAddress of the GridDB node definition file (gs_node.json). Parameter for future expansion.\nrestPort\n10040\nSpecify /system/listenerPort of the GridDB node definition file (gs_node.json). Parameter for future expansion.\nclusterName\ndefaultCluster\nSpecify the cluster name specified when forming a cluster name configuration (specify with the gs_joincluster command).\nlogPath\n/var/lib/gridstore/log\nSpecify the directory to output the error data and other logs when using the export/import tools Log is output in gs_expimp-YYYYMMDD0.log under the directory.\ngetCount\n1000\nSpecify the number of rows as a unit to export data when exporting container data with the export tool. When the numerical value becomes larger, the buffer for data processing becomes larger. If the row size is small, raise the numerical value, and if the row size is large, lower the numerical value. The parameter affects the fetch performance for data export.\ncommitCount\n1000\nSpecify the number of rows as a unit to register data when registering container data with the import tool. When the numerical value becomes larger, the buffer for data processing gets larger too. If the row size is small, raise the numerical value, and if the row size is large, lower the numerical value. The parameter affects the registration performance for data import.\ntransactionTimeout\n2147483647\nSpecify the time allowed from the start until the end of a transaction. When registering or acquiring a large volume of data, a large numerical value matching the data volume needs to be set. A maximum value has been specified for processing a large volume of data by default. (Unit: second)\nfailoverTimeout\n10\nSpecify the failover time to repeat retry starting from the time a node failure is detected. This is also used in the timeout of the initial connection to the cluster subject to import/export. Increase the value when performing a process such as registering/acquiring a large volume of data in/from a container. (Unit: second)\nrdb.driver\n-\nParameter for RDB linkage. Specify the path of the JDBC driver.\nrdb.kind\noracle\nParameter for RDB linkage. Specify the type of RDB \u0026ldquo;oracle\u0026rdquo;.\nrdb.host\n-\nParameter for RDB linkage. Specify the host name (address)) used to access RDB.\nrdb.port\n-\nParameter for RDB linkage. Specify the port no. used to access RDB.\nrdb.database\n-\nParameter for RDB linkage. Specify the applicable database name.\nrdb.url\n-\nParameter for RDB linkage. Specify the connection character string when accessing the RDB. Specify a set of the host, port and database or the url in the RDB connection destination.\nrdb.user\n-\nParameter for RDB linkage. Specify the user to access the target database.\nrdb.password\n-\nParameter for RDB linkage. Specify the password of the user to access the target database.\nload.input.threadNum\n1\nParameter for RDB linkage. Specify the number of processing threads to collect from RDB. (1-64)\nload.output.threadNum\n1\nParameter for RDB linkage. Specify the number of processing threads to register in GridDB. (1-16)\nstoreBlockSize\n64KB\nSpecify the block size specified in a GridDB cluster. The upper limit of the string data and binary data that can be registered in GridDB differs depending on the block size.\nmaxJobBufferSize\n512\nSpecify the buffer size (in MB) to hold collection and registration data.\n*[Memo]*\n When the GridDB version is upgraded, the definition file (/usr/gridstore/prop/gs_expimp.properties) is replaced by a new one.\nThe following message appears when upgrading the version.\nWarning: /usr/gridstore-X.X.X/prop/gs_expimp.properties has been saved as /usr/gridstore-X.X.X/prop/gs_expimp.properties.rpmsave.\nThe symbolic link of the directory referenced as the GridDB installation directory (_usr/gridstore) will be compared to the newly installed directory, and the definition file used will be saved as gsexpimp.properties.rpmsave in the location where the file was actually located (/usr/gridstore-X.X.X/prop).\nCompare the definition file used (/usr/gridstore-X.X.X/prop/gs_expimp.properties.rpmsave) and the newly installed definition file (/usr/gridstore-X.X.X/prop/gs_expimp.properties) and reflect the changes appropriately.\n(gridstore-X.X.X is the GridDB version installed before version upgrade. )  Export function The options that can be specified when using the export function is explained here (based on usage examples of the export function).\nSpecify process target 1. How to specify a container\nThere are 3 ways to remove a container from a GridDB cluster, by specifying all the containers of the cluster: by specifying the database, and by specifying the container individually.\n　(1) Specify all containers\n The entirety of the containers and databases in the cluster are applicable. \u0026ndash;Specify all options.  [Example]\n$ gs_export \u0026ndash;all -u admin/admin\n[Memo]\n When executed by a general user, all the containers in the database (in which the general user has access rights to) will be applicable.   　(2) Specify the database\n All containers in the specified database are applicable. Specify the database name with the \u0026ndash;db option. Multiple database names can also be specified repeatedly by separating the names with a \u0026ldquo; \u0026rdquo; (blank).  [Example]\n$ gs_export \u0026ndash;db db001 db002 -u admin/admin　//Enumerate DB name. Container in the DB\n[Memo]\n When executed by a general user, an error will occur if the general user has no access rights to the database specified in \u0026ndash;db. (\u0026ndash;Process can continue if executed by force. )   　(3) Specify container individually\n Specified container is applicable. Enumerate the container name.  \u0026ndash;Separate multiple container names with \u0026ldquo; \u0026rdquo; (blank) and specify them repeatedly in the container option.  Regular expression and specification of the container name  \u0026ndash;Specify part of the container name with a container option. A Java regular expression can be used in the specification. Enclose the specification with \u0026ldquo;\u0026rdquo; (double quotation) when specifying with a regular expression.   [Example]\n$ gs_export \u0026ndash;container c001 c002 -u admin/admin　//Enumerate container name $ gs_export \u0026ndash;container \u0026ldquo;^c0\u0026rdquo; -u admin/admin　//regular expression specification: Specify containers whose container name start with c0\n[Memo]\n Specify the name of the applicable database with the \u0026ndash;prefixdb option in the \u0026ndash;container option. If the \u0026ndash;prefixdb option is omitted, the container in the default connection destination database “public” will be processed. When executed by a general user, an error will occur if the general user has no access rights to the database where the container specified in the \u0026ndash;container option is stored. (\u0026ndash;Process can continue if executed by force. )  2. How to specify a row\nRows located by a search query can be exported by specifying a search query to remove rows from a container.\nAll rows stored in a container which has not been specified in the search query will be exported.\n　Specify search query\n Specify the definition file describing the container name and search query with the\u0026ndash;filterfile option. Describe the search query and its corresponding container in the definition file.\n[Example] Execution example\n$ gs_export -c c001 c002 -u admin/admin \u0026ndash;filterfile filter1.txt $ gs_export \u0026ndash;all -u admin/admin \u0026ndash;filterfile filter2.txt\n[Example] Description of definition file\n^cont_month :select * where time \u0026gt; 100 [return] ^cont_minutes_.*:select * where flag = 0 [return] cont_year2014 :select * where timestamp \u0026gt; TIMESTAMP(\u0026lsquo;2014-05-21T08:00:00.000Z\u0026rsquo;) [line return]\n  [Memo]\n Specify the container with a regular Java expression. Example: If \u0026ldquo;container 1\u0026rdquo; is used in the description, all containers containing container 1 will be relevant (container 10, container 12 etc.). If fully consistent, use \u0026ldquo;^container1$\u0026rdquo; in the description. Among the containers subject to export which are specified in the \u0026ndash;all and -c options, all rows in containers which the definition described in the definition file does not apply to will be exported. To describe the container and search query in 1 line, use a \u0026ldquo;:\u0026rdquo; for the separation. If the container applies to multiple definitions, the definition described at the beginning will be applied. Describe the file in the UTF-8 format. Execute the export test function to check whether the description of the definition file is correct.  3. How to specify user access rights\nInformation on GridDB cluster users and their access rights can also be exported. Use the following command when migrating all data in the cluster.\n Specify a \u0026ndash;all option and \u0026ndash;acl option. However, only user information of a general user can be exported. Migrate the data on the administrator user separately (copy the user definition file).\n[Example]\n$ gs_export \u0026ndash;all -u admin/admin \u0026ndash;acl\n  [Memo]\n The command needs to be executed by an administrator user.  Specifying the output format of a row data file A CSV data file or binary data file can be specified as the output format of a row data file.\n　Output in the CSV data file\n Execute an export command without specifying the \u0026ndash;binary option  　Output in the binary data file\n Specify the \u0026ndash;binary [file size upper limit] option. Split the binary data file using the specified file size upper limit and export the file. The file size upper limit is specified in Mbytes. If the file size upper limit is not specified, the size upper limit will be 10 Mbytes. The maximum file size that can be specified is 1,000 Mbytes.  [Example]\n$ gs_export -c c001 c002 -u admin/admin \u0026ndash;binary $ gs_export \u0026ndash;all -u admin/admin \u0026ndash;binary 500 //divide binary data file into 500 Mbytes each\nSpecifying the output configuration of container data file A single container data file to create container data file in a container unit, or a multi-container data file to output all containers to a single container data file can be specified.\n　Output using a single container data file\n If the \u0026ndash;out option is not specified during export, the data will be output using a single container data file.  　Output using a multi-container data file\n It specifies the \u0026ndash;out [file identifier] option. By specifying the file identifier, the file name of the meta data file will become “file identifier_properties.json”. The file will be named “file identifier.csv” or “file identifier.mc” if the multi-container data file format is CSV, or binary respectively. If the file identifier is omitted in the \u0026ndash;out [file identifier] option, a multi-container data file with time-stamp will be created. (Example: 20131031_155015_810_properties.json, 20131031_155015_810.csv)  [Example]\n$ gs_export -c c001 c002 -u admin/admin \u0026ndash;out test $ gs_export \u0026ndash;all -u admin/admin \u0026ndash;out //file is created with the date\nSpecifying the output destination The directory of the container data file can be specified as the output destination. Create a directory if the specified directory does not exist. If the directory is not specified, data will be output to the current directory when a command is executed. Use the -d option to specify the output destination.\n[Example]\n$ gs_export \u0026ndash;all -u admin/admin \u0026ndash;out test -d /tmp\n[Memo]\n A directory which already contains container data files cannot be specified.  Specifying the number parallel executions Get data to access a cluster in parallel with the export tool. If a command is executed in parallel on a cluster composed of multiple nodes, data can be acquired at a high speed as each node is accessed in parallel.\n Execute in parallel for the specified number by specifying the \u0026ndash;parallel option. When executed in parallel, the export data will be divided by the same number as the number of parallel executions. A range from 2 to 32 can be specified.\n[Memo]\n The \u0026ndash;parallel option can be specified only if the binary format (\u0026ndash;binary option) and multi-container format (\u0026ndash;out option) are specified.   [Example]\n$ gs_export \u0026ndash;all -u admin/admin \u0026ndash;binary \u0026ndash;out \u0026ndash;parallel 4\n  Test execution function Before exporting a container, the user can assess whether the export can be carried out correctly.\n　Specify test execution\n The export sequence can be checked simply by adding the \u0026ndash;test option to the export command. No files will be created as the process simply checks but does not actually acquire any data.  [Example]\n$ gs_export -u admin/admin \u0026ndash;all \u0026ndash;test Start export [Test mode] Output directory: /var/lib/gridstore/export Number of target containers: 5\ncontainer_2 : 10 container_3 : 10 container_0 : 10 container_1 : 10 container_4 : 10\nNumber of target containers: 5 (Success: 5 Failure: 0) Export terminated\nError continuation specification Export processing can be continued even if a row data acquisition error were to occur due to a lock conflict with another application.\n By specifying the \u0026ndash;force option, the export process will continue from the row data of the next container even if an acquisition error were to occur in a row data.\n[Example]\n$ gs_export \u0026ndash;all -u admin/admin \u0026ndash;force\n[Memo]\n Regarding containers which skipped the processing due to an error, data will still be output to the container data file even though it is not complete. However, import processing will not be carried out as the data will not be recorded in the export execution file. After resolving the row data acquisition error, execute the export process for the relevant container again.   Other functions Detailed settings in the operating display\n \u0026ndash;Processing details can be displayed by specifying the verbose option.  [Example]\n$ gs_export -c \u0026ldquo;^c0\u0026rdquo; -u admin/admin \u0026ndash;verbose Start export Connected to server:/239.0.0.15:31999 Container name list obtained from Gridstore Container name list subject to processing acquired File name list to store container data created Get row data from container data Start CSV format row data file output process: /data/exp/c001.csv Process the next container: c001 Start CSV format row data file output process: /data/exp/c002.csv Process the next container: c002 Start CSV format row data file output process: /data/exp/c010.csv Process the next container: c010 Start CSV format row data file output process: /data/exp/c003.csv Process the next container: c003 Acquisition of row data completed Output container data to metadata file Start metadata file creation process for a single container Metadata file creation process for a single container terminated Output of container data to metadata file completed\nExport completed\nSuccess: 4 Failure: 0\nDetailed settings in the operating display\n The processing status display can be suppressed by specifying the silent option.  [Example]\n$ gs_export -c c002 c001 -u admin/admin \u0026ndash;silent\nImport function Import the CSV file format or RDB data into the GridDB cluster.\nTypes of import original data source The input data sources used by the import tool are as follows.\n Container data file: container data saved by the export function, or container data created by the user RDB: Oracle database data\n[Memo]\n RDB(Oracle) can specify only input destination with the import tool. It cannot specify an output destination with the export tool.   Importing from a container data file Use the export function to import data in the exported data format into a GridDB cluster.\nSpecify process target\n　Processing data to be imported from the container data file needs to be specified.\n1. How to specify a container\nThere are 3 ways to specify a container, by specifying all the containers in the container data file, by specifying the database, and by specifying the container individually.\n(1) Specify all containers\n All containers in all the databases are applicable. \u0026ndash;Specify all options.  [Example]\n$ gs_import \u0026ndash;all -u admin/admin\n(2) Specify the database\n All containers in the specified database are applicable. Enumerate a database name  Specify multiple database names repeatedly by separating the names with a \u0026ldquo; \u0026rdquo; (blank) in the \u0026ndash;db option.   [Example]\n$ gs_import \u0026ndash;db db001 db002 -u admin/admin　//Enumerate DB name. Container in the DB\n　(3) Specify container individually\n Specified container is applicable. Enumerate the container name.  \u0026ndash;Separate multiple container names with \u0026ldquo; \u0026rdquo; (blank) and specify them repeatedly in the container option.  Regular expression and specification of the container name  \u0026ndash;Specify part of the container name with a container option. A Java regular expression can be used in the specification. Enclose the specification with \u0026ldquo;\u0026rdquo; (double quotation) when specifying with a regular expression.   [Example]\n$ gs_import \u0026ndash;container c001 c002 -u admin/admin　//Enumerat container name $ gs_import \u0026ndash;container \u0026ldquo;^c0\u0026rdquo; -u admin/admin　//regular expression specification: Specify containers whose container name start with c0\n[Memo]\n When executed by an administrator user, a database will be created if the database does not exist at the storage location of the container. When executed by a general user, an error will occur if the general user has no access rights, or if the database does not exist at the storage location of the container. (\u0026ndash;Process can continue if executed by force. ) Specify the name of the applicable database with the \u0026ndash;prefixdb option if the \u0026ndash;container option is specified. If the \u0026ndash;prefixdb option is omitted, the default connection destination database “public” will be processed. When importing NewSQL data, import the data with no NewSQL data existing in the database at the import destination. An error will occur if NewSQL data exists in the database at the import destination. (If the \u0026ndash;force option is appended, the NewSQL data causing the error will be skipped) Check the container list stored in the container data file with the \u0026ndash;list option.  If data is exported by specifying the \u0026ndash;acl option in the export function, data on the user and access rights can also be imported. Use the following command when migrating all data in the cluster.\n Specify a \u0026ndash;all option and \u0026ndash;acl option.\n[Example]\n$ gs_import \u0026ndash;all \u0026ndash;acl -u admin/admin\n  [Memo]\n The command needs to be executed by an administrator user. Use the following command when migrating all data in the cluster. Execute the command without any databases and general users existing in the migration destination.  Specifying the container data file\nSpecify the container data file. If this is not specified, the file in the current directory will be processed.\n Specify the directory\n Specify the directory address of the container data file using the –d optional command. If the directory is not specified, the container data file in the current directory will be chosen instead.   [Example]\n//Specify all containers from the current directory $ gs_import \u0026ndash;all -u admin/admin\n//Specify multiple databases from a specific directory $ gs_import \u0026ndash;db db002 db001 -u admin/admin -d /data/expdata\n//Specify multiple containers from a specific directory $ gs_import -c c002 c001 -u min/admin -d /data/expdata\n  [Memo]\n If the export execution data file (gs_export.json) does not exist, e.g. because the container data file is created manually; specify the metadata file (XXXXX_properties.json) using the -f optional command. If the -f command is not specified, import will fail  Get container list\nThe container data can be checked before it is imported.\n[Example]\n$ gs_import \u0026ndash;list Display the container list of the export data file. DB Name Type FileName public container_2 COLLECTION container_2.csv public container_0 TIME_SERIES container_0.csv public container_1 COLLECTION container_1.csv userDB container_1_db TIME_SERIES userDB.container_1_db.csv userDB container_2_db TIME_SERIES userDB.container_2_db.csv userDB container_0_db COLLECTION userDB.container_0_db.csv\nImporting from RDB The following section explains how to import RDB (Oracle) data to a GridDB cluster.\nSummary\nBasically, importing from RDB is simply done by connecting to the Oracle database, collecting data with a SQL command from the specified table, and registering the data in a GridDB container.\nImport from RDB\nData can be imported from RDB with the command below.\n$ gs_import -u admin/admin \u0026ndash;srcfile resource definition file\nSpecify the association between the Oracle table and GridDB container (mapping) in the resource definition file.\nThe following 4 settings can be specified in the json format in the resource definition file. The resource definition file is created in the RDB collection source unit.\n Connection information of RDB collection source/GridDB recovery destination RDB collection target table GridDB registered container Mapping data  Specify the connection information of the RDB collection source/GridDB recovery destination\n　Configure the RDB connection data serving as the collection source (address, port no., etc.), JDBC driver data, and GridDB recovery destination connection data.\nPath of JDBC driver　Property file\nRDB connection data of collection source　Property file or resource definition file\nGridDB connection data of recovery destination　Property file or resource definition file\n　[Memo]\n If the connection data has been configured in the resource definition file, use the data in the resource definition file. If the setting is omitted, use the data listed in the property file. See the property file and resource definition file for the description format.  Specifying the RDB collection target table\nSpecify the processing data to be imported from the Oracle database.\n　(1) Specify the table name\n All columns in the specified table will be applicable. If the items select, where, orderby are specified, the data can also be narrowed down by using column filters and conditional filters.  [Example]\n\u0026ldquo;table\u0026rdquo; : \u0026ldquo;customers\u0026rdquo;, \u0026ldquo;select\u0026rdquo; : \u0026ldquo;id, name\u0026rdquo;, \u0026ldquo;where\u0026rdquo; : \u0026ldquo;id \u0026gt; 5000\u0026rdquo;\nThe SQL to be executed is “select id, name from customers where id \u0026gt; 5000”.\n　(2) Specify the SQL command\n SQL execution result is applicable.  [Example]\nselect * from table　→all columns in a table select id, name, data from customer　→　id, name, data column\n[Memo]\n About the column order when registered in GridDB  If a table is specified: it will have the same column order as an Oracle table If a SQL command is specified: it will have the same column order as specified in SQL  If you want to change the Oracle table column order and the GridDB column order, either choose \u0026ldquo;select\u0026rdquo; for the table specified, or specify the order of the column name by specifying a SQL command. Note the column specification order in GridDB as only the first column can be set as the row key. The settings in (1) and (2) cannot be used together. If the SQL command is specified, the container division to be described later cannot be used.  Specifying a container subject to GridDB registration\nSpecify a GridDB container at the registration destination.\n Container name Container type Row key Index Trigger Time series data (if the container type is a time series container)  All registration destination data can be omitted, and association is also possible through automatic conversion of the mapping.\nHowever, if a processing table is specified in a SQL command, the container name must also be specified.\nSpecifying mapping data\nPerform data association between Oracle and GridDB.\n1. Auto conversion\nOracle tables will be associated with GridDB containers according to the default rules. The user does not need to perform definition of the conversion.\n[Default rules]\nItem\nDescription\nRemarks\nContainer name\nSpecified table name if the table name is specified\nSpecified container name if the SQL is specified\nContainer type\nCollection\nColumn\nName by column in SQL execution results. Specify the column name if names by column do not exist.\nColumn data type\nData type associated with the RDB column data type.\n An error will occur if the Oracle table name and column name do not satisfy the GridDB naming rules (see below).  Kanji, hiragana and katakana characters are used code $ # are used  If the above-mentioned naming rules are not satisfied, append another name to the column by following the procedure in “Specifying the processing table”.\n[Example]\nselect price as price from table\n  Perform association between Oracle data type and GridDB data type as shown below.\nData type\nOracle\nGridDB\nRemarks\nText data type\nCHAR\nSTRING\nVARCHAR2\nSTRING\nNumerical value data type\nNUMBER (maximum accuracy, digit)\nDOUBLE\nNUMBER (maximum accuracy)\nLONG\nDate data type\nDATE\nTIMESTAMP\nTIMESTAMP\nTIMESTAMP\nLOB data type\nCLOB\nSTRING\nBLOB\nBLOB\nRAW type\nRAW\nSTRING\nROWID type\nROWID\nSTRING\n[Memo]\n Not compatible with the Oracle extension data type. If the data type of the column is an unsupported data type, it will behave as follows.  If all columns are specified by specifying the tables subject to collection (\u0026ldquo;table\u0026rdquo; only specification, or \u0026ldquo;select\u0026rdquo; is \u0026ldquo;*\u0026rdquo; in a \u0026ldquo;table\u0026rdquo; specification), columns with unsupported data types will be skipped, and all other columns will be subject to acquisition. For all other cases, an error will occur if there are columns with unsupported data types. However, if a \u0026ndash;force option is used, columns with unsupported data types will be ignored and only columns with supported data types will be registered.   2. User definition conversion\nThe user can define the conversion rules.\nAssociation between Oracle and GridDB is described in the resource definition file.\n The name and type (collection/time series container) of a GridDB container to store table data and SQL result data can be specified by the user. The name and data type of a GridDB column can be specified by the user.  The following association can be specified for the data type.\nData type\nOracle\nGridDB\nRemarks\nText data type\nCHAR\nSTRING, INTEGER\nAn error will occur if conversion to an INTEGER conversion failed.\nVARCHAR2\nSTRING, INTEGER\nAn error will occur if conversion to an INTEGER conversion failed.\nNumerical value data type\nNUMBER (maximum accuracy, digit)\nBYTE, SHORT, INTEGER, LONG, FLOAT, DOUBLE\nDigits may be dropped if the correct data type is not selected.\nNUMBER (maximum accuracy)\nBYTE, SHORT, INTEGER, LONG, FLOAT, DOUBLE\nDigits may be dropped if the correct data type is not selected.\nDate data type\nDATE\nTIMESTAMP\nTIMESTAMP\nTIMESTAMP\nLOB data type\nCLOB\nSTRING\nBLOB\nBLOB\nRAW type\nRAW\nSTRING\nROWID type\nROWID\nSTRING\n[Memo]\n When converting a NUMBER, the data type can be specified as BYTE/SHORT/INTEGER/LONG/FLOAT/DOUBLE. Digits may be dropped if the correct data type is not selected. Data of the character data type and LOB data type data can be stored in GridDB as long as the size allows it. The size of the data that can be stored in GridDB is dependent on the block size of the node (64KB or 1MB). Set the block size of the node in the cluster definition file (gs_cluster.json), and a value that is the same as the setting in the property file. If the data type of the column is an unsupported data type, it will behave as follows.  If all columns are specified by specifying the tables subject to collection (\u0026ldquo;table\u0026rdquo; only specification, or \u0026ldquo;select\u0026rdquo; is \u0026ldquo;*\u0026rdquo; in a \u0026ldquo;table\u0026rdquo; specification), columns with unsupported data types will be skipped, and all other columns will be subject to acquisition. For all other cases, an error will occur if there are columns with unsupported data types. However, if a \u0026ndash;force option is used, columns with unsupported data types will be ignored and only columns with supported data types will be registered.   Table association An Oracle table can be associated with a GridDB containe either:\n 　(1) 1-to-1\n An Oracle table can be associated with a GridDB container on a 1-to-1 basis.\n 　(2) 1-to-many\n An Oracle table can be associated with multiple GridDB containers.\n  There are 2 types of 1-to-many association as shown below.\n Column value split: Split a container by the row value\nClassify data by the specified row value and store the data in a container for each value. The row value is appended at the end of the container name. (\u0026ldquo;Container name_row value\u0026rdquo;)\nSince the row value is assigned to the container name, the value has to be made up of alphanumeric characters and the underscore character only. If the name contains a different character type, an error will occur in the check process when a command is executed. An error will not result in the middle of creating a container as the check process is carried out before the container is created. The only row data types that can be specified in a column value split are NUMBER, CHAR and VARCHAR2.\n[Memo]\n When specifying a column value split, specify the processing target by the table name. For the column to be specified in a column value split, specify the column existing in the processing table, and the column included in the processing column. If the value of the column is Null, the name of the container where the data is stored will be \u0026ldquo;Container name_null\u0026rdquo;.   [Example] Split the list of sensor data into containers according to the sensor ID\n  1-to-many: Column value split\n Record number split: Split a container by the number of records\nSpecify the upper limit of a row to be stored in a single container If the upper limit is exceeded, the data is stored in a new container.\nA serial number (integer starting from 0) is appended at the end of the container name. \u0026ldquo;Container name_serial no.\u0026rdquo; (container_0, container_1, …)\n[Example] Split the list of sensor data into containers with 10,000 rows each\n  1-to-many: Record number split\n[Memo]\n Both column value split and record number split can be specified. In this case, processing is carried out by the column value split first followed by record number split. In the case of a container split, an error will occur if a container with the same name already exists. Re-registration (\u0026ndash;replace) is not possible even if data is added to an existing container (\u0026ndash;append) or an existing container is deleted. Invalid even if the \u0026ndash;append, \u0026ndash;replace option is specified.  ● Column association\n　An Oracle column can be associated with a GridDB column on a 1-to-1 basis.\n[Memo]\n If complex association including row consolidation and operation is necessary, create a view according to the conditions on the Oracle DB side and use the view as the acquisition target.  Resource definition file settings\nResource definition files are written in the json format. Specify the RDB (Oracle) data to be connected and the container data which is the recovery destination.\nThe settings required to connect and import data, to and from RDB (Oracle) are as follows.\nParameters\nDescription\n/inputSource\n　/type\nSpecify \u0026ldquo;rdb\u0026rdquo; when using a RDB link\n　/server\nCan be omitted. RDB connection destination of the property file is used by default. *1\n　/kind\nSpecify the type of RDB. Specify \u0026ldquo;oracle\u0026rdquo;\n　/host\nSpecify the address used to access RDB.\n　/port\nSpecify the port of the address used to access RDB.\n　/database\nSpecify the database name (SID)\n　/url\nSpecify the connection character string when accessing the RDB. (Specify the host, port, database or url.\n　/user\nSpecify the user to access the database.\n　/password\nSpecify the user password to access the database.\n/outputSource\nCan be omitted. GridDB connection destination of the property file is used by default\n　/type\nSpecify \u0026ldquo;gridstore\u0026rdquo; when registering in GridDB\n　/server\n*1\n　/host\nSpecify the address used to access GridDB.\n　/port\nSpecify the port of the address used to access GridDB.\n　/clusterName\n　/user\nSpecify the user to access the database.\n　/password\nSpecify the user password to access the database.\n/targetList\nThe following can be specified repeatedly in an array\n　/rdb\nSpecify the RDB collection targets. Either \u0026ldquo;table\u0026rdquo; or \u0026ldquo;sql\u0026rdquo; is required\n　/table\nSpecify the table name.\n　/sql\nSpecify a SQL command\n　/select\nSpecify column if the table name is specified\n　/where\nFilter the columns by conditions if the table name is specified\n　/orderby\nSort the specified columns if the table name is specified.\n　/partitionTable\nSpecify true when accessing partition tables in parallel\n　/gridstore\nSpecify a GridDB container at the registration destination.\n　/database\nSpecify the database name. Registered in the public database \u0026ldquo;public\u0026rdquo; by default.\n　/name\nSpecify the container name.\nContainer name may be omitted if the RDB collection target specifies the table name The table name will become the container name.\nContainer name is required when specifying a SQL command\n　/type\nSpecify the container type (COLLECTION/TIME_SERIES)\nContainer type is a collection by default..\n　/rowKeyAssigned\nSpecify whether there is any row key (true/false/omit. *2\n　/dataAffinity\nSpecify the data affinity name. Maximum 8 characters\n　/indexSet\nSpecify the index. *3\n　/triggerInfoSet\nSpecify a trigger. *3\n　/compressionInfoSet\nThe compression method (NO/SS) can be specified for time series containers only *3\n　/mapping\nCan be omitted, and the following can be specified repeatedly.\n　/column\n　The following can be specified repeatedly\n　/rdb\nSpecify the RDB column name.\n　/gridstore\nSpecify the GridDB column name\n　/type\nSpecify the GridDB column type\n　/containerSplit\nSpecify the container split method.\n　/type\nSpecify the column value split \u0026ldquo;columnValue\u0026rdquo; or record number split \u0026ldquo;dataNumber\u0026rdquo;.\n　/column\nFor column value split, specify the column value to split.\n　/number\nFor record number split, specify the number of records to split.\n*1:/inputSource/server/type,host,port,database,user,password\nSet whether to configure all settings or not for the groups in /inputSource/server.\n*2:/targetList/gridstore/rowKeyAssigned\ntrue: first column is set as the row key. An error will occur if the data type is not suitable for the row key.\nfalse or omitted: No row key will be set.\n*3: See Metadata file for the description format of each item.\n[Memo]\n Either the \u0026ldquo;table\u0026rdquo; specified in the table name or the \u0026ldquo;sql\u0026rdquo; specified in the SQL command is required. \u0026ldquo;sql\u0026rdquo; and \u0026ldquo;table\u0026rdquo;,\u0026ldquo;select\u0026rdquo;,\u0026ldquo;where\u0026rdquo;,\u0026ldquo;orderby\u0026rdquo; cannot be specified at the same time. When \u0026ldquo;sql\u0026rdquo; is specified in the SQL command, the GridDB container name cannot be omitted. For partition table (\u0026ldquo;partitionTable\u0026rdquo;: true), specify \u0026ldquo;table\u0026rdquo; in the table name.  An example of a resource definition file is shown below. Connection data shall be specified in the property file.\n [Example] When specifying the table name only\n{ \u0026ldquo;inputSource\u0026rdquo; : { \u0026ldquo;type\u0026rdquo; : \u0026ldquo;rdb\u0026rdquo; }, \u0026ldquo;targetList\u0026rdquo; : [ { \u0026ldquo;rdb\u0026rdquo; : { \u0026ldquo;table\u0026rdquo; : \u0026ldquo;sample_table\u0026rdquo; } } ] }\n [Example] When specifying a SQL command to perform column mapping\n{ \u0026ldquo;inputSource\u0026rdquo; : { \u0026ldquo;type\u0026rdquo; : \u0026ldquo;rdb\u0026rdquo; }, \u0026ldquo;targetList\u0026rdquo; : [ { \u0026ldquo;rdb\u0026rdquo; : { \u0026ldquo;sql\u0026rdquo; : \u0026ldquo;select * from sample_table order by id\u0026rdquo; }, \u0026ldquo;gridstore\u0026rdquo; : { \u0026ldquo;name\u0026rdquo; : \u0026ldquo;sample_collection\u0026rdquo; }, \u0026ldquo;mapping\u0026rdquo; : { \u0026ldquo;column\u0026rdquo; : [ {\u0026ldquo;rdb\u0026rdquo;: \u0026ldquo;id\u0026rdquo;, \u0026ldquo;gridstore\u0026rdquo;:\u0026ldquo;sensor_id\u0026rdquo;, \u0026ldquo;type\u0026rdquo; : \u0026ldquo;integer\u0026rdquo;}, {\u0026ldquo;rdb\u0026rdquo;: \u0026ldquo;value\u0026rdquo;, \u0026ldquo;gridstore\u0026rdquo;:\u0026ldquo;sensor_value\u0026rdquo;, \u0026ldquo;type\u0026rdquo; : \u0026ldquo;double\u0026rdquo;} ] } } ] }\n [Example] When attaching acquisition conditions targeting multiple tables in the respective table\n{ \u0026ldquo;inputSource\u0026rdquo; : { \u0026ldquo;type\u0026rdquo; : \u0026ldquo;rdb\u0026rdquo; }, \u0026ldquo;targetList\u0026rdquo; : [ { \u0026ldquo;rdb\u0026rdquo; : { \u0026ldquo;table\u0026rdquo; : \u0026ldquo;sample_table\u0026rdquo;, \u0026ldquo;select\u0026rdquo; : \u0026ldquo;id, value\u0026rdquo;, \u0026ldquo;where\u0026rdquo; : \u0026ldquo;id \u0026gt; 100\u0026rdquo;, \u0026ldquo;orderby\u0026rdquo; : \u0026ldquo;id\u0026rdquo; } }, { \u0026ldquo;rdb\u0026rdquo; : { \u0026ldquo;table\u0026rdquo; : \u0026ldquo;sample_table2\u0026rdquo;, \u0026ldquo;select\u0026rdquo; : \u0026ldquo;*\u0026ldquo;, \u0026ldquo;where\u0026rdquo; : \u0026ldquo;value \u0026gt; 1000\u0026rdquo;, \u0026ldquo;orderby\u0026rdquo; : \u0026ldquo;id\u0026rdquo; } } ] }\n [Example] When dividing a table by the column value in the id column, and then further dividing it into 300 records at a time\n{ \u0026ldquo;inputSource\u0026rdquo; : { \u0026ldquo;type\u0026rdquo; : \u0026ldquo;rdb\u0026rdquo; }, \u0026ldquo;targetList\u0026rdquo; : [ { \u0026ldquo;rdb\u0026rdquo; : { \u0026ldquo;table\u0026rdquo; : \u0026ldquo;sample_table\u0026rdquo;, \u0026ldquo;orderby\u0026rdquo; : \u0026ldquo;id\u0026rdquo; }, \u0026ldquo;mapping\u0026rdquo; : { \u0026ldquo;containerSplit\u0026rdquo; : [ {\u0026ldquo;type\u0026rdquo;:\u0026ldquo;columnValue\u0026rdquo;, \u0026ldquo;column\u0026rdquo;:\u0026ldquo;id\u0026rdquo;}, {\u0026ldquo;type\u0026rdquo;:\u0026ldquo;dataNumber\u0026rdquo;, \u0026ldquo;number\u0026rdquo;:300} ] } } ] }\n  Partition table\nFor Oracle partition tables, the partition unit (sub-partition unit in the case of a composite type) can be accessed in parallel. Using this, data of a partition table can be acquired at a high speed.\nWhen processing a partition table in parallel, set the \u0026ldquo;partitionTable\u0026rdquo; item to true in the collection target settings of the resource definition file.\n[Memo]\n When processing a partition table in parallel, a sort specified by a \u0026lsquo;order by\u0026rsquo; will be carried out on a partition basis and registered in GridDB. As a result, the sorting order of all the tables will not be guaranteed.  If there is a need to maintain the sorting order for all the tables, do not configure the parallel processing settings for the partition tables (specify false or do not specify the \u0026ldquo;partitionTable\u0026rdquo; item). In this case, since the process will not be executed in parallel on a partition basis, the import time may become longer.   Concurrency\nThe import process can be executed at a higher speed by making access to RDB and GridDB parallel.\nWhen performing parallel processing, specify the \u0026ndash;parallel option in the command line.\nCollection from RDB and registration in GridDB will be executed respectively with a degree of parallelism that is specified in \u0026ndash;parallel. If the degree of parallelism is not specified, the number of GridDB clusters and nodes will automatically become the degree of parallelism\n When you want to set in detail the collection parallelism (access to Oracle) and the registration parallelism (access to GridDB), list them down in the property file.\nload.input.threadNum=64 load.output.threadNum=16\n The registration parallelism (access to GridDB) is best set at the following range: number of GridDB nodes \u0026lt; = N \u0026lt; = (number of GridDB nodes x node concurrency). Set it according to the machine environment.\n  \nCommand line\nProperty file\nNo. of collected threads\nNo. of registered threads\ngs_import\n-\n→\n1\n1\ngs_import \u0026ndash;parallel 3\n-\n→\n3\n3\ngs_import \u0026ndash;parallel\ninput.threadNum=16\n→\n16\n3\noutput.threadNum=3\ngs_import \u0026ndash;parallel\nNot specified　→\nNo. of GridDB nodes\nNo. of GridDB nodes\nPreliminary checks and test run\nThe following items are checked prior to collection and registration processing. Preliminary checks on descriptive errors in the resource definition file and conformity of the specified data are carried out. If an error were to occur in the following checks, the processing of the tools will stop. \u0026ndash;The process cannot be continued even if the force option is selected.\nPreliminary check items\n Resource definition file  Errors in the description format Omissions of essential items Conformity of combination of definitions  RDB  Is the table name or column name a valid character string for a container name? Data type of column In the case of a column value split, is the column name a valid character string for a container name?   Perform a test run if you want to conduct a preliminary check to check out the operation only. Although communications between Oracle and GridDB are carried out during a test run, data registration will not be carried out in GridDB.\nTo perform a test run, specify the \u0026ndash;test option together with the \u0026ndash;srcfile option.\n[Example]\n$ gs_import -u admin/admin \u0026ndash;srcfile partition_table.json \u0026ndash;test Start import [Test mode] Import test execution terminated. No. of SQL subject to processing: 1920 Import terminated\nIf an error occurs in the preliminary checks, the following display will appear.\n[Example]\n$ gs_import -u admin/admin \u0026ndash;srcfile not_found_column.json \u0026ndash;test Start import [Test mode] D00C0C: A non-existent column has been specified in the mapping definition. : sql=[SELECT * FROM mytable], column=[NOT_FOUND_COLUMN]\nSmartEDA/DDS linkage\nData can be registered from Oracle to GridDB by linking the data collection/event processing base SmartEDA with the data collection server (DDS). Data is collected from Oracle using the import tool and sent to the DDS via HTTP The DDS then registers the data received in GridDB. Data can be imported even if it exists in a subnet that does not allow multicast communications between the GridDB cluster and Oracle and the Import tools.\n[Memo]\n See /usr/gridstore/misc/dds-plugin/Readme.txt for the operating procedure. Data collection/event processing base SmartEDA products are required separately. Only the multicast method can be used for connecting a GridDB cluster.  Data registration option When importing, if a specific option is not specified, an error will occur if the container that you are trying to register already exists in the GridDB cluster. Data can be added or replaced by specifying the next option. During data registration, the number of containers registered successfully and the number of containers which failed to be registered are shown.\n　Add/update data\n Data can be registered and updated in an existing container by specifying the \u0026ndash;append option Data can be added, registered or updated only if the schema, index and trigger setting data of the container that you are trying to register are the same as the existing container. The registration procedure according to the type of container is as follows.\nType of container\nRowkey designation\nOperation\nCollection\nYes\nColumns with the same key will be updated while data with different keys will be added.\nNone\nAll row data will be added and registered.\nTime series container\nYes\nIf compression is not specified, the time will be added and registered if it is newer than the existing registration data.\nIf the time is the same as the existing data, the column data will be updated.\nIf compression is specified, only rows newer than the existing data can be added.\n  　Replace container\n Delete the existing container, create a new container, and register data in it by specifying the \u0026ndash;replace option.  [Example]\n$ gs_import -c c002 c001 -u admin/min \u0026ndash;append .. Start import (addition mode) Import completed Success: 2 Failure: 0 $ gs_import -c c002 c001 -u admin/admin \u0026ndash;replace　//From a specific directory .. Start import (re-arrangement mode) Import completed Success: 2 Failure: 0 $ gs_import \u0026ndash;all -u admin/admin -d /datat/expdata \u0026ndash;replace\nError continuation specification The import process can be continued even if a registration error were to occur in a specific row data due to a user editing error in the container data file.\n By specifying the \u0026ndash;force option, the import process will continue from the row data of the next container even if a registration error were to occur in the row data.\n[Example]\n$ gs_import \u0026ndash;all -u admin/admin -d /data/expdata \u0026ndash;force\n  [Memo]\n Specify the container replacement option (\u0026ndash;replace) to re-register a collection in which an error has occurred after revising the container data file.  Other functions 　The following section explains the settings in the operating display in detail.\n \u0026ndash;Processing details can be displayed by specifying the verbose option.  [Example]\n$ gs_import -c c002 c001 -u admin/admin \u0026ndash;append \u0026ndash;verbose The next character string has been recognized as the container name character string: c002 The next character string has been recognized as the container name character string: c001 Start import (addition mode) Import completed\nSuccess: 2 Failure: 0\n　The following section explains the settings in the operating display in detail.\n \u0026ndash;The processing status display can be suppressed by specifying the silent option.  [Example]\n$ gs_import -c c002 c001 -u admin/admin \u0026ndash;append \u0026ndash;silent\nCommand/option specifications Export command  Command list\ngs_export\n-u｜\u0026ndash;user user name/password\n\u0026ndash;all｜\u0026ndash;container [container name] …｜\u0026ndash;db database name [database name]\n[-d｜\u0026ndash;directory output destination directory path]\n[\u0026ndash;out [file identifier]\n[\u0026ndash;binary [file size]]\n[\u0026ndash;filterfile definition file name]\n[\u0026ndash;count no. of acquisitions]\n[\u0026ndash;parallel no. of parallel executions]\n[\u0026ndash;acl]\n[\u0026ndash;prefixdb database name]\n[\u0026ndash;force]\n[-t｜\u0026ndash;test]\n[-v｜\u0026ndash;verbose]\n[\u0026ndash;silent]\ngs_export\n\u0026ndash;version\ngs_export\n[-h｜\u0026ndash;help]\n Optional specifications\nOptional\nEssential\nDescription\n-u｜\u0026ndash;user user/password\n〇\nSpecify the user and password used for authentication purposes.\n\u0026ndash;all\n〇\nAll containers of the cluster shall be exported. Either \u0026ndash;all, \u0026ndash;container or \u0026ndash;db option needs to be specified.\n-c｜\u0026ndash;container container name …\n〇\nSpecify the container to be exported. Multiple specifications are allowed by separating them with blanks. When using a regular expression, enclose it within double quotations to specify it. Either \u0026ndash;all, \u0026ndash;container or \u0026ndash;db option needs to be specified.\n\u0026ndash;db\n〇\nAll containers in the specified database shall be exported. Either \u0026ndash;all, \u0026ndash;container, \u0026ndash;db option needs to be specified.\n-d｜\u0026ndash;directory output destination directory path\nSpecify the directory path of the export destination. Default is the current directory.\n\u0026ndash;out [file identifier]\nSpecify this when using the multi-container format for the file format of the output data. The single container format will be used by default.\nIf the file identifier is specified, the file identifier will be used as the file name, and if it is omitted, the output start date and time will be used as the file name.\n\u0026ndash;binary [file size]\nSpecify this when using the binary format for the output format of the row data file. The CSV format will be used by default.\nSpecify the output file size in MB. Default is 100MB. A range from 1 to 1000 (1GB) can be specified.\n\u0026ndash;filterfile definition file name\nSpecify the definition file in which the search query used to export rows is described. All rows are exported by default.\n\u0026ndash;count Acquisition count\nSpecify the number of data to be acquired each time when acquiring data from a container. The value of gs_expimp.properties will be valid by default.\n\u0026ndash;parallel No. of parallel executions\nExecute in parallel for the specified number. When executed in parallel, the export data will be divided by the same number as the number of parallel executions. This can be specified only for the multi-container format (when the \u0026ndash;out option is specified). A range from 2 to 32 can be specified.\n\u0026ndash;acl\nData on the database, user, access rights will also be exported. This can be specified only if the user is an administrator user and \u0026ndash;all option is specified.\n\u0026ndash;prefixdb database name\n\u0026ndash;If a container option is specified, specify the database name of the container. The containers in the default database will be processed if they are omitted.\n\u0026ndash;force\nProcessing is forced to continue even if an error occurs. Error descriptions are displayed in a list after processing ends.\n-t｜\u0026ndash;test\nExecute the tool in the test mode.\n-v｜\u0026ndash;verbose\nOutput the operating display details.\n\u0026ndash;silent\nOperating display is not output.\n\u0026ndash;version\nDisplay the version of the tool.\n-h｜\u0026ndash;help\nDisplay the command list as a help message.   [Memo]\n - If the t (\u0026ndash;test) option is specified, a query will be executed until the data is fetched. Container data file will not be created. - If the v (\u0026ndash;verbose) option is specified, a message will appear during processing. If omitted, a message will appear only when there is an error. Create the respective directory and file if the specified directory path does not exist in the -d (\u0026ndash;directory) option, and the specified file name does not exist in the \u0026ndash;out option. -If a c (\u0026ndash;container) option is specified, a Java regular expression can be specified in the container name. See the Java “Class Pattern” for details.  Import command  Command list\ngs_import\n-u｜\u0026ndash;user user name/password\n\u0026ndash;all｜\u0026ndash;container container name [container name …]｜\u0026ndash;db database name [database name]\n[\u0026ndash;append｜\u0026ndash;replace]\n[-d｜\u0026ndash;directory import target directory path]\n[-f｜\u0026ndash;file file name [file name…]]\n[\u0026ndash;count no. of commit]\n[\u0026ndash;acl]\n[\u0026ndash;prefixdb database name]\n[\u0026ndash;force]\n[-v｜\u0026ndash;verbose]\n[\u0026ndash;silent]\nWhen the input source is other resources:\n　[\u0026ndash;srcfile file path [\u0026ndash;test]]\ngs_import\n-l｜\u0026ndash;list\n[-d｜\u0026ndash;directory directory path]\n[-f｜\u0026ndash;file file name [file name…]]\ngs_import\n\u0026ndash;version\ngs_import\n[-h｜\u0026ndash;help]\n Optional specifications\nOptional\nEssential\nDescription\n-u｜\u0026ndash;user user/password\n〇\nSpecify the user and password used for authentication purposes.\n\u0026ndash;all\n〇\nAll containers in the import source file shall be imported. Either \u0026ndash;all, \u0026ndash;container or \u0026ndash;db option needs to be specified.\n-c｜\u0026ndash;container container name …\n〇\nSpecify the container subject to import. Multiple specifications are allowed by separating them with blanks. When using a regular expression, enclose it within double quotations to specify it. Either \u0026ndash;all, \u0026ndash;container or \u0026ndash;db option needs to be specified.\n\u0026ndash;db\n〇\nAll containers in the specified database shall be imported. Either \u0026ndash;all, \u0026ndash;container, \u0026ndash;db option needs to be specified.\n-d｜\u0026ndash;directory directory path\nSpecify the directory path of the import source. Default is the current directory.\n-f｜\u0026ndash;file [file name]\nSpecify the container data file to be imported. Multiple specifications allowed. All container data files of the current directory or directory specified in d (\u0026ndash;directory) will be applicable by default.\n\u0026ndash;count no. of commit\nSpecify the number of input cases until the input data is committed together.\n\u0026ndash;acl\nData on the database, user, access rights will also be imported. This can be specified only if the user is an administrator user and the \u0026ndash;all option is specified for data exported by specifying the \u0026ndash;acl option.\n\u0026ndash;prefixdb database name\n\u0026ndash;If a container option is specified, specify the database name of the container. The containers in the default database will be processed if they are omitted.\n\u0026ndash;force\nProcessing is forced to continue even if an error occurs. Error descriptions are displayed in a list after processing ends.\n-v｜\u0026ndash;verbose\nOutput the operating display details.\n\u0026ndash;silent\nOperating display is not output.\n\u0026ndash;srcfile resource definition file path\nSet up the path of the resource definition file. Specify when importing from RDB.\n-l｜\u0026ndash;list\nDisplay a list of the specified containers to be imported.\n\u0026ndash;version\nDisplay the version of the tool.\n-h｜\u0026ndash;help\nDisplay the command list as a help message.\n  [Memo]\n If -l (\u0026ndash;list) is specified, and options other than the -d (\u0026ndash;directory) and -f (\u0026ndash;file) option are specified, an option argument error will occur. - If the v (\u0026ndash;verbose) option is specified, a message will appear during processing. If omitted, a message will appear only when there is an error. -If a c (\u0026ndash;container) option is specified, a Java regular expression can be specified in the container name. See the Java “Class Pattern” for details.  Format of container data file The respective file formats to configure container data files are shown below.\nMetadata file The metadata file stores the container data in the JSON format.\n[Memo]\n A metadata file is described using the UTF-8 character code.  Container data to be stored is shown below.\nItem\nDescription\nContainer name\nName of the container.\nContainer type\nRefers to a collection or time series container.\nSchema data\nData of a group of columns constituting a row. A column name is combined with a data type to form a group.\nCompression configuration data\nCompression type data to be configured in a Time series data. Set up thinning compression with error, thinning compression without error, or no compression.\nIndex setting data\nIndex type data set in a container. Availability of index settings. Specify the type of index e.g. hash index, spatial index, tree index, etc.\nTrigger (event notification) data\nNotification is triggered when a container is updated (PUT/DELETE) by the JMS or REST interface.\nRow key setting data\nSet up a row key when collection container is used. For time series containers, either there is no row key set or the default value, if set, will be valid.\nThe tag and data items of the metadata in the JSON format are shown below. Tags that are essential for new creations by the user are also listed (tag setting condition).\nTag name　Item　Description　Setting conditions　Common parameters\n　　　container\nContainer name\nContainer name\nEssential\ncontainerType\nContainer type\nSpecify either COLLECTION or TIME_SERIES\nEssential\ncontainerFileType\nContainer file type\nSpecify either csv or binary\nEssential\ncontainerFile\nContainer file name\nFile name\nEssential\ndataAffinity\nData affinity name\nSpecify the data affinity name. Maximum length of 8 characters. (Valid for TIME_SERIES data only)\nAny\nrowKeyAssigned\nRow key setting\nSpecify either true/false\nTrue if there is no arbitrary key word\npartitionNo\nPartition\nEmpty character string that has not been set yet\nArbitrary, output during export. (Need not be specified during import. Value is not used even if it is specified.)\ncolumnSet\nColumn data set, (schema data)\nColumn data needs to match when adding data to an existing container\nEssential\n　columnName\nColumn name\nEssential\n　type\nData type\nBOOLEAN/ STRING/ BYTE/ SHORT/ INTEGER/ LONG/ FLOAT/ DOUBLE/ TIMESTAMP/ GEOMETRY/ BLOB/ BOOLEAN[]/ STRING[]/ BYTE[]/ SHORT[]/ INTEGER[]/ LONG[]/ FLOAT[]/ DOUBLE[]/ TIMESTAMP[]\nEssential\nindexSet\nIndex data set\nCan be set for each column. Non-existent column name will be ignored or an error will be output\nAny\n　columnName\nColumn name\nArbitrary (essential when indexSet is specified)\n　IndexType\nIndex type\nHASH ( STRING/ BOOLEAN/ BYTE/ SHORT/ INTEGER/ LONG/ FLOAT/ DOUBLE/ TIMESTAMP ) SPATIAL ( GEOMETRY ) , TREE ( STRING/ BOOLEAN/ BYTE/ SHORT/ INTEGER/ LONG/ FLOAT/ DOUBLE/ TIMESTAMP )\nArbitrary (essential when indexSet is specified)\ntriggerInfoSet\nTrigger setting\nAny\n　eventName\nTrigger name\nTrigger name\nArbitrary (essential when triggerInfoSet is specified))\n　notificationType\nNotification method\nJMS/REST\nArbitrary (essential when triggerInfoSet is specified))\n　targetEvents\nOperations subject to monitoring\nPUT/DELETE\nArbitrary (essential when triggerInfoSet is specified\n　targetColumnNames\nColumn name\nArbitrary column subject to notification (multiple columns can be specified using commas to separate them), BLOB/GEOMETRY/ARRAY data types can be set but these will not work. The “,” (comma) separator is used, and an error will occur if a non-existent column name is specified.\n　Notification URI\nNotification destination URI\nArbitrary (essential when triggerInfoSet is specified\n　JmsDestinationType\nDestination type\nSpecify either “topic” or “queue”\nValid for JMS only\n　JmsDestinationName\nDestination name\nArbitrary (essential when notificationType is JMS) Specify for JMS only\n　user\nUser name\nArbitrary (essential when notificationType is JMS) Specify for JMS only\n　password\nPassword\nArbitrary (essential when notificationType is JMS) Specify for JMS only\nTIME_SERIES only parameter\ntimeSeriesProperties\nCompression data setting\nOnly data whose containerType is TIME_SERIES can be specified\nAny\ncompressionMethod\nNO, SS, HI\nAny\ncompressionWindowSize\nMaximum period of a row\nSpecify an integer value\nAny\ncompressionWindowSizeUnit\nTime data ENUM\nDAY/ HOUR/ MILLISECOND/ MINUTE/ MONTH/ SECOND/ YEAR\nAny\nexpirationDivisionCount\nDivision count of period release\nSpecify the division count of period release\nAny\nrowExpirationElapsedTime\nElapsed period\nSpecify an integer value\nAny\nrowExpirationTimeUnit\nTime data ENUM\nDAY/ HOUR/ MILLISECOND/ MINUTE/ MONTH/ SECOND/ YEAR\nAny\ncompressionInfoSet\nSettings for each column\nOnly Hi can be specified for the compressionMethod\nAny\n　columnName\nColumn name\nAny\n　compressionType\nAbsolute value/relative value\nRELATIVE: Relative value, ABSOLUTE: Absolute value\nAny\n　Width\nAbsolute error exists. Thinning and compression parameters\nCan be specified by a floating-point number\nAny Essential for specified column Error when specified at the same time as Rate/Span\n　Rate\nRelative error exists. Thinning and compression parameters\nCan be specified by a floating-point number\nAny can be set only when compressionMethod is set to HI. In SS/NO, error is ignored/output. Error occurs if width is specified at the same time\n　Span\nRelative error exists. Thinning and compression parameters\nCan be specified by a floating-point number\nAny can be set only when compressionMethod is set to HI. In SS/NO, error is ignored/output. Error occurs if width is specified at the same time\n[Memo]\n Container metadata is described in the json format in the metadata file of a single container data file. Container metadata is described in a json array in the metadata file of a multi-container data file. Met data file name of a single container data file  Container name_properties.json  Metadata file name of a multi-container data file\n Specify a file identifier with an \u0026ndash;out option: file identifier_properties.json\n Omit the file identifier with an \u0026ndash;out option: Date and time_properties.json\n   [Points to note]\n Do not edit metadata files if row data files are exported in the binary format.  [Example1] Example of a collection in a single container file (c001_properties.json)\n A single collection is described.  1: { 2: \u0026ldquo;container\u0026rdquo;: \u0026ldquo;c001\u0026rdquo;, 3: \u0026ldquo;containerFile\u0026rdquo;: \u0026ldquo;c001.csv\u0026rdquo;, 4: \u0026ldquo;containerFileType\u0026rdquo;: \u0026ldquo;csv\u0026rdquo;, 5: \u0026ldquo;containerType\u0026rdquo;: \u0026ldquo;COLLECTION\u0026rdquo;, 6: \u0026ldquo;columnSet\u0026rdquo;: [ 7: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_ID\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;INTEGER\u0026rdquo; }, 8: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_STRING\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;STRING\u0026rdquo;} 9: ], 10: \u0026ldquo;indexSet\u0026rdquo;: [ 11: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_ID\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;TREE\u0026rdquo;}, 12: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_ID\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;HASH\u0026rdquo;}, 13: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_STRING\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;HASH\u0026rdquo; } 14: ], 15: \u0026ldquo;rowKeyAssigned\u0026rdquo;: true 16: } 17:\n[Example 2] Example of a collection and time series container in a multi-container file (container01_properties.json)\n For collections and time series containers  1: 2: [ 3: { 4: \u0026ldquo;container\u0026rdquo;: \u0026ldquo;c001\u0026rdquo;, 5: \u0026ldquo;containerType\u0026rdquo;: \u0026ldquo;collection\u0026rdquo;, //for collection 6: \u0026ldquo;containerFileType\u0026rdquo;:\u0026ldquo;csv\u0026rdquo;, 7: \u0026ldquo;containerFile\u0026rdquo;:\u0026ldquo;container01.csv\u0026rdquo;, 8: \u0026ldquo;rowKeyAssigned\u0026rdquo;:true, 9: \u0026ldquo;columnSet\u0026rdquo;: [ 10: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_FLAG\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;BOOLEAN\u0026rdquo; }, 11: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_BLOB_DATA\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;BLOB\u0026rdquo; }, 12: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_STRING\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;STRING\u0026rdquo; } 13: ], 14: \u0026ldquo;indexSet\u0026rdquo;:[ 15: { \u0026ldquo;columnName\u0026rdquo;:\u0026rdquo; COLUMN_STRING \u0026ldquo;, \u0026ldquo;indexType\u0026rdquo;: \u0026ldquo;HASH\u0026rdquo; } 16: ], 17: \u0026ldquo;triggerInfoSet\u0026rdquo;:[ 18: { \u0026ldquo;eventName\u0026rdquo;:\u0026rdquo; FLAG_EVENT\u0026rdquo;, \u0026ldquo;notificationType\u0026rdquo;:\u0026ldquo;JMS\u0026rdquo;, 19: \u0026ldquo;targetEvents\u0026rdquo;:\u0026ldquo;DELETE\u0026rdquo;, \u0026ldquo;targetColumnNames\u0026rdquo;:\u0026ldquo;COLUMN_FLAG\u0026rdquo;, 20: \u0026ldquo;notificationURI\u0026rdquo;:\u0026ldquo;http://example.com\u0026quot;, 21: \u0026ldquo;JmsDestinationType\u0026rdquo;:\u0026ldquo;\u0026rdquo;, \u0026ldquo;JmsDestinationName\u0026rdquo;:\u0026ldquo;\u0026rdquo;, 22: \u0026ldquo;JmsUser\u0026rdquo;:\u0026ldquo;\u0026rdquo;, \u0026ldquo;JmsPassword\u0026rdquo;:\u0026ldquo;\u0026rdquo; }, 23: { \u0026ldquo;eventName\u0026rdquo;:\u0026ldquo;STRING_EVENT\u0026rdquo;, \u0026ldquo;notificationType\u0026rdquo;:\u0026ldquo;REST\u0026rdquo;, 24: \u0026ldquo;targetEvents\u0026rdquo;:\u0026ldquo;PUT\u0026rdquo;, \u0026ldquo;targetColumnNames\u0026rdquo;:\u0026ldquo;COLUMN_STRING\u0026rdquo;, 25: \u0026ldquo;notificationURI\u0026rdquo;:\u0026ldquo;\u0026rdquo; } 26: ] 27: }, 28: { 29: \u0026ldquo;container\u0026rdquo;: \u0026ldquo;c002\u0026rdquo;, 30: \u0026ldquo;containerType\u0026rdquo;: \u0026ldquo;timeSeries\u0026rdquo;, //for time series container 31: \u0026ldquo;containerFileType\u0026rdquo;:\u0026ldquo;csv\u0026rdquo;, 32: \u0026ldquo;containerFile\u0026rdquo;:\u0026ldquo;container01.csv\u0026rdquo;, 33: \u0026ldquo;rowKeyAssigned\u0026rdquo;:true, 34: \u0026ldquo;dataAffinity\u0026rdquo;:\u0026ldquo;month\u0026rdquo;, 35: \u0026ldquo;columnSet\u0026rdquo;: [ 36: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_TIMESTAMP\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;TIMESTAMP\u0026rdquo; }, 37: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_FLAG\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;BOOLEAN\u0026rdquo; }, 38: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_BLOB_DATA\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;BLOB\u0026rdquo; }, 39: { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;COLUMN_INTEGER\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;INTEGER\u0026rdquo; } 40: ], 41: \u0026ldquo;indexSet\u0026rdquo;:[ 42: { \u0026ldquo;columnName\u0026rdquo;:\u0026rdquo; COLUMN_FLAG \u0026ldquo;, \u0026ldquo;indexType\u0026rdquo;: \u0026ldquo;TREE\u0026rdquo; } 43: ], 44: \u0026ldquo;triggerInfoSet\u0026rdquo;:[ 45: { \u0026ldquo;eventName\u0026rdquo;:\u0026ldquo;TIMESTAMP_EVENT\u0026rdquo;, \u0026ldquo;notificationType\u0026rdquo;:\u0026ldquo;REST\u0026rdquo;, 46: \u0026ldquo;targetEvents\u0026rdquo;:\u0026ldquo;DELETE\u0026rdquo;, \u0026ldquo;targetColumnNames\u0026rdquo;:\u0026ldquo;COLUMN_TIMESTAMP\u0026rdquo;, 47: \u0026ldquo;notificationURI\u0026rdquo;:\u0026ldquo;\u0026rdquo;, 48: \u0026ldquo;JmsDestinationType\u0026rdquo;:\u0026ldquo;\u0026rdquo;, \u0026ldquo;JmsDestinationName\u0026rdquo;:\u0026ldquo;\u0026rdquo;, 49: \u0026ldquo;JmsUser\u0026rdquo;:\u0026ldquo;\u0026rdquo;, \u0026ldquo;JmsPassword\u0026rdquo;:\u0026ldquo;\u0026rdquo; } 50: ], 51: \u0026ldquo;timeSeriesProperties\u0026rdquo;:[ 52: { \u0026ldquo;compressMethod\u0026rdquo;: \u0026ldquo;HI\u0026rdquo;, 53: \u0026ldquo;compressionWindowSize\u0026rdquo;:10, \u0026ldquo;compressionWindowSizeUnit\u0026rdquo;:\u0026ldquo;SECOND\u0026rdquo;, 54: \u0026ldquo;expirationDivisionCount\u0026rdquo;:12, 55: \u0026ldquo;rowExpirationElapsedTime”: 1, \u0026ldquo;rowExpirationTimeUnit\u0026rdquo;: “DAY”} 56: ], 57: \u0026ldquo;compressionInfoSet\u0026rdquo;:[ 58: { “columnName”:COLUMN_INTEGER”, ”compressionType”:\u0026ldquo;RELATIVE\u0026rdquo;, 59: \u0026ldquo;rate\u0026rdquo;:”1.0E2”, \u0026ldquo;span\u0026rdquo;:”1.0E2” } 60: ] 61: } 62: ] 63:\nRow data file (binary data file) The row data file in the binary format can be created in the zip format by gs_export only. No readability, and cannot be edited as well. No readability, and cannot be edited as well.\nRow data file (CSV data file) The row data file in the CSV format describes the references to the metadata file which is the definition of a row in the container data file information section.\n[Memo]\n A CSV data file is described using the UTF-8 character code.  1. Header section (1st - 2nd row)\nHeader section contains data output during export. Header data is not required during import.\n Assign a “#” at the beginning of the command to differentiate it. The format will be as follows.\n\u0026rdquo;# (Date and time data) (blank)GridDB release version\u0026rdquo; \u0026ldquo;#User: (user name)\u0026rdquo;\n[Example]\n\u0026rdquo;#2013-06-14T17:34:36.520+0900 GridStore V1.5.00\u0026rdquo; \u0026ldquo;#User:admin \u0026ldquo;\n  2. Container data file data section (3rd and subsequent rows)\nDescribe the references to the metadata file.\n Assign a “%” at the beginning of the command to differentiate it. The format of one row will be as follows.\n\u0026rdquo;%\u0026ldquo;,\u0026rdquo;(container name)_properties.json\u0026rdquo;\n  3. Row data section (container data and subsequent sections)\nThe following section describes the row data.\n Assign a “$” at the beginning of the container name and describe the row data for the number of cases that you want to register in the container. Separate the row data of the column with commas and describe them in one line of the CSV file.\n\u0026rdquo;$\u0026ldquo;,\u0026rdquo;(Container name)\u0026rdquo; \u0026ldquo;value\u0026rdquo;,\u0026ldquo;value\u0026rdquo;,\u0026ldquo;value\u0026rdquo;,．． (number of column definitions) \u0026ldquo;value\u0026rdquo;,\u0026ldquo;value\u0026rdquo;,\u0026ldquo;value\u0026rdquo;,．． (number of column definitions) : : //Describe the number of row cases you want to register :\n  4. Comments section\nThe comment section can be described anywhere in the CSV data file except the header section.\n Assign a “#” at the beginning of the command to differentiate it.  [Memo]\n The CSV data file of a single container data file is configured as follows.  　1. Header section, 2. Container data file data section, 3. Row data section  The CSV data file of a multi-container data file is configured as follows.  　1. Header section, 2. Container data file data section, 3. Row data section (multiple)   The name of the CSV data file output by the export tool is as follows.\n 　container_name.csv for single container data files 　file_identifier.csv for multi-container data files and when the file identifier is specified in the \u0026ndash;out option 　date\u0026amp;time.csv for multi-container data files and when the file identifier is omitted in the \u0026ndash;out option  [Example] Description of a CSV data file (including external object file) Data description of a metadata file in Example 1\n\u0026rdquo;#2013-11-01T11:19:03.437+0900 GridStore V1.5.00\u0026rdquo; \u0026ldquo;#User:admin\u0026rdquo; \u0026ldquo;%\u0026rdquo;,\u0026ldquo;c001_properties.json\u0026rdquo; \u0026ldquo;$\u0026rdquo;,\u0026ldquo;c001\u0026rdquo; \u0026ldquo;1\u0026rdquo;,\u0026ldquo;Tokyo\u0026rdquo; \u0026ldquo;2\u0026rdquo;,\u0026ldquo;Kanagawa\u0026rdquo; \u0026ldquo;3\u0026rdquo;,\u0026ldquo;Osaka\u0026rdquo;\nWhen the data below is included in some of the rows of the CSV data file, prepare an external object file separate from the CSV data file as an external object. List the references of the external data file in the target column of the CSV file. \u0026ldquo;@data type:” (file name)\n BLOB data  List “@BLOB:” + (file name) as BLOB data in the “value” section of the relevant column. The file naming section has a format which is the file name + “.blob”. The binary file is located according to the rules of the file naming section.  Spatial data  List “@GEOMETRY:” + (file name) as GEOMETRY data in the relevant “value” section. The file naming section has a format which is the file name + “.geometry”. List a spatial column in the external object file. Describe using character code UTF-8.  Array (BOOLEAN[]/ STRING[]/ BYTE[]/ SHORT[]/ INTEGER[]/ LONG[]/ FLOAT[]/ DOUBLE[]/ TIMESTAMP[])  List “@(Data Type)_ARRAY:” + (file name) as ARRAY data in the relevant “value” section. The file naming section has a format which is the file name + “.(data type)_array”. If the length of the character string exceeds 100 characters, list down array data in the external object file. Describe using character code UTF-8.  Character string data  List “@STRING:” + (file name) as STRING data in the relevant “value” section. The file naming section has a format which is the file name + “.string”. If the length of the character string exceeds 100 characters, or if it includes a line return (\\r), list down string data in the external object file. Describe using character code UTF-8.   When an external object file is exported, the external object file name is created in accordance with the following rules during export.\n For single container data files  Container name_Container name_ROW No. _COLUMN No.data type If the container column is a Byte array, the name of the external object file will be Container name_Container name_ROW no. _COLUMN no. .byte_array. The ROW no. and COLUMN no. shows the sequence no. of the container data and they are numbered starting from 0.  When the format is a multi-container format  File identifier specified in the \u0026ndash;out option File identifier_Container name_ROW no. _COLUMN no.data type File identifier omitted in \u0026ndash;out option Date and time_Container name_ROW no. _COLUMN no.data type   For import purposes, any file name can be used for the external object file. List down the CSV data file with a file name of any data type in the relevant column.\n[Example] Naming example of an external object file\n　//When a collection (colb) having a BYTE array in the 3rd column is exported Oct 4 12:51 2013 colb.csv Oct 4 12:51 2013 colb_colb_0_3.byte_array　Oct 4 12:51 2013 colb_colb_1_3.byte_array　Oct 4 12:51 2013 colb_colb_2_3.byte_array Oct 4 12:51 2013 colb_colb_3_3.byte_array Oct 4 12:51 2013 colb_colb_4_3.byte_array Oct 4 12:51 2013 colb_properties.json\n[Example] Description of an external object file in a single container data file is shown below.\n　// meta data file col01_properties.json\n{ \u0026ldquo;container\u0026rdquo;: \u0026ldquo;col01\u0026rdquo;, \u0026ldquo;containerFile\u0026rdquo;: \u0026ldquo;col01.csv\u0026rdquo;, \u0026ldquo;containerFileType\u0026rdquo;: \u0026ldquo;csv\u0026rdquo;, \u0026ldquo;containerType\u0026rdquo;: \u0026ldquo;COLLECTION\u0026rdquo;, \u0026ldquo;columnSet\u0026rdquo;: [ { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;name\u0026rdquo;,\u0026ldquo;type\u0026rdquo;: \u0026ldquo;string\u0026rdquo; }, {\u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;status\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;boolean\u0026rdquo;}, { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;count\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;long\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;lob\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;byte[]\u0026ldquo; } ],\n\u0026ldquo;indexSet\u0026rdquo;: [ { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;name\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;TREE\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;: \u0026ldquo;count\u0026rdquo;, \u0026ldquo;type\u0026rdquo;: \u0026ldquo;TREE\u0026rdquo; } ], \u0026ldquo;rowKeyAssigned\u0026rdquo;: true }\n　//CSV data file col01.csv\n\u0026rdquo;#2013-11-01T19:41:35.320+0900 GridStore V1.5.00\u0026rdquo; \u0026ldquo;#User:admin\u0026rdquo; \u0026ldquo;%\u0026rdquo;,\u0026ldquo;col01_properties.json\u0026rdquo;　\u0026ldquo;$\u0026rdquo;,\u0026ldquo;col01\u0026rdquo; \u0026ldquo;name02\u0026rdquo;,\u0026ldquo;false\u0026rdquo;,\u0026ldquo;2\u0026rdquo;,\u0026ldquo;@BYTE_ARRAY:col101_col01_0_3.byte_array\u0026rdquo;\n　// external object file col01_col01_0_3.byte_array\n1,10,15,20,40,70,71,72,73,74\n"
},
{
	"uri": "http://example.org/sample-applications/",
	"title": "Sample Application",
	"tags": [],
	"description": "",
	"content": " Chapter 5 Sample Application A look at a GridDB sample application\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-5_transactions-and-acid/",
	"title": "Transactions and ACID",
	"tags": [],
	"description": "",
	"content": " GridDB supports transaction processing on a container basis and ACID (atomictity, consistency, isolation, durability) characteristics which are generally known as transaction characteristics. The supporting functions in a transaction process are explained in detail below.\nWhat is ACID?\nRelational Databases long-reign of dominance can be somewhat attributed to them being ACID compliant. Financial transactions, for example, need ACID compliance to function with a degree of confidence and usability. Here is a more detailed look at each component of ACID:\nAtomicty: All transactions are \u0026ldquo;all or nothing\u0026rdquo;. This means, if one part of the transaction fails, then the entire thing is canceled. Transactions only \u0026ldquo;complete\u0026rdquo; when the whole operation is finished. This protects against half-measures that can be caused by things like sudden failues, errors, etc.\nConsistency: Guarantees that data being changed must follow the established rules set by the database and its administrator.\nIsolation: Guarantees that transactions are committed serially (one after the other) even if transactions are coming in at the same time.\nDurability: Guarantees that committed transactions stay committed, even if an error occurs. Ensures the data is always safe.\nWith all of these attributes working in conjunction, database transactions have a certain guarantee that lend themselves well for more \u0026ldquo;important\u0026rdquo; duties (like finance or mission critical applications).\nStarting and Ending a Transaction When a row search (or update, etc.) is carried out on a container, a new transaction is started and this transaction ends when the update results of the data are committed or aborted.\n[Memo]\n A commit is a process to confirm transaction information under processing to perpetuate the data.  In GridDB, updated data of a transaction is stored as a transaction log by a commit process, and the lock that had been maintained will be released.  An abort is a process to rollback (delete) all transaction data under processing.  In GridDB, all data under processing are discarded and retained locks will also be released.   The initial action of a transaction is set in autocommit.\nIn autocommit, a new transaction is started every time a container is updated (data addition, deletion or revision) by the application, and this is automatically committed at the end of the operation. A transaction can be committed or aborted at the requested timing by the application by turning off autocommit.\nA transaction recycle may terminate in an error due to a timeout in addition to being completed through a commit or abort. If a transaction terminates in an error due to a timeout, the transaction is aborted. The transaction timeout is the elapsed time from the start of the transaction. Although the initial value of the transaction timeout time is set in the definition file (gs_node.json), it can also be specified as a parameter when connecting to GridDB on an application basis.\nTransaction Consistency Level There are 2 types of transaction consistency levels, immediate consistency and eventual consistency. This can also be specified as a parameter when connecting to GridDB for each application. The default setting is immediate consistency.\n Immediate consistency: Container update results from other clients are reflected immediately at the end of the transaction. As a result, the latest details can be referenced all the time. Eventual consistency: Container update results from other clients may not be reflected immediately at the end of the transaction. As a result, there is a possibility that old details may be referred to.  Immediate consistency is valid in update operations and read operations. Eventual consistency is valid in read operations only. For applications which do not require the latest results to be read all the time, the reading performance improves when eventual consistency is specified.\nTransaction Isolation Level Conformity of the database contents need to be maintained all the time. When executing multiple transaction simultaneously, the following events will generally surface as issues.\n An event which involves uncommitted data written by a dirty read transaction being read by another transaction. An event which involves data read previously by a non-recurrent read transaction becoming unreadable.  Even if you try to read the data read previously by a transaction again, the previous data can no longer be read as the data has already been updated and committed by another transaction (the new data after the update will be read instead).\n An event in which the inquiry results obtained previously by a phantom read transaction can no longer be acquired.  Even if you try to execute an inquiry executed previously in a transaction again in the same condition, the previous results can no longer be acquired as the data satisfying the inquiry condition has already been changed, added and committed by another transaction (new data after the update will be acquired instead). In GridDB, \u0026lsquo;READ_COMMITTED\u0026rsquo; is supported as a transaction isolation level. In READ_COMMITTED, the latest data confirmed data will always be read. When executing a transaction, this needs to be taken into consideration so that the results are not affected by other transactions. The isolation level is an indicator from 1 to 4 that shows how isolated the executed transaction is from other transactions (the extent that consistency can be maintained). The 4 isolation levels and the corresponding possibility of an event raised as an issue occurring during simultaneous execution are as follows.\nIsolation level\nDirty read\nNon-recurrent reading\nPhantom read\nREAD_UNCOMMITTED\nPossibility of occurrence\nPossibility of occurrence\nPossibility of occurrence\nREAD_COMMITTED\nSafe\nPossibility of occurrence\nPossibility of occurrence\nREPEATABLE_READ\nSafe\nSafe\nPossibility of occurrence\nSERIALIZABLE\nSafe\nSafe\nSafe\nIn READ_COMMITED, if data read previously is read again, data that is different from the previous data may be acquired, and if an inquiry is executed again, different results may be acquired even if you execute the inquiry with the same search condition. This is because the data has already been updated and committed by another transaction after the previous read.\nIn GridDB, data that is being updated by MVCC is isolated.\nMVCC In order to realize READ_COMMITTED, \u0026lsquo;MVCC (Multi-Version Concurrency Control)\u0026rsquo; has been adopted.\nMVCC is a processing method that refers to the data prior to being updated instead of the latest data that is being updated by another transaction when a transaction sends an inquiry to the database. System throughput improves as the transaction can be executed concurrently by referring to the data prior to the update.\nWhen the transaction process under execution is committed, other transactions can also refer to the latest data.\nLock There is a data lock mechanism to maintain the consistency when there are competing container update requests from multiple transactions. The lock granularity differs depending on the type of container. In addition, the lock range changes depending on the type of operation in the database.\n Lock granularity  A TimeSeries container is a data structure to hold data that is being generated with each passing moment and rarely includes cases in which the data is updated at a specific time. Collection data may include cases in which an existing ROW data is updated as it manages data just like a RDB table.   Based on the use case analysis of such a container, the lock granularity (smallest unit) adopted in GridDB is as follows. The lock granularity of a collection which is updated relatively more frequently is a ROW in order to improve the concurrent execution performance.\n Collection\u0026hellip; Lock by ROW unit TimeSeries container\u0026hellip; Locked by ROW collection  In a row set, multiple rows are placed in a TimeSeries container by dividing a block into several data processing units. This data processing unit is known as a row set. It is a data management unit to process a large volume of TimeSeries containers at a high speed even though the data granularity is coarser than the lock granularity in a collection.   The lock granularity of a collection which is updated randomly more frequently compared to a TimeSeries container collection adopts a row unit in order to improve the concurrent execution performance.\n Lock range by database operations Container operations are not limited to just data registration and deletion but also include schema changes accompanying a change in data structure, index creation to improve speed of access, and other operations. The range of the lock differs between an operation on a specific row of the container and an operation on all rows of the container.  Lock equivalent of a container unit  Index operations (createIndex/dropIndex) Container deletion Schema change  Lock in accordance with the lock granularity  insert/update/remove get (forUpdate)    In a data operation on a row, a lock following the lock granularity is ensured.\n If there is competition in securing the lock, the subsequent transaction will be put in standby for securing the lock until the earlier transaction has been completed by a commit or rollback process and the lock is released. A standby for securing a lock can also be cancelled by a timeout besides completing the execution of the transaction.  Timeout Process The timeout details that can be set differ between a NOSQL interface and a NewSQL interface.\n NoSQL timeout\nThere are 2 types of timeout in a NoSQL that the application developer is kept informed of. There are 2 types of timeout, a transaction timeout that is related to the processing time limit of a transaction and a failover timeout that is related to the retry time of a recovery process when a failure occurs.  TransactionTimeout\nThe timer is started when access to the container subject to the process begins, and a timeout occurs when the specified time is exceeded. Timeout time prepared to delete the lock and memory from a transaction possessing an extended update lock (application searches for data in the update mode and does not delete the data when the lock is maintained) or a transaction maintaining a large amount of results for an extended time (application does not delete the memory of the cluster system for an extended time) and so on. Application is aborted upon reaching the transaction timeout. Besides the node definition file, a transaction timeout can also be specified in the application with a parameter during cluster connection. The specification in the application is prioritized. The default transaction timeout setting is 0 sec. 0 sec means that there is no timeout specified. In order to monitor an extended transaction, set the timeout time to meet the system requirements. FailoverTimeout\nTimeout time during an error retry when a client connected to a node constituting a cluster which failed connects to a replacement node. If a new connection point is discovered in the retry process, the client application will not be notified of the error. Default value is 5 minutes. This can also be specified in the application by a parameter during cluster connection. Failover timeout is also used in timeout during initial connection.  Both the transaction timeout and failover timeout can be set when connecting to a cluster using a GridDB object in the Java API or C API. See GridDB API Reference for details.  "
},
{
	"uri": "http://example.org/sample-applications/basics/",
	"title": "Basics",
	"tags": [],
	"description": "",
	"content": " Chapter 5.1 Basics Showcasing the GridDB API with Sample Applications\n"
},
{
	"uri": "http://example.org/administration/6-6_cluster-administration-operations/",
	"title": "Cluster Administration &amp; Operations",
	"tags": [],
	"description": "",
	"content": " The cluster operation control command interpreter (hereinafter referred to gs_sh) is a command line interface tool to manage GridDB cluster operations and data operations.\nThe following can be carried out by gs_sh.\n Operation control of GridDB cluster  Definition of GridDB cluster Starting and stopping a GridDB node and cluster Status, log display  GridDB cluster data operation  Database and user management Collection, trigger display Index setting, deletion Search using a tql/sql   [Memo]\n All functions provided in the GridDB operating commands are planned to be provided as gs_sh subcommands. Where possible, use of gs_sh is recommended as there is a possibility that the operating commands may be deleted in future releases. See the chapter on “Operating Commands” for details of the operating commands.  Using gs_sh Preliminary Preparations Carry out the following preparations before using gs_sh.\n GridDB setup\n Installation of GridDB node, client library User creation Network setting (GridDB cluster definition file, node definition file)   　See the chapter on “System Design \u0026amp; Construction” in the “GridDB Quick Start Guide” (GridDB_QuickStartGuide.html) for details on the procedure.\n Remote connection setting using SSH\n This setting is necessary in order to connect to each GridDB node execution environment from the gs_sh execution environment as an OS user “gsadm”.   　*See the manual of each OS for details on the SSH connection procedure.\n  gs_sh start-up There are two types of start modes in gs_sh.\n Startup in interactive mode\n The interactive mode is started when gs_sh is executed without any arguments. The gs_sh prompt will appear, allowing subcommands to be entered. Example:   $ gs_sh // execution of subcommand “version” gs\u0026gt; version gs_sh version 2.0.0\n[Memo]\n When a subcommand is started in the interactive mode,  a .gssh_history file is created in the home directory of the execution user and saved in the history. Click the arrow key to display/execute up to 20 subcommands started earlier. Enter some of the subcommands and click the Tab key to display a list of the subcommand input candidates.   Startup in batch mode\n When the script file for user creation is specified in gs_sh, the system will be started in the batch mode. Batch processing of a series of subcommands described in the script file will be carried out. gs_sh will terminate at the end of the batch processing. Example:   // specify the script file (test.gsh) and execute $ gs_sh test.gsh\n  [Memo]\n Execute gs_sh commands as the OS user “gsadm”. During gs_sh startup, .gsshrc script files under the gsadm user home directory are imported automatically. The .gsshrc contents will also be imported to the destination from other script files. Extension of script file is gsh. A script file is described using the character code UTF-8.  Definition of a GridDB Cluster The definition below is required in advance when executing a GridDB cluster operation control or data operation.\n Define each node data in the node variable Use the node variable to define the GridDB cluster configuration in the cluster variable Define the user data of the GridDB cluster  An explanation of node variables, cluster variables, and how to define user data is given below. An explanation of the definition of an arbitrary variable, display of variable definition details, and how to save and import variable definition details in a script file is also given below.\nDefinition of Node Variable Define the IP address and port no. of a GridDB node in the node variable.\n Subcommand\nsetnode\nNode variable name IP address and port no. [SSH port no.]\n Description of each argument\nArgument\nDescription\nNode variable name\nSpecify the node variable name. If the same variable name already exists, its definition will be overwritten.\nIP address\nSpecify the IP address of the GridDB node (for connecting operation control tools).\nPort no.\nSpecify the port no. of the GridDB node (for connecting operation control tools).\nSSH port no.\nSpecify the SSH port number. Number 22 is used by default.\n Example:\n//Define 4 GridDB nodes gs\u0026gt; setnode node0 192.168.0.1 10000 gs\u0026gt; setnode node1 192.168.0.2 10000 gs\u0026gt; setnode node2 192.168.0.3 10000 gs\u0026gt; setnode node3 192.168.0.4 10000\n  [Memo]\n Only single-byte alphanumeric characters and the symbol \u0026ldquo;_\u0026rdquo; can be used in the node variable name. Check the GridDB node “IP address” and “port no. ” for connecting the operation control tools in the node definition file of each tool.  “IP address”: /system/serviceAddress “Port no.” : /system/servicePort   Definition of Cluster Variable Define the GridDB cluster configuration in the cluster variable.\n Subcommand\nsetcluster\n[\u0026lt; Node variable\u0026gt;\u0026hellip;]\nsetcluster\nFIXED_LIST \u0026lt; Address list of fixed list method\u0026gt; [\u0026hellip;]\nsetcluster\nPROVIDER [\u0026hellip;]\n Description of each argument\nArgument\nDescription\nCluster variable name\nSpecify the cluster variable name. If the same variable name already exists, its definition will be overwritten.\nCluster name\nSpecify the cluster name.\nMulticast address\n[For the multicast method] Specify the GridDB cluster multicast address (for client connection).\nPort no.\n[For the multicast method] Specify the GridDB cluster multicast port no. (for client connection).\nNode variable\nSpecify the nodes constituting a GridDB cluster with a node variable.\nWhen using a cluster variable in a data operation subcommand, the node variable may be omitted.\nAddress list of fixed list method\n[For fixed list method] Specify the list of transaction addresses and ports for cluster.notificationMember in gs_cluster.json Example: 192.168.15.10:10001,192.168.15.11:10001\nURL of provider method\n[For provider method] Specify the value of cluster.notificationProvider in gs_cluster.json.\n Example:\n// define the GridDB cluster configuration gs\u0026gt; setcluster cluster0 name 200.0.0.1 1000 $node0 $node1 $node2\n  [Memo]\n Only single-byte alphanumeric characters and the symbol \u0026ldquo;_\u0026rdquo; can be used in the cluster variable name. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a node variable. Check the “cluster name”, “multicast address” and “port no.” defined in a cluster variable in the cluster definition file of each GridDB node.\n “Cluster name”: /clustergs/clusterName “Multicast address”: /transaction/notificationAddress “Port no.”: /transaction/notificationPort   *All settings in the cluster definition file of a node constituting a GridDB cluster have to be configured the same way. If the settings are configured differently, the cluster cannot be composed.\n  In addition, node variables can be added or deleted for a defined cluster variable.\n Subcommand\nmodcluster\nAdd cluster variable name｜remove node variable name\u0026hellip;\n Description of each argument\nArgument\nDescription\nCluster variable name\nSpecify the name of a cluster variable to add or delete a node.\nadd｜remove\nSpecify add when adding a node variable, and remove when deleting a node variable.\nNode variable\nSpecify a node variable to add or delete a cluster variable.\n Example:\n//Add a node to a defined GridDB cluster configuration gs\u0026gt; modcluster cluster0 add $node3 //Delete a node from a defined GridDB cluster configuration gs\u0026gt; modcluster cluster0 remove $node3\n  [Memo]\n Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a node variable.  Defining the SQL Connection Destination of a Cluster Define the SQL connection destination in the GridDB cluster configuration. This is set up only when using the GridDB Advanced Edition NewSQL interface.\n Subcommand\nsetclustersql\nsetclustersql\nFIXED_LIST \u0026lt; SQL address list of fixed list method\u0026gt;\nsetclustersql\nPROVIDER  Description of each argument\nArgument\nDescription\nCluster variable name\nSpecify the cluster variable name. If the same variable name already exists, the SQL connection data will be overwritten.\nCluster name\nSpecify the cluster name.\nSQL address\n[For multicast method] Specify the reception address for the SQL client connection.\nSQL port no.\n[For multicast method] Specify the port no. for the SQL client connection.\nSQL address list of fixed list method\n[For fixed list method] Specify the list of sql addresses and ports for cluster.notificationMember in gs_cluster.json Example: 192.168.15.10:20001,192.168.15.11:20001\nURL of provider method\n[For provider method] Specify the value of cluster.notificationProvider in gs_cluster.json.\n Example:\n//Definition method when using both NoSQL interface and NewSQL interface to connect to a NewSQL server gs\u0026gt; setcluster cluster0 name 239.0.0.1 31999 $node0 $node1 $node2 gs\u0026gt; setclustersql cluster0 name 239.0.0.1 41999\n  [Memo]\n Only single-byte alphanumeric characters and the symbol \u0026ldquo;_\u0026rdquo; can be used in the cluster variable name. This is set up only when using the GridDB Advanced Edition NewSQL interface. When an existing cluster variable name is specified, only the section containing SQL connection data will be overwritten. When overwriting, the same method as the existing connection method needs to be specified. Execute only this command when using SQL only. Check the “SQL address” and “SQL port no.” defined in a cluster variable in the cluster definition file of each GridDB node.  “SQL address”: /sql/notificationAddress “SQL port no.”: /sql/notificationPort   Definition of a user Define the user and password to access the GridDB cluster.\n Subcommand\nsetuser\nUser name and password [gsadm password]\n Description of each argument\nArgument\nDescription\nUser name\nSpecify the name of the user accessing the GridDB cluster.\nPassword\nSpecify the corresponding password.\ngsadm password\nSpecify the password of the OS user gs_admin.\nThis may be omitted if start node (startnode subcommand) is not going to be executed.\n Example:\n//Define the user, password and gsadm password to access a GridDB cluster gs\u0026gt; setuser admin admin gsadm\n  [Memo]\n The user definition is divided and stored in the variable below.\nVariable Name\nStorage data\nuser\nUser name\npassword\nPassword\nospassword\ngsadm password\n Multiple users cannot be defined. The user and password defined earlier will be overwritten. When operating multiple GridDB clusters in gs_sh, reset the user and password with the setuser subcommand every time the connection destination cluster is changed.\n  Definition of arbitrary variables Define an arbitrary variable.\n Subcommand\nset\nVariable name [value]\n Description of each argument\nArgument\nDescription\nVariable Name\nSpecify the variable name.\nValue\nSpecify the setting value. The setting value of the variable concerned can be cleared by omitting the specification.\n Example:\n// define variable gs\u0026gt; set GS_PORT 10000 // clear variable settings gs\u0026gt; set GS_PORT\n  [Memo]\n Node variable and cluster variable settings can also be cleared with the set subcommand. Only single-byte alphanumeric characters and the symbol \u0026ldquo;_\u0026rdquo; can be used in the variable name.  Displaying the variable definition Display the detailed definition of the specified variable.\n Subcommand\nshow\n[Variable name]\n Description of each argument\nArgument\nDescription\nVariable Name\nSpecify the name of the variable to display the definition details. If the name is not specified, details of all defined variables will be displayed.\n Example:\n//Display all defined variables gs\u0026gt; show Node variable: node0=Node[192.168.0.1:10000,ssh=22] node1=Node[192.168.0.2:10000,ssh=22] node2=Node[192.168.0.3:10000,ssh=22] node3=Node[192.168.0.4:10000,ssh=22] Cluster variable: cluster0=Cluster[name=name,200.0.0.1:1000,nodes = (node0,node1,node2 Other variables: user=admin password=***** ospassword=*****\n  [Memo]\n Password character string will not appear. Display replaced by \u0026ldquo;*****\u0026ldquo;.  Saving a variable definition in a script file Save the variable definition details in the script file.\n Subcommand\nsave\n[Script file name]\n Description of each argument\nArgument\nDescription\nScript file name\nSpecify the name of the script file serving as the storage destination. Extension of script file is gsh.\nIf the name is not specified, the data will be saved in the .gsshrc file in the gsadm user home directory.\n Example:\n// Save the defined variable in a file gs\u0026gt; save test.gsh\n  [Memo]\n If the storage destination script file does not exist, a new file will be created. If the storage destination script file exists, the contents will be overwritten. A script file is described using the character code UTF-8. Contents related to the user definition (user, password, gsadm password) will not be output to the script file. Contents in the .gsshrc script file will be automatically imported during gs_sh start-up.  Executing a script file Execute a read script file.\n Subcommand\nload\n[Script file name]\n Description of each argument\nArgument\nDescription\nScript file name\nSpecify the script file to execute.\nIf the script file is not specified, the .gsshrc file in the gsadm user home directory will be imported again.\n Example:\n//execute script file gs\u0026gt; load test.gsh\n  [Memo]\n Extension of script file is gsh. A script file is described using the character code UTF-8.  GridDB cluster operation controls The following operations can be executed by the administrator user only as functions to manage GridDB cluster operations.\n GridDB node start, stop, join cluster, leave cluster (startnode/stopnode/joincluster/leavecluster) GridDB cluster operation start, operation stop (startcluster/stopcluster) Increase the number of new nodes in a GridDB cluster (appendcluster) Get various data  Cluster status This section explains the status of a GridDB node and GridDB cluster.\nA cluster is composed of 1 or more nodes.\nThe node status represents the status of the node itself e.g. start or stop etc.\nThe cluster status represents the acceptance status of data operations from a client. Cluster statuses are determined according to the status of the node group constituting the cluster.\nAn example of the change in the node status and cluster status due to a gs_sh subcommand operation is shown below.\nA cluster is composed of 4 nodes.\nWhen the nodes constituting the cluster are started (startnode), the node status changes to “Start”. When the cluster is started after starting the nodes (startcluster), each node status changes to “Join”, and the cluster status also changes to “In Operation”.\nStatus example\nA detailed explanation of the node status and cluster status is given below.\n Node status  Node status changes to “Stop”, “Start” or “Join” depending on whether a node is being started, stopped, joined or detached.\nIf a node has joined a cluster, there are 2 types of node status depending on the status of the joined cluster.\nNode status\nStatus\nStatus name\nDescription\nJoin\nSERVICING\nNode is joined to the cluster, and the status of the joined cluster is “In Operation”\nWAIT\nNode is joined to the cluster, and the status of the joined cluster is “Halted”\nStart\nSTARTED\nNode is started but has not joined a cluster\nSTARTING\nStarting node\nStop\nSTOP\nStop node\nSTOPPING\nStopping node\n　 Cluster status\nGridDB cluster status changes to “Stop”, “Halted” or “In Operation” depending on the operation start/stop status of the GridDB cluster or the join/leave operation of the GridDB node. Data operations from the client can be accepted only when the GridDB cluster status is “In Operation”.\nCluster status\nStatus\nStatus name\nDescription\nOperation\nSERVICE_STABLE\nAll nodes defined in the cluster configuration have joined the cluster\nSERVICE_UNSTABLE\nMore than half the nodes defined in the cluster configuration have joined the cluster\nHalted\nWAIT\nMore than half the nodes defined in the cluster configuration have left the cluster\nINIT_WAIT\n1 or more of the nodes defined in the cluster configuration have left the cluster (when the cluster is operated for the first time, the status will not change to “In Operation” unless all nodes have joined the cluster)\nStop\nSTOP\nAll nodes defined in the cluster configuration have left the cluster\nThe GridDB cluster status will change from “Stop” to “In Operation” when all nodes constituting the GridDB cluster are allowed to join the cluster. In addition, the GridDB cluster status will change to “Halted” when more than half the nodes have left the cluster, and “Stop” when all the nodes have left the cluster.\nJoin and leave operations (which affect the cluster status) can be applied in batch to all the nodes in the cluster, or to individual node.\nWhen all nodes are subject to the operation\nWhen the operating target is a single node\nOperation\nJoin\nstartcluster Batch entry of a group of nodes that are already operating but have not joined the cluster yet.\njoincluster Entry by a node that is in operation but has not joined the cluster yet.\nLeft\nstopcluster Batch detachment of a group of nodes attached to a cluster.\nleavecluster Detachment of a node attached to a cluster.\n  [Memo]\n Join and leave cluster operations can be carried out on nodes that are in operation only. A node which has failed will be detached automatically from the GridDB cluster. The GridDB cluster status can be checked with the cluster status data display subcommand (configcluster).  Details of the various operating methods are explained below.　Starting a node Explanation on how to start a node is shown below.\n Sub-command\nstartnode\nnode variable｜cluster variable [timeout time in sec]\n Description of each argument\nArgument\nDescription\nNode variable｜cluster variable\nSpecify the node to start by its node variable or cluster variable.\nIf the cluster variable is specified, all nodes defined in the cluster variable will be started.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// start node gs\u0026gt; startnode $node1 Node Start node 1. All nodes have been started.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. The cluster start process (startcluster subcommand) can be executed in batches by waiting for the start process to complete.  Stopping a node Explanation on how to stop a node is shown below.\n Sub-command\nstopnode\nNode｜cluster variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nnode｜cluster variable\nSpecify the node to stop by its node variable or cluster variable.\nIf the cluster variable is specified, all nodes defined in the cluster variable will be stopped.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// stop node gs\u0026gt; stopnode $node1 Node Stop Node 1. Node 1 has started the stop process. Waiting for the node stop process to end. All nodes have been stopped.\n  In addition, a specified node can be forced to stop as well.\n Sub-command\nstopnodeforce\nNode｜cluster variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nnode｜cluster variable\nSpecify the node to stop by force by its node variable or cluster variable.\nIf the cluster variable is specified, all nodes defined in the cluster variable will be stopped by force.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// stop node by force gs\u0026gt; stopnodeforce $node1 Node Stop Node 1. Node 1 has started the stop process. Waiting for the node stop process to end. All nodes have been stopped.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. In a stopnode sub-command, nodes which have joined the GridDB cluster cannot be stopped. In a stopnodeforce command, nodes which have joined the GridDB cluster can also be stopped but data may be lost.  Batch entry of nodes in a cluster Explanation on how to add batch nodes into a cluster is shown below. In this case when a group of unattached but operating nodes are added to the cluster, the cluster status will change to In Operation.\n Sub-command\nstartcluster\nCluster variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster by its cluster variable.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// start GridDB cluster gs\u0026gt; startcluster $cluster1 Waiting for cluster to start. Cluster has started.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. To change the status of a GridDB cluster from “Stop” to “In Operation”, all nodes must be allowed to join the cluster. Check beforehand that all nodes constituting the GridDB cluster are in operation.  Batch detachment of nodes from a cluster Explanation on how to perform batch detachment of nodes in the cluster is shown below. To stop a GridDB cluster, simply make the attached nodes leave the cluster using the stopluster command.\n Sub-command\nstopcluster\nCluster variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster by its cluster variable.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// stop GridDB cluster gs\u0026gt; stopcluster $cluster1 Waiting for a cluster to stop. Cluster has stopped.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable.  Node entry in a cluster Explanation on how to attach a node into a cluster is shown below. Use the joincluster command to attach the node.\n Sub-command\njoincluster\nCluster variable node variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster by its cluster variable.\nNode variable\nSpecify the node to join with the node variable.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// start node gs\u0026gt; startnode $node2 Node Start node 2. All nodes have been started. // join node joincluster $cluster1 $node2 Waiting for a node to be joined to a cluster. Node has joined the cluster.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. Only nodes that are in operation can join a GridDB cluster. Check that the nodes joining a cluster are in operation. Use the appendcluster sub-command when adding a node that is not yet defined in the cluster’s configuration to the cluster (the node is not part of the cluster).  Detaching a node from a cluster Explanation on how to remove a node from a cluster is shown below. Use the leavecluster or leavecluster force command to detach the node.\n Sub-command\nleavecluster\nNode variable [Timeout time in sec.]\nleaveclusterforce\nNode variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nNode variable\nSpecify the node to detach with the node variable.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// leave node gs\u0026gt; leavecluster $node2 Waiting for node to separate from cluster Node has separated from cluster.\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. A node can safely leave a GridDB cluster only when the data has been duplicated in other nodes. However, a leavecluster subcommand forces a node to leave regardless of whether the data has been duplicated or not and so there is always a risk of data loss. Use the gs_leavecluster command to detach a node. See the section on “System Design \u0026amp; Construction - Designing Tuning Parameters” in the “GridDB Quick Start Guide” (GridDB_QuickStartGuide.html) for details regarding data duplication. A leaveclusterforce command forces a node to leave a cluster even if there is a risk that data may be lost due to the detachment.  Adding a note to a cluster Explanation on how to append a node to a pre-defined cluster is shown below. Use the appendcluster command to add the node.\n Sub-command\nappendcluster\nCluster variable node variable [Timeout time in sec.]\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster by its cluster variable.\nNode variable\nSpecify the node to join with the node variable.\nTimeout time in sec.\nSpecify the wait time after node start-up is completed.\nImmediate recovery if -1 is specified. Wait with no time limit if nothing is specified, or 0 is specified.\n Example:\n// define node gs\u0026gt; setnode node5 192.168.0.5 10044 // start node gs\u0026gt; startnode $node5 // increase no. of nodes gs\u0026gt; appendcluster $cluster1 $node5 Waiting for a node to be added to a cluster. A node has been added to the cluster. Add node variables$ node5 to cluster variable $cluster1. (Execute a save command when saving changes to a variable. )\nCluster[name=name1,239.0.5.111:33333,nodes=($node1,$node2,$node3,$node4,$node5)]\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. To increase the number of new nodes, all nodes that constitute a GridDB cluster needs to join the cluster. If there is any node detached from the GridDB cluster, re-attach the nodes first. In addition, check that the new node to be added is in operation. When executing an appendcluster subcommand, the node variable to be added is added to the cluster variable automatically. There is no need to manually change the cluster variable definition. If a variable is changed, execute a save command to save the data. Unsaved contents will be discarded. See the chapter on “System Design \u0026amp; Construction” in the “GridDB Quick Start Guide” (GridDB_QuickStartGuide.html) for details on how to set up a new node.  Displaying cluster status data The following command displays the status of a GridDB cluster and each node constituting the cluster.\n Sub-command\nconfigcluster\nCluster variable\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster by its cluster variable.\n Example:\n// display cluster data gs\u0026gt; configcluster $cluster1 Name : cluster1 ClusterName : defaultCluster Designated Node Count : 4 Active Node Count : 4 ClusterStatus : SERVICE_STABLE\nNodes: Name Role Host:Port Status ------------------------------------------------- node1 F 10.45.237.151:10040 SERVICING node2 F 10.45.237.152:10040 SERVICING node3 M 10.45.237.153:10040 SERVICING node4 F 10.45.237.154:10040 SERVICING\n  [Memo]\n Command can be executed by an administrator user only. ClusterStatus will be one of the following.  INIT_WAIT: Waiting for cluster to be composed SERVICE_STABLE: In operation SERVICE_UNSTABLE: Halted (specified number of nodes constituting a cluster has not been reached)  Role will be one of the following.  M: MASTER (master) F: FOLLOWER (follower) S: SUB_CLUSTER (temporary status in a potential master candidate) -: Not in operation   Display of configuration data The following command displays the GridDB cluster configuration data.\n Sub-command\nconfig\nNode variable\n Description of each argument\nArgument\nDescription\nNode variable\nSpecify the node belonging to a GridDB cluster to be displayed with a node variable.\n Example:\n// display cluster configuration data gs\u0026gt; config $node1 { \u0026ldquo;follower\u0026rdquo; : [ { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.151\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040 }, { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.152\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040 }, { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.153\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040 }, { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.154\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040 } ], \u0026ldquo;master\u0026rdquo; : { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.155\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040 }, \u0026ldquo;multicast\u0026rdquo; : { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;239.0.5.111\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 33333 }, \u0026ldquo;self\u0026rdquo; : { \u0026ldquo;address\u0026rdquo; : \u0026ldquo;10.45.237.150\u0026rdquo;, \u0026ldquo;port\u0026rdquo; : 10040, \u0026ldquo;status\u0026rdquo; : \u0026ldquo;ACTIVE\u0026rdquo; } }\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. The output contents differ depending on the version of the GridDB node. Check with the support desk for details.  Status display The following command displays the status of the specified node and the statistical data.\n Sub-command\nstat\nNode variable\n Description of each argument\nArgument\nDescription\nNode variable\nSpecify the node to display by its node variable.\n Example:\n// display node status, statistical data gs\u0026gt; stat $node1 { \u0026ldquo;checkpoint\u0026rdquo; : { \u0026ldquo;archiveLog\u0026rdquo; : 0, \u0026ldquo;backupOperation\u0026rdquo; : 0, \u0026ldquo;duplicateLog\u0026rdquo; : 0, \u0026ldquo;endTime\u0026rdquo; : 1413852025843, \u0026ldquo;mode\u0026rdquo; : \u0026ldquo;NORMAL_CHECKPOINT\u0026rdquo;, : : }\n  [Memo]\n Command can be executed by an administrator user only. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. The output contents differ depending on the version of the GridDB node.  Log display The following command displays the log of the specified node.\n Sub-command\nlogs\nNode variable\n Description of each argument\nArgument\nDescription\nNode variable\nSpecify the node to display by its node variable.\n Example:\n// display log of node gs\u0026gt; logs $node0 2013-02-26T13:45:58.613+0900 c63x64n1 4051 INFO SYSTEM_SERVICE ../server/system_service.cpp void SystemService::joinCluster(const char8_t, uint32_t) line=179 : joinCluster requested (clusterName=\u0026ldquo;defaultCluster\u0026rdquo;, minNodeNum=1) 2013-02-26T13:45:58.616+0900 c63x64n1 4050 INFO SYSTEM_SERVICE ../server/system_service.cpp virtual void SystemService::JoinClusterHandler::callback(EventEngine\u0026amp;, util::StackAllocator\u0026amp;, Event, NodeDescriptor) line=813 : ShutdownClusterHandler called g 2013-02-26T13:45:58.617+0900 c63x64n1 4050 INFO SYSTEM_SERVICE ../server/system_service.cpp void SystemService::completeClusterJoin() line=639 : completeClusterJoin requested 2013-02-26T13:45:58.617+0900 c63x64n1 4050 INFO SYSTEM_SERVICE ../server/system_service.cpp virtual void SystemService::CompleteClusterJoinHandler::callback(EventEngine\u0026amp;, util::StackAllocator\u0026amp;, Event*, NodeDescriptor) line=929 : CompleteClusterJoinHandler called\n  In addition, the log output level can be displayed and changed.\nlogconf\nNode variable [category name [log level]]\n Description of each argument\nArgument\nDescription\nNode variable\nSpecify the node to operate by its node variable.\nCategory name\nSpecify the log category name subject to the operation. Output level of all log categories will be displayed by default.\nLog level\nSpecify the log level to change the log level of the specified category.\nLog level of the specified category will be displayed by default.\n Example:\n// display log level of node gs\u0026gt; logconf $node0 { \u0026ldquo;CHECKPOINT_SERVICE\u0026rdquo; : \u0026ldquo;INFO\u0026rdquo;, \u0026ldquo;CHUNK_MANAGER\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;CLUSTER_OPERATION\u0026rdquo; : \u0026ldquo;INFO\u0026rdquo;, \u0026ldquo;CLUSTER_SERVICE\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;COLLECTION\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;DATA_STORE\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;EVENT_ENGINE\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;HASH_MAP\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;IO_MONITOR\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;LOG_MANAGER\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;MAIN\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;OBJECT_MANAGER\u0026rdquo; : \u0026ldquo;INFO\u0026rdquo;, \u0026ldquo;RECOVERY_MANAGER\u0026rdquo; : \u0026ldquo;INFO\u0026rdquo;, \u0026ldquo;REPLICATION\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;REPLICATION_TIMEOUT\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;SESSION_TIMEOUT\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;SYNC_SERVICE\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;SYSTEM_SERVICE\u0026rdquo; : \u0026ldquo;INFO\u0026rdquo;, \u0026ldquo;TIME_SERIES\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;TRANSACTION_MANAGER\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;TRANSACTION_SERVICE\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo;, \u0026ldquo;TRANSACTION_TIMEOUT\u0026rdquo; : \u0026ldquo;WARNING\u0026rdquo;, \u0026ldquo;TRIGGER_SERVICE\u0026rdquo; : \u0026ldquo;ERROR\u0026rdquo; }\n  [Memo]\n Command can be executed by an administrator user only. Log levels are ERROR, WARNING, INFO, and DEBUG. Be sure to follow the instructions of the support desk when changing the log level. Log level is initialized by restarting the node. Changes to the log level are not saved. Batch changes cannot be made to the log level of multiple categories. The output contents differ depending on the version of the GridDB node. Check with the support desk for details.  Data Operation in a Database To execute a data operation, there is a need to connect to the cluster subject to the operation.\nData in the database configured during the connection (“public” when the database name is omitted) will be subject to the operation.\nConnecting to a Cluster The following command establishes connection to a GridDB cluster to execute a data operation.\n Sub-command\nconnect\nCluster variable [database name]\n Description of each argument\nArgument\nDescription\nCluster variable\nSpecify a GridDB cluster serving as the connection destination by its cluster variable.\nDatabase name\nSpecify the database name.\n Example:\n// connect to GridDB cluster // for NoSQL gs\u0026gt; connect $cluster1 Connection successful (NoSQL). gs[public]\u0026gt;\ngs\u0026gt; connect $cluster1 userDB Connection successful (NoSQL). gs[userDB]\u0026gt;\n// for NewSQL (configure both NoSQL/NewSQL interfaces) gs\u0026gt; connect $cluster1 Connection successful (NoSQL). Connection successful (NewSQL). gs[public]\u0026gt;\n  [Memo]\n Connect to the database when the database name is specified. Connect to the “public” database if the database name is omitted. If the connection is successful, the connection destination database name appears in the prompt. Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable. When executing a data operation subcommand, it is necessary to connect to a GridDB cluster. If the SQL connection destination is specified (execution of setclustersql sub-command), SQL connection is also carried out.  Search (TQL) The following command will execute a search and retain the search results.\n Sub-command\ntql\nContainer name query;\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the container subject to the search.\nQuery;\nSpecify the TQL command to execute. A semicolon (;) is required at the end of a TQL command.\n Example:\n// execute search gs[public]\u0026gt; tql c001 select *; 5 hits located.\n  [Memo]\n When executing a data operation subcommand, it is necessary to connect to a GridDB cluster. A return can be inserted in the middle of a TQL command. Retain the latest search result. Search results are discarded when a tql or sql subcommand is executed. See the chapter on “TQL Syntax \u0026amp; Operating Functions” in the “GridDB API Reference” (GridDB_API_Reference.html) for the TQL details.  SQL Command Execution The following command executes an SQL command and retains the search result. This function can be executed in the GridDB Advanced Edition only.\n Sub-command\nsql\nSQL command;\n Description of each argument\nArgument\nDescription\nSQL command;\nSpecify the SQL command to execute. A semicolon (;) is required at the end of the SQL command.\n Example:\ngs[public]\u0026gt; sql select * from con1; → search for SQL 10,000 hits located. gs[public]\u0026gt; get 1 → display SQL results id,name ---------------------- 0,tanaka\n  [Memo]\n Can be executed in the GridDB Advanced Edition only. Before executing a sql command, there is a need to specify the SQL connection destination and perform a connection first. Retain the latest search result. Search results are discarded when a sql or tql sub-command is executed. The following results will appear depending on the type of SQL command.\nProcess\nExecution results when terminated normally\nSearch SELECT\nDisplay the no. of search results found. Search results are displayed in sub-command get/getcsv/getnoprint.\nUpdate INSERT/UPDATE/DELETE\nDisplay the no. of rows updated.\nDDL text\nNothing is displayed.\n See “GridDB API Reference” (GridDB_API_Reference.html) for the SQL details.\n  Getting Search Results The following command gets the inquiry results and presents them in different formats. There are 3 ways to output the results as listed below.\n(A) Display the results obtained in a standard output.\n Sub-command\nget\n[No. of cases acquired]\n Description of each argument\nArgument\nDescription\nNo. of cases acquired\nSpecify the number of search results to be acquired. All search results will be obtained and displayed by default.\n  (B) Save the results obtained in a file in the CSV format.\n Sub-command\ngetcsv\nCSV file name [No. of search results found]\n Description of each argument\nArgument\nDescription\nFile name\nSpecify the name of the file where the search results are saved.\nNo. of cases acquired\nSpecify the number of search results to be acquired. All search results will be obtained and saved in the file by default.\n  Results obtained in \u0026copy; will not be output.\n Sub-command\ngetnoprint\n[No. of cases acquired]\n Description of each argument\nArgument\nDescription\nNo. of cases acquired\nSpecify the number of search results to be acquired. All search results will be obtained by default.\n Example:\n// execute search gs[public]\u0026gt; tql c001 select *; 5 hits located.\n//Get first result and display gs[public]\u0026gt; get 1 name,status,count mie,true,2 Acquisition of one result completed.\n//Get second and third results and save them in a file gs[public]\u0026gt; getcsv /var/lib/gridstore/test2.csv 2 Acquisition of 2 results completed.\n//Get fourth result gs[public]\u0026gt; getnoprint 1 Acquisition of one result completed.\n//Get fifth result and display gs[public]\u0026gt; get 1 name,status,count akita,true,45 Acquisition of one result completed.\n  [Memo]\n When executing a data operation subcommand, it is necessary to connect to a GridDB cluster. Output the column name to the first row of the search results An error will occur if the search results are obtained when a search has not been conducted, or after all search results have been obtained or discarded.  Getting the Execution Plan The following command displays the execution plan of the specified TQL command. Search is not executed.\n Sub-command\ntqlexplain\nContainer name query;\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the target container.\nQuery;\nSpecify the TQL command to get the execution plan. A semicolon (;) is required at the end of a TQL command.\n Example:\n//Get execution plan gs[public]\u0026gt; tqlexplain c001 select * ; 0 0 SELECTION CONDITION NULL 1 1 INDEX BTREE ROWMAP 2 0 QUERY_EXECUTE_RESULT_ROWS INTEGER 0\n  In addition, the actual measurement values such as the number of processing rows etc. can also be displayed together with the executive plan by actually executing the specified TQL command.\n Sub-command\ntqlanalyze\nContainer name query;\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the target container.\nQuery;\nSpecify the TQL command to get the execution plan. A semicolon (;) is required at the end of a TQL command.\n Example:\n// Execute search to get execution plan gs[public]\u0026gt; tqlanalyze c001 select *; 0 0 SELECTION CONDITION NULL 1 1 INDEX BTREE ROWMAP 2 0 QUERY_EXECUTE_RESULT_ROWS INTEGER 5 3 0 QUERY_RESULT_TYPE STRING RESULT_ROW_ID_SET 4 0 QUERY_RESULT_ROWS INTEGER 5\n  [Memo]\n When executing a data operation sub-command, it is necessary to connect to a GridDB cluster. See the chapter on “TQL Syntax \u0026amp; Operating Functions” in the “GridDB API Reference” (GridDB_API_Reference.html) for the detailed execution plan. Since search results are not retained, search results cannot be acquired and thus there is also no need to execute a tqlclose subcommand. When the search results are required, execute a query with the tql subcommand.  Discarding Search Results Close the tql and discard the search results saved.\n Sub-command\ntqlclose\n  Close query. Discard the search results retained. 。\n Sub-command\nqueryclose\n Example:\n//Discard search results gs[public]\u0026gt; tqlclose\ngs[public]\u0026gt; queryclose\n  [Memo]\n Search results are discarded at the following timing.  When a tqlclose or query close sub-command is executed When executing a new search using a tql or sql sub-command When disconnecting from a GridDB cluster using a disconnect sub-command  An error will occur if search results are acquired (get sub-command, etc.) after they have been discarded.  Disconnecting from a Cluster The following command disconnect user from a GridDB cluster.\n Sub-command\ndisconnect\n Example:\n//Disconnect from a GridDB cluster gs[public]\u0026gt; disconnect gs\u0026gt;\n  [Memo]\n Retained search results are discarded. When disconnected, the connection database name will disappear from the prompt.  Database Management This section explains the available sub-commands that can be used for database management. Connect to the cluster first prior to performing database management (sub-command connect).\nCreating a Database The following command is used to create a database.\n Sub-command\ncreatedatabase\nDatabase name\n Description of each argument\nArgument\nDescription\nDatabase name\nSpecify the name of the database to be created.\n Example:\n//Create a database with the name “db1” gs[public]\u0026gt; createdatabase db1\n  [Memo]\n Command can be executed by an administrator user only. Only the administrator user can access a database immediately after it has been created. Assign access rights to general users where necessary.  Deleting a Database The following command is used to delete a database.\n Sub-command\ndropdatabase\nDatabase name\n Description of each argument\nArgument\nDescription\nDatabase name\nSpecify the name of the database to be deleted.\n Example:\n//Delete databases like those shown below //db1: No container exists in the database //db2: Database does not exist //db3: Container exists in the database\ngs[public]\u0026gt; dropdatabase db1　// normal termination gs[public]\u0026gt; dropdatabase db2　// abnormal termination D20340: Database \u0026ldquo;db2\u0026rdquo; does not exist. gs[public]\u0026gt; dropdatabase db3　// abnormal termination D20336: Error occurred in deleting the database : msg=[[145045:JC_DATABASE_NOT_EMPTY] Illegal target error by non-empty database.]\n  [Memo]\n Command can be executed by an administrator user only. A public database which is a default connection destination cannot be deleted.  Current DB Display The following command is used to display the current database name.\n Sub-command\ngetcurrentdatabase\n　 Example:\ngs[db1]\u0026gt; getcurrentdatabase db1\n  Database List The following command is used to display the database list and access rights data.\n Sub-command\nshowdatabase\n[Database name]\n Description of each argument\nArgument\nDescription\nDatabase name\nSpecify the name of the database to be displayed.\n Example:\ngs[public]\u0026gt; showdatabase database ACL ---------------------------- public ALL_USER db1 user1 db2 user1 db3 user3\ngs[public]\u0026gt; showdatabase db1 database ACL ---------------------------- public ALL_USER db1 user1\n  [Memo]\n For general users, only databases for which access rights have been assigned will be displayed. For administrator users, a list of all the databases will be displayed.  Assignment of Access Rights The following command is used to assign access rights to the database.\n Sub-command\ngrant\nDatabase name user name\n Description of each argument\nArgument\nDescription\nDatabase name\nSpecify the name of the database for which access rights are going to be assigned\nUser name\nSpecify the name of the user to assign access rights to.\n Example:\ngs[public]\u0026gt; grant db1 user001\n  [Memo]\n Command can be executed by an administrator user only. An error will occur if access rights have already been assigned (only 1 user can be assigned access rights to each database). Execute this command after revoking the access rights (\u0026ldquo;revoke\u0026rdquo; command).  Revoking of Access Rights The following command is used to revoke access rights to the database.\n Sub-command\nrevoke\nDatabase name user name\n Description of each argument\nArgument\nDescription\nDatabase name\nSpecify the name of the database for which access rights are going to be revoked.\nUser name\nSpecify the name of the user whose access rights are going to be revoked.\n Example:\ngs[public]\u0026gt; revoke db1 user001\n  [Memo]\n Command can be executed by an administrator user only.  User Management This section explains the available sub-commands that can be used to perform user management. Connect to the cluster first prior to performing user management (sub-command connect).\nCreating a General User The following command is used to create a general user (username and password).\n Sub-command\ncreateuser\nUser name password\n Description of each argument\nArgument\nDescription\nUser name\nSpecify the name of the user to be created.\nPassword\nSpecify the password of the user to be created.\n Example:\ngs[public]\u0026gt; createuser user01 pass001\n  [Memo]\n Command can be executed by an administrator user only. A name starting with \u0026ldquo;gs#\u0026rdquo; cannot be specified as the name of a general user as it is reserved for use by the administrator user. When creating an administrator user, use the gs_adduser command in all the nodes constituting the cluster.  Deleting a General User The following command is used to delete a user.\n Sub-command\ndropuser\nUser name\n Description of each argument\nArgument\nDescription\nUser name\nSpecify the name of the user to be deleted.\n Example:\ngs[public]\u0026gt; dropuer user01\n  [Memo]\n Command can be executed by an administrator user only.  Update Password The following command is used to update the user password.\n Sub-command\nsetpassword\n　Password (general user only)\nsetpassword\n　User name password (administrator user only)\n Description of each argument\nArgument\nDescription\nPassword\nSpecify the password to change.\nUser name\nSpecify the name of the user whose password is going to be changed.\n  [Memo]\n The general user can change its own password only. An administrator user can change the passwords of other general users only.\n Example:   gs[public]\u0026gt; setpassword newPass009\n  General User List The following command displays the user data.\n Sub-command\nshowuser\n[User name]\n Description of each argument\nArgument\nDescription\nUser name\nSpecify the name of the user to be displayed.\n Example:\ngs[public]\u0026gt; showuser UserName ------------------------------------ user002 user001 user003\ngs[public]\u0026gt; showuser user001 Name : user001 GrantedDB: public, userDB\n  [Memo]\n Command can be executed by an administrator user only.  Container Management This section explains the available sub-commands that can be used when performing container operations. Connect to the cluster first before performing container management. Containers in the database at the (sub-command connect) connection destination will be subject to the operation.\nContainer Creation Create a container.\n Sub-command\n Simplified version\nContainer (collection)\n　createcollection\nContainer name Column name Type [Column name Type …]\nContainer (time series container)\n　createtimeseries\nContainer name Compression method Column name type [Column name Type …]\n Detailed version\n　createcontainer\nContainer data file [Container name]\n  Description of each argument\nArgument\nDescription\nContainer name\nSpecify the name of the container to be created. If the name is omitted in the createcontainer command, a container with the name given in the container data file will be created.\nColumn name\nSpecify the column name.\nType\nSpecify the column type.\nCompression method\nFor time series data, specify the data compression method.\nContainer definition file\nSpecify the file storing the container data in JSON format.\nSimplified version\nSpecify the container name and column data (column name and type) to create the container. The compression type can also be specified for time series containers only.\n Specify \u0026ldquo;NO\u0026rdquo;, \u0026ldquo;SS\u0026rdquo; for the compression method. Use the detailed version if \u0026ldquo;HI\u0026rdquo; is specified. The collection will be created with a specified row key. The first column will become the row key.   Detailed version\nSpecify the container definition data in the json file to create a container.\n The container definition data has the same definition as the metadata file output by the export tool. See Metadata files with the container data file format for the column type and data compression method, container definition format, etc. However, the following data will be invalid in this command even though it is defined in the metadata file of the export command.  version Export tool version database Database name containerFileType Export data file type containerFile Export file name partitionNo Partition no.  Describe a single container definition in a single container definition file. If the container name is omitted in the argument, create the container with the name described in the container definition file. If the container name is omitted in the argument, ignore the container name in the container definition file and create the container with the name described in the argument. An error will not result even if the database name is described in the container definition file but the name will be ignored and the container will be created in the database currently being connected. When using the container definition file, the metadata file will be output when the \u0026ndash;out option is specified in the export function. The output metadata file can be edited and used as a container definition file.   　Example: When using the output metadata file as a container definition file\n{ \u0026ldquo;version\u0026rdquo;:\u0026ldquo;2.1.00\u0026rdquo;,　←invalid \u0026ldquo;container\u0026rdquo;:\u0026ldquo;container_354\u0026rdquo;, \u0026ldquo;database\u0026rdquo;:\u0026ldquo;db2\u0026rdquo;,　←invalid \u0026ldquo;containerType\u0026rdquo;:\u0026ldquo;TIME_SERIES\u0026rdquo;, \u0026ldquo;containerFileType\u0026rdquo;:\u0026ldquo;binary\u0026rdquo;,　←invalid \u0026ldquo;containerFile\u0026rdquo;:\u0026ldquo;20141219_114232_098_div1.mc\u0026rdquo;, ←invalid \u0026ldquo;rowKeyAssigned\u0026rdquo;:true, \u0026ldquo;partitionNo\u0026rdquo;:0,　←invalid \u0026ldquo;columnSet\u0026rdquo;:[ { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;timestamp\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;timestamp\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;active\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;boolean\u0026rdquo; }, { \u0026ldquo;columnName\u0026rdquo;:\u0026ldquo;voltage\u0026rdquo;, \u0026ldquo;type\u0026rdquo;:\u0026ldquo;double\u0026rdquo; } ], \u0026ldquo;timeSeriesProperties\u0026rdquo;:{ \u0026ldquo;compressionMethod\u0026rdquo;:\u0026ldquo;NO\u0026rdquo;, \u0026ldquo;compressionWindowSize\u0026rdquo;:-1, \u0026ldquo;compressionWindowSizeUnit\u0026rdquo;:\u0026ldquo;null\u0026rdquo;, \u0026ldquo;expirationDivisionCount\u0026rdquo;:8, \u0026ldquo;rowExpirationElapsedTime\u0026rdquo;:-1, \u0026ldquo;rowExpirationTimeUnit\u0026rdquo;:\u0026ldquo;null\u0026rdquo; }, \u0026ldquo;compressionInfoSet\u0026rdquo;:[ ]\nContainer Deletion The following command is used to delete a container.\n Sub-command\ndropcontainer\nContainer name\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the name of the container to be deleted.\n Example:\ngs[public]\u0026gt; dropcontainer　Con001\n  Container Indication The following command is used to display the container data.\n Sub-command\nshowcontainer\nContainer name\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the container name to be displayed. Display a list of all containers if omitted.\n Example:\n// display container list gs[userDB]\u0026gt; showcontainer Database : userDB Name Type PartitionId ------------------------------------------------ cont001 COLLECTION 10 col00a COLLECTION 3 time02 TIME_SERIES 5 cont003 COLLECTION 15 cont005 TIME_SERIES 17\n// display data of specified container gs[public]\u0026gt; showcontainer cont003 Database : userDB Name : cont003 Type : COLLECTION Partition ID: 15 DataAffinity: -\nColumns: No Name Type Index ------------------------------------------------------------ 0 col1 INTEGER [TREE] (RowKey) 1 col2 STRING [] 2 col3 TIMESTAMP []\n  [Memo]\n The data displayed in a container list are the “Container name”, “Container type” and “Partition ID”. The data displayed in the specified container are the “Container name”, “Container type”, “Partition ID”, “Defined column name”, “Column data type” and “Column index setting”. Container data of the current DB will be displayed.  Displaying a Table The following command is used to display the table data. This function can be used in the GridDB Advanced Edition only.\n Sub-command\nshowtable\nTable name\n Description of each argument\nArgument\nDescription\nTable name\nSpecify the table name to be displayed. Display a list of all tables if omitted.\n Example:\n// display table list gs[userdb]\u0026gt; showtable Database : userdb Name PartitionId ------------------------------ table09 3 table02 7 table03 12\nTotal Count: 3\n//Display the specified table data gs[userdb]\u0026gt; showtable table09 Database : userdb Name : table09 Partition ID: 3\nColumns: No Name Type Index --------------------------------------------------- 0 col1 INTEGER [TREE] 1 col2 STRING [] 2 col3 STRING []\n  [Memo]\n The data displayed in a table list are the “Table name“ and “Partition ID”. The data displayed in the specified table are the “Table name“, “Partition ID”, “Defined column name“, “Column data type“ and “Column index setting”. Table data of the current DB will be displayed.  Creation of Index The following command is used to create an index in the column of a specified container.\n Sub-command\ncreateindex\nContainer name Column name Index type\u0026hellip;\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the name of container that the column subject to the index operation belongs to.\nColumn name\nSpecify the name of the column subject to the index operation.\nIndex type\u0026hellip;\nSpecify the index type. Specify TREE, HASH or SPATIAL (or multiple) for the index type\n Example:\n// create index gs[public]\u0026gt; createindex cont003 col2 tree hash\ngs[public]\u0026gt; showcontainer cont003 Database : public Name : cont003 Type : COLLECTION Partition ID: 15 DataAffinity: -\nColumns: No Name Type Index ------------------------------------------------------------ 0 col1 INTEGER [TREE] (RowKey) 1 col2 STRING [TREE, HASH] 2 col3 TIMESTAMP []\n  [Memo]\n An error will not occur even if an index that has already been created is specified.  Deletion of Index The following command is used to delete the index in the column of a specified container.\n Sub-command\ndropindex\nContainer name Column name Index type\u0026hellip;\n Description of each argument\nArgument\nDescription\nContainer name\nSpecify the name of container that the column subject to the index operation belongs to.\nColumn name\nSpecify the name of the column subject to the index operation.\nIndex type\u0026hellip;\nSpecify the index type. Specify TREE, HASH or SPATIAL (or multiple) for the index type\n Example:\n//deletion of index gs[public]\u0026gt; showcontainer cont003 Database : public Name : cont003 Type : COLLECTION Partition ID: 15 DataAffinity: -\nColumns: No Name Type Index ------------------------------------------------------------ 0 col1 INTEGER [TREE] (RowKey) 1 col2 STRING [TREE, HASH] 2 col3 TIMESTAMP [HASH]\ngs[public]\u0026gt; dropindex cont003 col2 hash\ngs[public]\u0026gt; showcontainer cont003 Database : public Name : cont003 Type : COLLECTION Partition ID: 15 DataAffinity: -\nColumns: No Name Type Index ------------------------------------------------------------ 0 col1 INTEGER [TREE] (RowKey) 1 col2 STRING [TREE] 2 col3 TIMESTAMP [HASH]\n  [Memo]\n An error will not occur even if an index that has not been created is specified.  Deletion of Trigger The following command is used to delete the trigger of a specified container.\n Sub-command\ndroptrigger\nContainer name Trigger name\n Description of each argument\nArgument\nDescription\nContainer\nSpecify the name of the container whose trigger is going to be deleted.\nTrigger name\nSpecify the trigger name to delete.\n Example:\ngs[public]\u0026gt; droptrigger con01 tri03\n  Display of Trigger The following command is used to display the trigger data of a specified container.\n Sub-command\nshowtrigger\nContainer name [trigger name]\n Description of each argument\nArgument\nDescription\nContainer\nSpecify the container name to be displayed.\nTrigger name\nSpecify the trigger name to be displayed. Display a list of all trigger data if omitted.\n Example:\n//Display the trigger data list of the specified container gs[public]\u0026gt; showtrigger cont003 Name Type Columns Events --------------------------------------------------------------- rtrig01 REST [col1, col3] [PUT]\ngs[public]\u0026gt; showtrigger cont003 rtrig01 Name : rtrig01 Type : REST Target Columns: [col1, col3] Target Events : [PUT]\nDestination URI: http://example.com\n  [Memo]\n The data displayed in a trigger list are the “Trigger name”, “Notification method”, “Column to be notified”, “Operation to be monitored (create new or update, delete a row)”. The data displayed in the specified trigger data are the “Trigger name”, “Notification method”, “Column to be notified”, “Operation to be monitored” and “Notification destination URI”. In addition, the “Destination name”, “Destination type“, “User“ and “Password” are also displayed together in a JMS notification. See the chapter on “Trigger Function” in the “GridDB API Reference” (GridDB_API_Reference.html) for the trigger function details.  Other Operations This section explains the sub-commands for other operations.\nEcho Back Setting The following command is used to display the executed sub-command in the standard output.\n Sub-command\necho\nboolean\n Description of each argument\nArgument\nDescription\nboolean\nDisplay the executed subcommand in the standard output when TRUE is specified Prescribed value is FALSE.\n Example:\n// display the executed subcommand in the standard output gs\u0026gt; echo TRUE\n  [Memo]\n gs_sh prompt \u0026ldquo;gs\u0026gt;\u0026rdquo; always appear in the standard output.  Message Display The following command is used to display the definition details of the specified character string or variable.\n Sub-command\nprint\nMessage\n Description of each argument\nArgument\nDescription\nMessage\nSpecify the character string or variable to display.\n Example:\n// display of character string gs\u0026gt; print print executed. Print executed.\n  [Memo]\n Append \u0026ldquo;$\u0026rdquo; in front of the variable name when using a variable.  Sleep The following command can be used to set the time for the sleeping function.\n Sub-command\nsleep\nNo. of sec\n Description of each argument\nArgument\nDescription\nNo. of sec\nSpecify the no. of sec to go to sleep.\n Example:\n// sleep for 10 sec gs\u0026gt; sleep 10\n  [Memo]\n Specify a positive integer for the no. of sec number.  Execution of External Commands The following command is used to execute an external command.\n Sub-command\nexec\nExternal command [External command argument]\n Description of each argument\nArgument\nDescription\nExternal command\nSpecify an external command.\nExternal command argument\nSpecify the argument of an external command.\n Example:\n// display the file data of the current directory gs\u0026gt; exec ls -la\n  [Memo]\n Pipe, redirect, hear document cannot be used.  Terminating gs_sh The following command is used to terminate gs_sh.\n Sub-command\nexit\nquit\n Example:\n// terminate gs_sh. gs\u0026gt; exit\n  　In addition, if an error occurs in the sub-command, the setting can be configured to end gs_sh.\n Sub-command\nerrexit\nboolean\n Description of each argument\nArgument\nDescription\nboolean\nIf TRUE is specified, gs_sh ends when an error occurs in the sub-command. Default is FALSE.\n Example:\n// configure the setting so as to end gs_sh when an error occurs in the sub-command gs\u0026gt; errexit TRUE\n  [Memo]\n There is no functional difference between the exit sub-command and quit sub-command.  Help The following command is used to display a description of the sub-command. - subcommand\nhelp\n[Sub-command name]\n Description of each argument\nArgument\nDescription\nSub-command name\nSpecify the subcommand name to display the description Display a list of the subcommands if omitted.\n Example:\n// display the description of the subcommand gs\u0026gt; help exit exit The following command is used to terminate gs_sh.\n  [Memo]\n A description of gs_sh can be obtained with the command “gh_sh \u0026ndash;help”.  Version The following command is used to display the version of gs_sh.\n Sub-command\nversion\n Example:\n// display of version gs\u0026gt; version gs_sh version 2.0.0\n  [Memo]\n The gs_sh version data can be obtained with the command “gh_sh \u0026ndash;version” as well.  Options and Subcommand Specifications Options  Command list\ngs_sh\n[Script file]\ngs_sh\n-v｜\u0026ndash;version\ngs_sh\n-h｜\u0026ndash;help\n Optional specifications\nOptional\nEssential\nDescription\n-v｜\u0026ndash;version\nDisplay the version of the tool.\n-h｜\u0026ndash;help\nDisplay the command list as a help message.\n  [Memo]\n In order to batch process the gs_sh sub-command, a script file can be created. Extension of the script file is gsh. During gs_sh startup, .gsshrc script files under the gsadm user home directory are imported automatically. The .gsshrc contents will also be imported to the destination from other script files.  Sub-command List  GridDB cluster definition sub-command list\nSub-command\nArgument\nDescription\n*1\n　setnode\nNode variables name IP address\nDefine the node variable.\n　Port no. [SSH port no.]\n　setcluster\nCluster variable name cluster name　Define the cluster variable.\n　Multicast address port no.\n　[Node variable\u0026hellip; ]\n　setclustersql\nCluster variable name cluster name\nDefine the SQL connection destination in the cluster configuration.\n　SQL address SQL port no.\n　modcluster Cluster variable name\nAdd or delete a node variable to or from the cluster variable.\n　add｜remove node variable\u0026hellip;\n　setuser\nUser name password　Define the user and password to access the cluster.\n　[Password of OS user gsadm]\n　set\nVariable name [value]\nDefine an arbitrary variable.\n　show\n[Variable name]\nDisplay the detailed definition of the variable.\n　save\n[Script file name]\nSave the variable definition in the script file.\n　load\n[Script file name]\nExecute a read script file.\n GridDB cluster operation sub-command list\nSub-command\nArgument\nDescription\n*1\n　startnode\nNode｜cluster variable [Timeout time in sec.]\nExplanation on how to start a node is shown below.\n*\n　stopnode\nNode｜cluster variable [Timeout time in sec.]\nExplanation on how to stop a node is shown below.\n*\n　stopnodeforce\nNode｜cluster variable [Timeout time in sec.]\nStop the specified node by force.\n*\n　startcluster\nCluster variable [Timeout time in sec.]\nAttach the active node groups to a cluster, together at once.\n*\n　stopcluster\nCluster variable [Timeout time in sec.]\nDetach all of the currently attached nodes from a cluster, together at once.\n*\n　joincluster\nCluster variable node variable [Timeout time in sec.]\nAttach a node individually to a cluster.\n*\n　leavecluster\nNode variable [Timeout time in sec.]\nDetach a node individually from a cluster.\n*\n　leaveclusterforce\nNode variable [Timeout time in sec.]\nForce the specified node to leave/get detached from a cluster.\n*\n　appendcluster\nCluster variable node variable [Timeout time in sec.]\nAdd an undefined node to a pre-defined cluster.\n*\n　configcluster\nCluster variable\nDisplay the cluster status data.\n*\n　config\nNode variable\nDisplay the cluster configuration data.\n*\n　stat\nNode variable\nDisplay the status of the specified node.\n*\n　logs\nNode variable\nThe following command displays the log of the specified node.\n*\n　logconf\nNode variable [category name [output level]]\nDisplay and change the log settings.\n*\n  　*1: Commands marked with an * can be executed by an administrator user only.\n Data operation sub-command list in database\nSub-command\nArgument\nDescription\n*1\n　connect\nCluster variable [database name]\nConnect to a GridDB cluster.\n　tql\nContainer name query;\nThe following command will execute a search and retain the search results.\n　get\n[No. of cases acquired]\nGet the search results and display them in a standard output.\n　getcsv\nCSV file name [No. of search results found]\nGet the search results and save them in a file in the CSV format.\n　getnoprint\n[No. of cases acquired]\nGet the query results but do not display them in a standard output.\n　tqlclose\nDiscard the search results retained.\n　tqlanalyze\nContainer name query;\nThe following command displays the execution plan of the specified TQL command.\n　tqlexplain\nContainer name query;\nExecute the specified TQL command and display the execution plan and actual measurement values such as the number of cases processed etc.\n　sql\nSQL command;\nThe following command executes an SQL command and retains the search result.\n　queryclose\nClose SQL.\n　disconnect\nThe following command disconnect user from a GridDB cluster.\n  　*1: Commands marked with an * can be executed by an administrator user only.\n Database management sub-command list\nSub-command\nArgument\nDescription\n*1\n　createdatabase\nDatabase name\nCreate a database.\n*\n　dropdatabase\nDatabase name\nDelete a database.\n*\n　getcurrentdatabase\nDisplay the current database name.\n　showdatabase\n[Database name]\nThe following command is used to display the database list and access rights data.\n　grant\nDatabase name user name\nThe following command is used to assign access rights to the database.\n*\n　revoke\nDatabase name user name\nThe following command is used to revoke access rights to the database.\n*\n  　*1: Commands marked with an * can be executed by an administrator user only.\n User management sub-command list\nSub-command\nArgument\nDescription\n*1\n　createuser\nUser name password\nCreate a general user.\n*\n　dropuser\nUser name\nDelete a general user.\n*\n　setpassword\nPassword\nChange your own password.\n　setpassword\nUser name password\nChange the password of a general user.\n　showuser\n[User name]\nThe following command displays the user data.\n  　*1: Commands marked with an * can be executed by an administrator user only.\n Container management sub-command list\nSub-command\nArgument\nDescription\n*1\n　createcollection\nContainer name Column name Type [Column name Type …]\nCreate a container (collection),\n　createtimeseries\nContainer name Compression method Column name type [Column name Type …]\nCreate a container (time series container).\n　createcontainer\nContainer data file [Container name]\nCreate a container from the container data file.\n　dropcontainer\nContainer name\nThe following command is used to delete a container.\n　showcontainer\n[Container name]\nThe following command is used to display the container data.\n　showtable\n[Table name]\nThe following command is used to display the table data.\n　createindex\nContainer name Column name Index type\u0026hellip;\nCreate an index in the specified column.\n　dropindex\nContainer name Column name Index type\u0026hellip;\nDelete an index of the specified column.\n　droptrigger\nContainer name Trigger name\nDelete the trigger data.\n　showtrigger\nContainer name [trigger name]\nDisplay the trigger data.\n  　*1: Commands marked with an * can be executed by an administrator user only.\n Other operation sub-command list\nSub-command\nArgument\nDescription\n*1\n　echo\nboolean\nSet whether to echo back.\n　print\nMessage\nThe following command is used to display the definition details of the specified character string or variable.\n　sleep\nNo. of sec\nThe following command can be used to set the time for the sleeping function.\n　exec\nExternal command [External command argument]\nThe following command is used to execute an external command.\n　exit\nThe following command is used to terminate gs_sh.\n　quit\nThe following command is used to terminate gs_sh.\n　errexit\nboolean\nSet whether to terminate gs_sh when an error occurs.\n　help\n[Subcommand name]\nThe following command is used to display a description of the sub-command.\n　version\nDisplay the version data.\n  　*1: Commands marked with an * can be executed by an administrator user only.\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-6_indexes/",
	"title": "Indexes",
	"tags": [],
	"description": "",
	"content": "Indexes exist to make queries and searches more efficient. When dealing with very large data sets, these indexes save on precious time and resources. A condition-based search can be processed quickly by creating an index for the columns of a container. There are 3 types of index - hash index (HASH), tree index (TREE) and space index (SPATIAL). A hash index is used in an equivalent-value search when searching with a query in a container. Besides equivalent-value search, a tree index is used in comparisons including the range (bigger/same, smaller/same etc.). The index that can be set differs depending on the container type and column data type.\n Hash Index  An equivalent value search can be conducted quickly but this is not suitable for searches that read the rows sequentially. Columns of the following data type can be set in a collection. Cannot be set in a TimeSeries container.  String Bool Byte Short Integer Long Float Double Timestamp   Besides equivalent-value search, a tree index  is used in comparisons including the range (bigger/same, smaller/same etc.). This can be used for columns of the following data type in any type of container, except for columns corresponding to a rowkey in a TimeSeries container.  String Bool Byte Short Integer Long Float Double Timestamp   Space Index  Can be used for only GEOMETRY columns in a collection. This is specified when conducting a spatial search at a high speed.   Although there are no restrictions on the no. of indexes that can be created in a container, creation of an index needs to be carefully designed. An index is updated when the rows of a configured container are inserted, updated or deleted. Therefore, when multiple indexes are created in a column of a row that is updated frequently, this will affect the performance in insertion, update or deletion operations.\nAn index is created in a column as shown below.\n A column that is frequently searched and sorted. A column that is frequently used in the condition of the WHERE section of TQL High cardinality column (containing few duplicated values)  "
},
{
	"uri": "http://example.org/administration/",
	"title": "Administration",
	"tags": [],
	"description": "",
	"content": " Chapter 6 Administration Admin duties for GridDB\n"
},
{
	"uri": "http://example.org/administration/6-7_monitoring-troubleshooting/",
	"title": "Monitoring &amp; Troubleshooting",
	"tags": [],
	"description": "",
	"content": " This chapter describes the troubleshooting procedures of GridDB. It contains information on how to resolve problems which occur when constructing and operating a GridDB’s system.\nThis chapter is written for developers, users and system administrators responsible for GridDB’s operation management.\nThe following subjects are covered in this chapter.\n Introduction Verification of the circumstances under which problems occurred: explains how to check the detailed circumstances (log file). Countermeasures to be adopted when problems occur: contains a list of expected problems and their countermeasures.  Checking the Circumstances Under Which the Problems Occurred Check the process (action) and error code etc. to see what kind of problem has occurred and the circumstances under which the problem occurred. After checking the circumstances, review/execute the “countermeasures (resolution method)”.\nThe circumstances can be checked by either checking the process (action) or by checking the error code.\n Based on the process (action) carried out when the problem occurred and the error notification, select the appropriate “cause” in Countermeasures to be adopted when a problem occurs and study the countermeasures. Based on the error notification, select the appropriate “symptom” in Countermeasures to be adopted when a problem occurs, check the [Description] and “cause” and study the countermeasures.  If the circumstances cannot be narrowed down from the notification error or the process (action) being carried out, check the GridDB status from the log file. See below for the log files such as the event log, client log, etc.\nChecking the Event Log File of a GridDB Server Check whether an error has occurred and its description from the event log file in the server.\nThe contents of the latest event log can be checked with the \u0026ldquo;gs_logs\u0026rdquo; command.\n[Example]\n$ gs_logs -u admin/admin 2015-03-20T10:07:47.219+0900 host01 22962 INFO CHECKPOINT_SERVICE [30902:CP_STATUS] [CP_START] mode=NORMAL_CHECKPOINT, backupPath=\nThe format of the event log file is as follows. Check the server status when an error occurs from the time and other data.\n(Date and time) (host name) (thread no.) (log level) (category) [(error/trace no.): (error/trace no. name)] (message) \u0026lt; (Detailed data: source code position etc. Only when an error occurs)\u0026gt;\nEarlier event log files are output to the event log storage directory (default: \u0026ldquo;/var/lib/gridstore/log\u0026rdquo;). File name is gridstore-log taking date (YYYYMMDD) log. A sequential no. is appended if the log taking date is duplicated.\n[Memo]\n Execute the \u0026ldquo;gs_logconf\u0026rdquo; command when checking or changing the output level of the event log.  Checking the Log File of a Client If an error is detected in the client program such as an application program etc., check the description of the error which occurred.\nIn the client program where Java API is used, the client log can be output to a file by setting the log output, including the login library in the class path of the execution environment.\nFor example, the following log is recorded.\n[Example]\n15/03/25 13:07:56 INFO GridStoreLogger.Transaction: Repairing session (context=7c469c48, statement=PUT_MULTIPLE_ROWS, partition=11, statementId=1, containerId=0, sessionId=1, trialCount=0, transactionStarted=false) com.toshiba.mwcloud.gs.common.GSStatementException: [110016:TM_SESSION_UUID_UNMATCHED] Failed to operate session or transaction status (pId=11, clientId=87a89588-b6e1-4dac-bf42-92fa596b7efb:1, sessionMode=1, txnMode=1, reason=) (address=/192.168.10.10:10001, partitionId=11)\nCompare the event log of the server at the time the error was recorded in the client log and check the surrounding logs to verify the status of the server when the error occurred.\n[Memo]\n An error message may be recorded in the event log if an error were to occur during notification or if the trigger process were to fail. See the paragraph on GridStoreFactory class in the “GridDB API Reference” (GridDB_API_Reference.html) for the client login settings.  Checking the Log File of a JDBC Driver If an error occurs in the GridDB Advanced Edition NewSQL interface e.g. during SQL execution, check the description of the error.\nThe log file below is created under the execution directory of the application, or under the log output directory specified by the application.\n Log file of the JDBC driver is \u0026ldquo;/NewSQLJDBC.log\u0026rdquo;. Log file of the engine is \u0026ldquo;/newsql- log taking date and time (YYYYMMDDHHMMSS).log\u0026rdquo;  A log will not be output by default. If the log file has not been created, set up the log output level and output destination.\nThe log output level and output destination settings are configured from gs.util.Debug.\n(1) Import gs.util.Debug.\n(2) Use setLogLevel() to change the output level of the log.\n　setLogLevel (int log level)\nThere are 4 log output levels as shown below.\n 3: ERROR 2: WARNING 1: INFO 0: DEBUG  When the level is set to low, all logs with a level higher than the level will be output.\n(3) Use setLogFilePath() to set the log output destination. Default directory is the current directory.\n　Debug.setLogFilePath(String output directory)\n[Example] When the log output level is debug, and log output destination is under the jdbcLog folder in the current directory\nimport java.sql.*; import gs.util.Debug;\n// usage: java SampleJDBC (multicastAddress) (port) (clusterName)\npublic class Sample { public static void main(String[] args) { Debug.setLogFilePath(\u0026ldquo;jdbcLog\u0026rdquo;); Debug.setLevel(0) }\nChecking the Log File of an Operating Command If an error occurs in the operating tool (gs_admin), check the description of the error which occurred from the log file of the operating command.\nLog file of operating command is located at \u0026ldquo;\u0026lt; log data output destination (\u0026ldquo;/var/lib/gridstore/log\u0026rdquo; by default)\u0026gt; / operating command name.log\u0026rdquo;.　Example) \u0026ldquo;operating command name.log\u0026rdquo; may be gs_startnode.log or gs_stopcluster.log, gs_stat.log etc.\n[Example] Log file of gs_startnode command file name: gs_startnode.log\n2014-10-20 15:25:16,876 [25444] [INFO] (56) /usr/bin/gs_startnode start. 2014-10-20 15:25:16,877 [25444] [DEBUG] (105) command: [\u0026rsquo;/usr/bin/gsserver\u0026rsquo;, \u0026lsquo;\u0026ndash;conf\u0026rsquo;, \u0026lsquo;/var/lib/gridstore/conf\u0026rsquo;] 2014-10-20 15:25:17,889 [25444] [INFO] (156) wait for starting node. (node=127.0.0.1:10040 waitTime=0) 2014-10-20 15:25:18,905 [25444] [INFO] (192) /usr/bin/gs_startnode end.\n[Memo]\n Event log data of the server is also recommended to be checked together with the data of the operating command.  Troubleshooting Understand the circumstances based on the error notification from the server or application, or the process (action) being executed etc., then select the appropriate problem from the problem and countermeasure list and study the countermeasures.\nProblems and Countermeasures According to the Circumstances Under Which the Problems Occurred Expected problems have been classified as follows according to the circumstances under which they occurred and the countermeasures to resolve these problems have been summarized.\n Problems related to cluster configuration Problems related to cluster expansion, reduction Problems related to client failover Problems related to cluster failure Problems related to recovery processing Problems related to container operation TQL-related problems  Problems Related to Cluster Configuration  List of problems related to cluster configuration  Problems Related to Cluster Expansion, Reduction  List of problems related to cluster expansion, reduction  Problems Related to Client Failover  List of problems related to client failover  Problems Related to Cluster Failure  List of problems related to cluster failure  Problems Related to Recovery Processing  List of problems related to recovery processing  Problems Related to Container Operation  List of problems related to container operations  TQL-Related Problems  List of TQL-related problems  [Reference] Compatibility\n Compatibility between client and DB  General GridDB problems This sheet summarizes the list of problems above according to the circumstances under which the problems occurred. If the area of the problem which occurred can be clearly classified, use key words such as the error code etc. to conduct a search.\nList of general GridDB problems (HTML)\nList of general GridDB problems (PDF)\n[Memo]\n A list of the general GridDB problems which summarizes all the documents on the problems according to the circumstances under which they occurred into a single document.  "
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-7_storage-architecture/",
	"title": "Storage Architecture",
	"tags": [],
	"description": "",
	"content": " GridDB Nodes GridDB is operated by clusters which are composed of multiple nodes. To access the database from an application system, the nodes have to be started up and the cluster has to be constituted (cluster service is executed). A cluster is formed and cluster service is started when a number of nodes specified by the user joins the cluster. Cluster service will not be started and access from the application will not be possible until all nodes constituting a cluster have joined the cluster. A cluster needs to be composed even when operating with 1 node only. In this case, the number of nodes constituting a cluster is 1. A composition that operates a single node is known as a single composition.\nCluster names are used to separate multiple clusters so that the correct clusters (using the intended nodes) can be composed using multiple GridDB nodes on a network. Multiple GridDB clusters can be composed in the same network. A cluster is composed of nodes with the same cluster name, number of nodes constituting a cluster, and with the same multi-cast address setting. When composing a cluster, the parameters need to be specified as well in addition to setting the cluster name in the cluster definition file which is a definition file saved for each node constituting a cluster.\nIn GridDB, there is no SPOF (single point of failure) because the clusters are constantly duplicating data (hereinafter known as replicas). Replicas also allow data-processing to continue, even when failure occurs in any of the nodes constituting the cluster. Special operating procedures are not necessary as the system will also automatically perform re-arrangement of the data after a node failure occurs (autonomous data arrangement). When the data arranged in a failed node is restored from a replica, the data is re-arranged so that the set number of replicas is reached automatically.\nDuplex, triplex or multiplex replica can be set according to the availability requirements.\nEach node performs persistence of the data update information using a disk, and all registered and updated data up to that point in time can be restored without any data being lost even following failure across the entire cluster system.\nIn addition, since the client also possesses cache information on the data arrangement and management, upon detecting a node failure, it will automatically perform a failover and data access can be continued using a replica.\nData perpetuation Data registered or updated in a container or table is perpetuated in the disk or SSD, and protected from data loss when a node failure occurs. There are 2 types of transaction log process, one to synchronize data in a data update and write the updated data sequentially in a transaction log file, and the other is a checkpoint process to store updated data in the memory regularly in the database file on a block basis.\nTo write to a transaction log, either one of the following settings can be made in the node definition file.\n 0: SYNC An integer value of 1 or higher1: DELAYED_SYNC  In the \u0026lsquo;SYNC\u0026rsquo; mode, log writing is carried out synchronously every time an update transaction is committed or aborted. In the \u0026lsquo;DELAYED_SYNC\u0026rsquo; mode, log writing during an update is carried out at a specified delay of several seconds regardless of the update timing. Default value is \u0026lsquo;1 (DELAYED_SYNC 1 sec)\u0026lsquo;.\nWhen \u0026lsquo;SYNC\u0026rsquo; is specified, although the possibility of losing the latest update details when a node failure occurs is lower, the performance is affected in systems that are updated frequently.\nOn the other hand, if \u0026lsquo;DELAYED_SYNC\u0026rsquo; is specified, although the update performance improves, any update details that have not been written in the disk when a node failure occurs will be lost.\nIf there are 2 or more replicas in a raster configuration, the possibility of losing the latest update details when a node failure occurs is lower even if the mode is set to \u0026lsquo;DELAYED_SYNC\u0026rsquo; as the other nodes contain replicas. Consider setting the mode to \u0026lsquo;DELAYED_SYNC\u0026rsquo; as well if the update frequency is high and performance is required. In a checkpoint, the update block is updated in the database file. A checkpoint process operates at the cycle set on a node basis. A checkpoint cycle is set by the parameters in the node definition file. Initial value is 1200 sec (20 minutes).\nBy raising the checkpoint execution cycle figure, data perpetuation can be set to be carried out in a time band when there is relatively more time to do so e.g. by perpetuating data to a disk at night and so on. On the other hand, when the cycle is lengthened, the disadvantage is that the number of transaction log files that have to be rolled forward when a node is restarted outside the system process increases, thereby increasing the recovery time.\nData that is updated in a checkpoint execution is pooled and maintained in a memory separate from the checkpoint writing block. Set checkpoint concurrent execution for the checkpoint to carry out the checkpoint quickly. If concurrent execution is set, concurrent processing is carried out until the number of transactions executed simultaneously is the same.\nIn-Memory Data Overflow In GridDB, most of the data processing is handled by the machine\u0026rsquo;s memory. When configuring the cluster, storeMemoryLimit must be set to some factor, though most typically it is most of the available memory (64GB, for example). All data loaded under that limit is kept in memory and flushed to disk as is needed.\nIf your database begins to grow to a larger dataset than your previously set parameter storeMemoryLimit, GridDB will overflow data not likely to be used soon onto the disk. That disk can be either an SSD or an HDD, but SSDs will provide smaller drop-offs in speed should this process need to occur. This method of handling data ensures most transactions occur in-memory to ensure high performance throughout use.\n"
},
{
	"uri": "http://example.org/drivers-and-integrators/",
	"title": "Drivers &amp; Integrators",
	"tags": [],
	"description": "",
	"content": " Chapter 7 Drivers \u0026amp; Integrators A look at GridDB\u0026rsquo;s drivers and integrators\n"
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-8_replication-distribution/",
	"title": "Replication &amp; Distribution",
	"tags": [],
	"description": "",
	"content": "Data replicas are created on a partition basis in accordance with the number of replications set by the user. A process can be continued non-stop even when a node failure occurs by maintaining replicas of the data among scattered nodes. In the client API, when a node failure is detected, the client automatically switches access to another node where the replica is maintained.\nThe default number of replications is 2, allowing data to be replicated twice when operating in a cluster configuration with multiple nodes. When there is an update in a container, the owner node (the node having the master replica) among the replicated partitions is updated.\nThere are 2 ways of subsequently reflecting the updated details from the owner node in the backup node.\n Replication is carried out without synchronizing with the timing of the non-synchronous replication update process. Update performance is better for quasi-synchronous replication but the availability is worse. Although replication is carried out synchronously at the quasi-synchronous replication update process timing, no appointment is made at the end of the replication. Availability is excellent but performance is inferior.  If performance is more important than availability, set the mode to non-synchronous replication and if availability is more important, set it to quasi-synchronous replication.\n[Memo]\nThe number of replications is set in the cluster definition file (gs_cluster.json) /cluster/replicationNum.\nSynchronous settings of the replication are set in the cluster definition file (gs_cluster.json) /transaction/replicationMode.\n"
},
{
	"uri": "http://example.org/administration/6-8_tuning/",
	"title": "Tuning",
	"tags": [],
	"description": "",
	"content": " After installation, the following settings are necessary in order to operate GridDB.\n Network environment settings Cluster name settings  GridDB settings are configured by editing 2 types of definition files.\n Cluster definition file (gs_cluster.json) Node definition file (gs_node.json)  The cluster definition file defines the parameters that are common in the entire clusters.\nThe node definition files define the parameters for the different settings in each node.\nThese definition file samples are installed as follows.\n/usr/gridstore/ # installation directory conf/ # definition file directory gs_cluster.json # cluster definition file sample gs_node.json # node definition file sample\nIn a new installation, the same files are also placed in the conf directory under the GridDB home directory.\n/var/lib/gridstore/ # GridDB home directory conf/ # definition file directory gs_cluster.json # (edited) cluster definition file gs_node.json # (edited) node definition file\nDuring operations, edit these definition files.\n[Points to note]\n When the GridDB version is upgraded, compare the newly installed sample with these definition files to adequately reflect the parameters added. A cluster definition file defines the parameters that are common in the entire clusters. As a result, the settings must be the same in all of the nodes in the cluster. Nodes with different settings will get an error upon joining the cluster and prevented from participating in the cluster. Further details will be explained in the later chapter.  Network environment settings (essential) First, set up the network environment.\nAn explanation of the recommended configuration method in an environment that allows a multicast to be used is given below. In an environment which does not allow a multicast to be used, or an environment in which communications between fellow nodes cannot be established in a multicast, a cluster configuration method other than the multicast method has to be used. See Other cluster configuration method settings for the details.\nThe configuration items can be broadly divided as follows.\n Address information serving as an interface with the client Address information for cluster administration and processing Address information serving as an interface with the JDBC client (GridDB Advanced Edition only)  Although these settings need to be set to match the environment, default settings will also work.\nHowever, an IP address derived in reverse from the host name of the machine needs to be an address that allows it to be connected from the outside regardless of whether the GridDB cluster has a multiple node configuration or a single node configuration.\nNormally, this can be set by stating the host name and the corresponding IP address in the /etc/hosts file.\n/etc/hosts setting\nFirst, check with the following command to see whether the setting has been configured. If the IP address appears, it means that the setting has already been configured.\n$ hostname -i 192.168.11.10\nThe setting has not been configured in the following cases.\n$ hostname -i hostname: no address corresponding to name\nIn addition, a loopback address that cannot be connected from the outside may appear.\n$ hostname -i 127.0.0.1\nIf the setting has not been configured or if a loopback address appears, use the following example as a reference to configure /etc/hosts. The host name and IP address, and the appropriate network interface card (NIC) differ depending on the environment.\n Check the host name and IP address.\n$ hostname GS_HOST $ ip route | grep eth0 | cut -f 12 -d \u0026ldquo; \u0026rdquo; | tr -d \u0026ldquo;\\n\u0026rdquo; 192.168.11.10\n Add the IP address and corresponding host name checked by the root user to the /etc/hosts file.\n192.168.11.10 GS_HOST\n Check that the settings have been configured correctly.\n$ hostname -i 192.168.11.10\n  *If the displayed setting remains the same as before, it means that a setting higher in priority is given in the /etc/hosts file. Change the priority order appropriately.\nProceed to the next setting after you have confirmed that /etc/hosts has been configured correctly.\n(1) Address information serving as an interface with the client\nIn the address data serving as an interface with the client, there are configuration items in the node definition file and cluster definition file.\nNode definition file\nParameters\nData type\nMeaning\n/transaction/serviceAddress\nstring\nReception address of transaction process\n/transaction/servicePort\nstring\nReception port of transaction process\n/system/serviceAddress\nstring\nConnection address of operation command\n/system/servicePort\nstring\nConnection port of operation command\nThe reception address and port of transaction processes are used to connect individual client to the nodes in the cluster, and to request for the transaction process from the cluster. This address is used when configuring a cluster with a single node, but in the case where multiple nodes are present through API, the address is not used explicitly.\nThe connection address and port of the operating command are used to specify the process request destination of the operation command, as well as the repository information of the integrated operation control GUI.\nThese reception/connection addresses need not be set so long as there is no need to use/separate the use of multiple interfaces.\nCluster definition file\nParameters\nData type\nMeaning\n/transaction/notificationAddress\nstring\nInterface address between client and cluster\n/transaction/notificationPort\nstring\nInterface port between client and cluster\nA multi-cast address and port are specified in the interface address between a client and cluster. This is used by a GridDB cluster to send cluster information to its clients and for the clients to send processing requests via the API to the cluster. See the description of GridStoreFactory class/method in “GridDB API reference” (GridDB_API_Reference.html) for details.\nIt is also used as a connection destination address of the export/import tool, or as repository data of the integrated operation control GUI.\n(2) Address information for cluster administration and processing\nIn the address data for a cluster to autonomously perform cluster administration and processing, there are configuration items in the node definition file and cluster definition file. These addresses are used internally by GridDB to exchange the heart beat (live check among clusters) and information among the clusters. These settings are not necessary so long as the address used is not duplicated with other systems on the same network or when using multiple network interface cards.\nNode definition file\nParameters\nData type\nMeaning\n/cluster/serviceAddress\nstring\nReception address used for cluster administration\n/cluster/servicePort\nstring\nReception port used for cluster administration\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationAddress\nstring\nMulticast address for cluster administration\n/cluster/notificationPort\nstring\nMulticast port for cluster administration\n Although a synchronization process is carried out with a replica when the cluster configuration is changed, a timeout time can be set for the process.  /sync/timeoutInterval   [Points to note]\n An address or port that is not in use except in GridDB has to be set. The same address can be set for the node definition file gs_node.json /transaction/serviceAddress, /system/serviceAddress, and /cluster/serviceAddress for operations to be performed. If a machine has multiple network interfaces, the bandwidth can be increased by assigning a separate address to each respective interface.  The following settings are applicable in the GridDB Advanced Edition only.\n(3) Address information serving as an interface with the JDBC client\nIn the address data serving as an interface with the JDBC/ODBC client, there are configuration items in the node definition file and cluster definition file.\nNode definition file\nParameters\nData type\nMeaning\n/sql/serviceAddress\nstring\nReception address for JDBC/ODBC client connection\n/sql/servicePort\nint\nReception port for JDBC/ODBC client connection\nThe reception address and port of JDBC/ODBC client connection are used to connect JDBC/ODBC individual client to the nodes in the cluster, and to access the cluster data in SQL. This address is used when configuring a cluster with a single node, but in the case where multiple nodes are present through API, the address is not used explicitly.\nCluster definition file\nParameters\nData type\nMeaning\n/sql/notificationAddress\nstring\nAddress for multi-cast distribution to JDBC/ODBC client\n/sql/notificationPort\nint\nMulticast port to JDBC/ODBC client\nThe address and port used for multicast distribution to a JDBC/ODBC client are used for the GridDB cluster to notify the JDBC/ODBC client of cluster data, and to access the cluster data in SQL with the JDBC/ODBC client.\nRefer to Annex Parameter List for the other parameters and default values.\nCluster name settings (essential) Set the name of the cluster to be composed by the target nodes in advance. The name set will be checked to see if it matches the value specified in the command to compose the cluster. As a result, this prevents a different node and cluster from being composed when there is an error in specifying the command.\nThe cluster name is specified in the following configuration items of the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/clusterName\nstring\nName of cluster to create\n[Points to note]\n Node failed to start with default value (\u0026ldquo;\u0026rdquo;). A unique name on the sub-network is recommended. A cluster name is a string composed of 1 or more ASCII alphanumeric characters and the underscore “_”. However, the first character cannot be a number. The name is also not case-sensitive. In addition, it has to be specified within 64 characters.  Settings of other cluster configuration methods In an environment which does not allow the multicast method to be used, configure the cluster using the fixed list method or provider method. An explanation of the respective network settings in the fixed list method and provider method is given below.\nWhen using the multicast method, proceed to Setting the tuning parameters.\n(1) Fixed list method\nWhen a fixed address list is given to start a node, the list is used to compose the cluster.\nWhen composing a cluster using the fixed list method, configure the parameters in the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationMember\nstring\nSpecify the address list when using the fixed list method as the cluster configuration method.\nA configuration example of a cluster definition file is shown below.\n{ : : \u0026ldquo;cluster\u0026rdquo;:{ \u0026ldquo;clusterName\u0026rdquo;:\u0026ldquo;yourClusterName\u0026rdquo;, \u0026ldquo;replicationNum\u0026rdquo;:2, \u0026ldquo;heartbeatInterval\u0026rdquo;:\u0026ldquo;5s\u0026rdquo;, \u0026ldquo;loadbalanceCheckInterval\u0026rdquo;:\u0026ldquo;180s\u0026rdquo;, \u0026ldquo;notificationMember\u0026rdquo;: [ { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} } ] }, : : }\n(2) Provider method\nGet the address list supplied by the address provider to perform cluster configuration.\nWhen composing a cluster using the provider method, configure the parameters in the cluster definition file.\nCluster definition file\nParameters\nData type\nMeaning\n/cluster/notificationProvider/url\nstring\nSpecify the URL of the address provider when using the provider method as the cluster configuration method.\n/cluster/notificationProvider/updateInterval\nstring\nSpecify the interval to get the list from the address provider. Specify a value that is 1s or higher and less than 2^31s.\nA configuration example of a cluster definition file is shown below.\n{ : : \u0026ldquo;cluster\u0026rdquo;:{ \u0026ldquo;clusterName\u0026rdquo;:\u0026ldquo;yourClusterName\u0026rdquo;, \u0026ldquo;replicationNum\u0026rdquo;:2, \u0026ldquo;heartbeatInterval\u0026rdquo;:\u0026ldquo;5s\u0026rdquo;, \u0026ldquo;loadbalanceCheckInterval\u0026rdquo;:\u0026ldquo;180s\u0026rdquo;, \u0026ldquo;notificationProvider\u0026rdquo;:{ \u0026ldquo;url\u0026rdquo;:\u0026ldquo;http://example.com/notification/provider\u0026quot;, \u0026ldquo;updateInterval\u0026rdquo;:\u0026ldquo;30s\u0026rdquo; } }, : : }\nThe address provider can be configured as a Web service or as a static content. The specifications below need to be satisfied.\n Compatible with the GET method. When accessing the URL, the node address list of the cluster containing the cluster definition file in which the URL is written is returned as a response.  Response body: Same JSON as the contents of the node list specified in the fixed list method Response header: Including Content-Type:application/json   An example of a response sent from the address provider is as follows.\n$ curl http://example.com/notification/provider [ { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.44\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.45\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} }, { \u0026ldquo;cluster\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10010}, \u0026ldquo;sync\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10020}, \u0026ldquo;system\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10040}, \u0026ldquo;transaction\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:10001}, \u0026ldquo;sql\u0026rdquo;: {\u0026ldquo;address\u0026rdquo;:\u0026ldquo;172.17.0.46\u0026rdquo;, \u0026ldquo;port\u0026rdquo;:20001} } ]\n[Memo]\n Specify the serviceAddress and servicePort of the node definition file in each module (cluster,sync etc.) for each address and port. SQL items are required in the GridDB Advanced Edition only. Set either the /cluster/notificationAddress, /cluster/notificationMember, /cluster/notificationProvider in the cluster definition file to match the cluster configuration method used. See “GridDB technical reference” (GridDB_TechnicalReference.html) for details on the cluster configuration method.  Setting the tuning parameters The main tuning parameters are described here. These parameters are not mandatory but affect the processing performance of the cluster.\nParameter related to update performance GridDB creates a transaction log file and checkpoint file for persistency purposes. As data written in these files has an impact on the update performance, the method of creation can be changed by the following parameters. However, the disadvantage is that the possibility of data being lost during a failure is higher.\nThe related parameters are shown below.\nNode definition file\nParameters\nData type\nMeaning\n/dataStore/persistencyMode\nstring\nPersistency mode\n/dataStore/logWriteMode\nint\nLog write mode\nThe persistency mode specifies whether to write data to a file during a data update. The log write mode specifies the timing to write data to the transaction log file.\nEither one of the values below can be set in the persistency mode.\n \u0026ldquo;NORMAL\u0026rdquo; \u0026ldquo;KEEP_ALL_LOGS\u0026rdquo;  A \u0026ldquo;NORMAL\u0026rdquo; writes data to the transaction log file and checkpoint file every time there is an update. A transaction log file which is no longer required will be deleted by a checkpoint. The write timing of \u0026ldquo;KEEP_ALL_LOGS\u0026rdquo; is the same as \u0026ldquo;NORMAL\u0026rdquo; but all transaction log files are retained. Default value is \u0026ldquo;NORMAL\u0026rdquo;.\n[Points to note]\n When performing a differential backup, set the persistency mode to \u0026ldquo;NORMAL\u0026rdquo;.  Either one of the values below can be set in the log write mode.\n 0: SYNC An integer value of 1 or higher1: DELAYED_SYNC  In the \u0026ldquo;SYNC\u0026rdquo; mode, log writing is carried out synchronously every time an update transaction is committed or aborted. In the \u0026ldquo;DELAYED_SYNC\u0026rdquo; mode, log writing during an update is carried out at a specified delay of several seconds regardless of the update timing. Default value is \u0026ldquo;1 (DELAYED_SYNC 1 sec)\u0026ldquo;.\nParameter related to performance and availability By composing a cluster, GridDB can replicate data in multiple nodes to improve the search performance and availability. As replication of these data has an impact on the update performance, the method of creation can be changed by the following parameters. However, the disadvantage is that the possibility of data being lost during a failure is higher.\nThe related parameters are shown below.\nCluster definition file\nParameters\nData type\nMeaning\n/transaction/replicationMode\nint\nReplication mode\nThe replication mode specifies how to create a replica. The replication method has to be same for all nodes in the cluster .\n \u0026ldquo;0\u0026rdquo;: Asynchronous replication \u0026ldquo;1\u0026rdquo;: Semi-synchronous replication  \u0026ldquo;Asynchronous replication\u0026rdquo; performs replication without synchronizing the timing of the update process. \u0026ldquo;Semi-synchronous replication\u0026rdquo; performs replication synchronously at the timing of the update process timing but makes no appointment at the end of the replication. Default is \u0026ldquo;0\u0026rdquo;.\nParameter related to access performance immediately after startup Data perpetuated on a disk etc. can be loaded into the memory at the same time when a node is started (warm start process).\nThe warm start process can be enabled/disabled by the parameter below.\nNode definition file\nParameters\nData type\nMeaning\n/dataStore/storeWarmStart\nboolean\nStart processing mode\n false: non-warm start mode true: warm start mode  Default is true (valid).\nOther parameters An explanation of the other parameters is given. See Annex Parameters list for the default values.\nNode definition file\nParameters\nData type\nMeaning\n/dataStore/dbPath\nstring\nDatabase file directory\n/dataStore/backupPath\nstring\nBackup file directory\n/dataStore/storeMemoryLimit\nstring\nMemory buffer size\n/dataStore/concurrency\nint\nProcessing parallelism\n/dataStore/affinityGroupSize\nint\nNo. of data affinity groups\n/checkpoint/checkpointInterval\nint\nCheckpoint execution interval (unit: sec)\n/system/eventLogPath\nstring\nOutput directory of event log file\n/transaction/connectionLimit\nint\nNo. of connections upper limit value\n/trace/category\nstring\nEvent log output level\n A database file directory is created when the data registered in the in-memory is perpetuated. In this directory, the transaction log file and checkpoint files are created. A backup file directory is created when a backup is executed (further details will be explained in the subsequent chapters). In this directory, the backup file is stored. The memory buffer size is used for data management. To set the memory size, simply use numbers and text to specify the memory size and its unit respectively. E.g. “2048MB”. Processing parallelism refers to the upper limit value for GridDB to perform I/O processing of the secondary memory device in parallel. With regards to data affinity, related data are collected, and the number of groups is specified during layout management. Any number from 1 to 64 can be selected when specifying the number of groups. Please note though that the larger the group, the lower the memory utilization efficiency becomes. The checkpoint execution interval is the interval to execute an internal checkpoint process regularly (related to the perpetuation of data). The output directory of event log is the directory where messages (event message files) related to event data occurring inside a node such as exceptions etc, will be saved into. As a rule of thumb, please set the no. of connection upper limit to at least twice the number of expected clients. The event log output level is the output level for each category of the event log.  "
},
{
	"uri": "http://example.org/technical-architecture-and-guide/3-9_sharding/",
	"title": "Sharding",
	"tags": [],
	"description": "",
	"content": " System expansion can be carried out online with a scale-out approach. As a result, a system in operation can be supported without having to stop it as it will support the increasing volume of data as the system grows.\nIn the scale-out approach, data is arranged in an appropriate manner according to the load of the system in the nodes built into the system. As GridDB will optimize the load balance, the application administrator does not need to worry about the data arrangement. Operation is also easy because a structure to automate such operations has been built into the system. Sharding is the process of distributing the data amongst many different nodes. With sharding, the a chunk of data may be split amongst 4 different nodes, with each node containing different parts of the whole.\n3.9.1 Concept of GridDB Cluster For scalability, most databases are classified as either of two types of organization:\nMaster-slave type: A cluster which consists of a master node \u0026ndash; which manages the cluster \u0026ndash; and multiple database nodes \u0026ndash; which store data. Since the master node is a single point of failure (SPOF), it is necessary to replicate and make redundant the master node in order to ensure the availability of the cluster. Moreover, when the number of database (DB) nodes increase, there is a possibility that the load of the master node can overload, causing system-wide slow down.\nPeer-to-peer type: All nodes which constitute the DB cluster are homogeneous and can perform the same functions. Since each node operates according to fragmentary node information \u0026ndash; when compared with a master-slave type \u0026ndash; data reconstruction is hard to optimize. The main issue stemming from this practice is the large overheard between nodes.\nGridDB is a hybrid system of master-slave type and a peer-to-peer type. All nodes which constitute the DB cluster are homogeneous with the same functions.\nWhen the DB cluster is constructed, the master node is autonomously determined. The other nodes in the cluster are called \u0026lsquo;follower nodes\u0026rsquo;. If the master node ever gets knocked out or fails, a new master node is determined from the surviving nodes. In order to avoid the split brain problem by network partitioning, the number of nodes which constitutes the DB cluster must be more than a quorum.\nA partition is a logical area which stores containers and it is not directly visible to the user. Although multiple partitions are possible, the number of partitions must equal the amount of nodes within the cluster. All containers belong to either of the partition sets using the hash value to a primary key. One owner node plus zero or more backup nodes exist in each partition. There is also a node called the \u0026lsquo;catch-up node\u0026rsquo;, which will be promoted to a backup node in the future if needed (this node is essentially the backup to the backup).\nThe owner node is the node which can update the containers. A backup node is a node which holds the replica of the container and enables only reference opertation to te container. An allocation table of the node to each partition is called a partition table. A master node distributes this partition table to the follower nodes or client libraries. By referring to this partition table, the owner node and backup node belonging to a certain container can be made known.\nAs mentioned above, the role for any given GridDB node is two-tiered.\n master node / follower node owner node / backup node / catch-up node  3.9.2 Determination of a master node  Determination procedure of a master node\nIn GridDB, a master node is autonomously determined using a bully algorithm. An election message is transmitted and received between nodes. When a certain node receives the message from a node stronger than a self-node, it returns the response \u0026ndash; a follow message that follows the opponent, turning that node into a \u0026lsquo;follower node\u0026rsquo;. Then, the node ends these types of message transmission and reception.\nFinally by repeating this procedure, only one node remains undefeated, and the node is determined as the master node. In this algorithm, the number of times this is repeated until a master node is determined is logN, where N is the number of nodes.\nThe master node collects information on all follower nodes by the \u0026lsquo;heartbeat\u0026rsquo; of a constant cycle. The communication cost of this \u0026lsquo;heartbeat\u0026rsquo; is dependent on the number of follower nodes. Although a heartbeat interval is 5 seconds by default, according to the number of nodes or network traffic, you may adjust accordingly.\n When a master node goes down\nWhen the master node of the cluster goes down, a new master node is autonomously determined out of the follower nodes which remained. A cluster configuration is reset when the heartbeat to a master node severs follower nodes. Thereafter, a new master node is determined by the procedure described above.\n  3.9.3 Determination of partition role  Owner node and backup node\nThe number of backup nodes on a partition can be specified with a parameter called the number of replicas. When backup nodes exist in a partition, replication of updating data in an owner node is carried out immediately to all backup nodes.\nThe method of replication can choose either asynchronous mode or semi-synchronous mode. In asynchronous mode, when the contents of updating in an ownder node are transmitted to all backup nodes, update process is completed. In semi-synchronous mode, the contents of updating in an owner node are transmitted to all backup nodes, and after checking the reception response from all backuo nodes, update process is completed. Asynchronus mode is superior in availability.\nSince replication is carried out to all backup nodes, the processing cost is proportional to the number of backup nodes. If a performance of update process without a backup node is set to 100, a performance of update process with a backup node in asynchronous mode is about 70, and a performance of update process with a backup node in semi-synchronous mode is about 50.\nFailover is performed when a node failure is detected by a heartbeat. When a failure node is an owner node of a partition, a master node determines a new owner node from current backup nodes for the parition table. When a failure node is a backup node, the master node separates the backup node from the parititon table, and continues processing.\nThe master node newly updates a partition table and distributes it to follower nodes. According to this partition table, synchronus data is transmitted and received between a new owneer node and a new backup node, and it checks that the time stamp of both updating log is in agreement. This processing is called a short-term synchronization.\nSince the updating data in the owner node is reflected one by one by replication to the backup node, the data size which should synchonize in a short-term synchonization is slight, and short-term synchonization usually completes in several seconds.\n Determination of catch-up node\nA catch-up node is a node to be promoted to a backup node in the future, and it is set up when as follows.\n The number of backup nodes is insufficient to the number of replicas. The owner node and backup node of a partition have deviation between nodes.   A master node determines a catch-up node to resolve the above case. Then, a data image and an updating log are transmitted to the catch-up node from the owner node. By this, the time stamp of the data which the catch-up node has is brought close to the time stamp which the owner node has.\nA master node performs a short-term synchronization between the owner node and the catch-up node, when the difference of the time stamp of the owner node and the catch-up node becomes within a steady value.\nIt takes several hours during the period after starting transmission of data from the owner node to the catch-up node until it is set to the backup node.\n  "
},
{
	"uri": "http://example.org/data-modeling/4-3_modeling-relationships/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " 4 Data Modeling \u0026gt;\n4.3 Modeling Relationships What are Modeling Relationships? Though GridDB does not employ strict schemas overall, it is important to have some semblance of foresight when dealing with the cluster\u0026rsquo;s data. Utilizing data to its potential is the end-goal when choosing a database, and so when setting up the data for the first time, it is important to make certain decisions.\nRelationships between data and how they are organized are very important for both analytics and performance. When setting up the cluster, how the relationships between data is organized can make a difference on both fronts.\n"
},
{
	"uri": "http://example.org/data-modeling/4-4_possibilities-and-variations-of-data-models-in-griddb/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " 4 Data Modeling \u0026gt;\n4.4 Possibilities and Variations of Data Models in GridDB T.B.D "
},
{
	"uri": "http://example.org/drivers-and-integrators/7-4_grafana-connector/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n7.4 Grafana Connector* TBD\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-10_collection-delete/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.10 Data Deletion Overview This chapter covers deleting data from a GridDB collection.\nDelete data Delete data from a collection with specifying a Row key.\nList.1 Delete Data (CollectionDeleteRow.java)\n// Get Collection Collection weatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\n// Delete Row boolean deleteSucceed = weatherStationCol.remove(\u0026ldquo;1\u0026rdquo;); System.out.println(\u0026ldquo;Delete Succeed:\u0026rdquo; + deleteSucceed);\nSystem.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); for (int i = 0; i \u0026lt; WeatherStationLogic.JP_PREFECTURE; i++) { // Retrieve row by key WeatherStation weatherStation = weatherStationCol.get(String.valueOf(i + 1)); if (weatherStation != null) { System.out.println(String.format(\u0026ldquo;%-3s\\t%-20s\\t%-10s\\t%-10s\\t%-5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); } else { System.out.println(String.format(\u0026ldquo;ID:%s is not exist\u0026rdquo;, (i + 1))); } }\n L.28: Delete a Row with specifying the measuring instrument ID using Container.remove(String) method. If the deletion was sucesseful, the return value is True. L.32-42: Display the retrieved data after the deletion.   Execution results are as follows:\nList.2 Result\nDelete Succeed:true ID Name Longitude Latitude Camera ID: 1 does not exist 2 Aomori-Aomori 40.82444 140.74 false 3 Iwate-Morioka 39.70361 141.1525 true 4 Miyagi-Sendai 38.26889 140.87194 false 5 Akita-Akita 39.71861 140.1025 true (Snip)\nComplete Source Code Complete source code used in this sample can be downloaded from the following.\nDownload: griddb-delete.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-11_collection-modify/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.11 Collection Modification Overview This section describes modifying the schema and the index of a GridDB collection.\nSchema Modification You can modify the schema by adding or deleting columns in a container after the container has been created.\nThis example demonstrates how to modify a newly created class with a new column in addition to WeatherStation class.\nIn addition, there is a way to modify the schema without creating a new class. Please refer to Meta-information for more details.\nList.1 Modified Class (AnotherWeatherStation.java)\npackage sample.row;\nimport com.toshiba.mwcloud.gs.RowKey;\npublic class AnotherWeatherStation { / ** * ID of WeatherStation * / @RowKey public String id;\n/ \\*\\* \\* Name of WeatherStation \\* / public String name; / \\*\\* \\* Installation Latitude \\* / public double latitude; / \\*\\* \\* Installation Longitude \\* / public double longitude; / \\*\\* \\* Camera exists or not \\* / public boolean hasCamera; / \\*\\* \\* Added field to WeatherStation \\* / public String description;  }\n L.35: The newly added column.  In the case creating a new collection, the GridStore.putCollection(String, Class) method is used, and the case modifying schema of column, the method with parameters of modification options GridStore.putCollection(String, Class, boolean) is used.\nList.2 putCollection Method\n\u0026lt; K, R \u0026gt; Collection \u0026lt; K, R \u0026gt; putCollection (java.lang.String name, java.lang.Class \u0026lt; R \u0026gt; rowType, boolean modifiable) throws GSException\nYou can modify a schema of an existing collection with setting the modifiable parameter to true.\nHowever there are several conditions in order to modify. Please refer to GridDB API Reference for more information.\nList.3 Modifying the collection to be added a new column (CollectionModify.java)\n// Modify ano ther schema Collection Collection ano therWeatherStationCol = store.putCollection(\u0026ldquo;weather_station\u0026rdquo;, AnotherWeatherStation.class, true );\n L.29-30: Set the modifiable paramater to true.   Add an Index You can set an Index to column in GridDB identically with general RDB.\nList.4 Adding an Index (CollectionIndex.java)\n// Create Index weatherStationCol.createIndex(\u0026ldquo;id\u0026rdquo;); weatherStationCol.createIndex(\u0026ldquo;name\u0026rdquo;, IndexType.TREE);\n L.34: Add an index with specifying a column name id of the measuring instrument ID with Container.createIndex(String) method. L.35: Add ab index specifying a column name of the measuring instrument name with Container.createIndex(String, IndexType) method and set IndexType.TREE to the index type.  Types of indexes are as follows:\nTable.1 Index types\nIndex type\nDescription\nHASH\nHigh speed retrieval, but not suitable for the operation of reading Row sequentially and impossible to set in the TimeSeries container.\nTREE\nIt is suitable for a retrival specifying that whether the value is larger or smaller than a retrieval vulue to a retrieval range.\nThere are index types which can not be specified owing to the container type or the column type. Please refer to GridDB API Reference and GridDB Technical Reference for more information.\nDelete an Index Index set in the column can be deleted.\nList.5 Delete an Index (CollectionIndex.java)\n// Drop Index weatherStationCol.dropIndex(\u0026ldquo;id\u0026rdquo;); weatherStationCol.dropIndex(\u0026ldquo;name\u0026rdquo;);\n L.38-39: Delete an Index with the Container.dropIndex (String) method specifying column name set to the index.   Complete source code Complete source code used in this sample can be downloaded from the following.\nDownload: collection_modify.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-12_timeseries-register/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.12 Data Registration Overview This section describes the data registration for TimeSeries containers\nSample Data Similar to collection sample, data to be registered is initially stored in a CSV file. Data must be registered in order to later read. The contents of the CSV file (Instrument_log.csv) is as follows:\n Data held Measuring instrument ID Timestamp Temperature Live image file path   List.1 Measurement Log Contents (instrument_log.csv)\nweather_station_1,2016/7/1 0: 00,50, liveimage1.jpg weather_station_2,2016/7/1 0: 00,50, weather_station_3,2016/7/1 0: 00,50, liveimage2.jpg weather_station_4,2016/7/1 0: 00,50, weather_station_5,2016/7/1 0: 00,50, liveimage1.jpg (Snip)\nData Registration Process The following snippet demonstrates data registration.\nList.2 Register TimeSeries Container (TimeSeriesRegister.java)\n// Read InstrumentLog data from csv List logList = logLogic.readCsv(); // Register TimeSeries Container logLogic.registerTimeSeriesContainer(store, logList);\n L.32: Process the CSV into a list of Instrument Logs. L.34: Register the parsed data in GridDB.   List.3 Read Measurement Log (InstrumentLogLogic.java)\npublic List readCsv() throws IOException, ParseException, SerialException, SQLException { // Read CSV file List logList = new ArrayList(); SimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/dd HH: mm\u0026rdquo;, Locale.US); String[] line = null; CSVReader reader = new CSVReader(new FileReader(\u0026ldquo;data/instrument_log.csv\u0026rdquo;));\ntry { while ((line = reader.readNext()) != null) { InstrumentLog log = new InstrumentLog(); log.weatherStationId = line\\[0\\]; log.timestamp = format.parse(line\\[1\\]); log.temperture = Float.valueOf(line\\[2\\]); // Write Blob data ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); String filePath = line\\[3\\]; if ((filePath != null) \u0026amp;\u0026amp; (!filePath.isEmpty())) { File imageFile = new File(filePath); if (imageFile.exists() \u0026amp;\u0026amp; imageFile.isFile()) { InputStream inputStream = null; try { inputStream = new BufferedInputStream(new FileInputStream(imageFile)); byte\\[\\] buff = new byte\\[1024\\]; while ((inputStream.read(buff)) != -1) { outputStream.write(buff); } } finally { if (inputStream != null) { inputStream.close (); } } } } log.liveImage = new SerialBlob(outputStream.toByteArray()); logList.add(log); } } finally { reader.close(); } return logList;  }\nRead CSV  L.67: Loading a CSV file instrument_log.csv with the data of the measurement log. L.70-74: read the contents of the CSV, it is set to an instance of InstrumentLog. L.77-95: read the file of the CSV of the live image file path, is writing the data to the Blob data.   List.4 TimeSeries Container Registration Process (InstrumentLogLogic.java)\npublic void registerTimeSeriesContainer(GridStore store, List logList) throws GSException { for (InstrumentLog log : logList) { // Retrieve TimeSeries Container TimeSeries logTs = store.putTimeSeries(log.weatherStationId, InstrumentLog.class);\n // Disable Auto Commit logTs.setAutoCommit(false); // Specify Time logTs.put(log.timestamp, log); // Specify Current Time // Comment out because the result would change on every execution // ts.append(log); // Instrument log timestamp is every 6 hours. // When the interpolation of 3 hours prior time to the log, // the temperature of the intermediate is registered Date medianTime = TimestampUtils.add(log.timestamp, -3, TimeUnit.HOUR); // Creating a TimeSeries Container when interpolating the values of temperature InstrumentLog medianLog = logTs.interpolate(medianTime, \u0026quot;temperture\u0026quot;); // If there is no log, a NULL is returned before the interpolated time if (medianLog != null) { logTs.put(medianTime, medianLog); } // Commit logTs.commit(); }  }\nTo register the data to the time series container in one of the following ways.\nSpecified time  L.128: TimeSeries.put (Date, Class) method uses the time specified as the key when the data is registered.  Current time  L.132: TimeSeries.append (Class) method uses the current time as the key. However, it has been commented out so that execution result is always constant.  Interpolated time  L.137: TimestampUtils.add (Date, int, TimeUnit) specifies a time 3 hours prior to the time of the time of measurement log. L.139: TimeSeries.interpolate (Date, String) generates a linear interpolation of the value corresponding to the specified time. This means, for example, when the temperature is 70°F at 6 and 80°F at 12, the interpolated value would be 75°F if 9 was specified. To obtain a linear interpolation value must have adjacent keys or the same timestamp must have been previously registered. L.142: TimeSeries.put (Date, Class) and is registered with the data that is linear interpolation with method.   The following topics are described in more detail in the Collection Registration chapter:\n Auto-commit Commit Roll Back   Complete source code Complete source code used in this sample can be downloaded from the following.\nDownload: timeseries-register.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-13_timeseries-retrieve/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.13 Data Retrieval Overview This section describes the data acquisition of time-series container of GridDB in this chapter.\nData Acquisition The following are methods to fetch and read TimeSeries containers:\nSpecified time Time to get the data (= low) of the series container, you will need to specify the time is Rouki of time series container.\nList.1 Acquire Data for a Specified Time (TimeSeriesRetrieve.java)\n// Specify Time InstrumentLog log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;)); System.out.println(\u0026ldquo;get by Time\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\n Line 100: Use TimeSeries.get (Date) to get rows with a specified time stamp.  Execution results are as follows.\nList.2 Results\nget by Time Sat Jul 02 12:00:00 EDT 2016 weather_station_1 80.0\nSpecified range Time series container will be able to get to specify the range to be retrieved.\nList.3 Aquire rows within a Range (TimeSeriesRetrieve.java)\n// Specify Time Range System.out.println(\u0026ldquo;get by Time Range\u0026rdquo;); Date start = format.parse(\u0026ldquo;2016/07/02 9:00\u0026rdquo;); Date end = TimestampUtils.add(start, 6, TimeUnit.HOUR);\nQuery query = logTs.query(start, end); // fetch row RowSet rowSet = query.fetch(); while (rowSet.hasNext()) { InstrumentLog log = rowSet.next(); System.out.println(\u0026ldquo;Timestamp\\t\\t\\tWeatherStation ID\\tTemperture\\tLive Image\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\\t%s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture)); }\n Line 119-120: Generate the start and end time for the range. Line 122: Use TimeSeries.query (Date, Date) method with start and end times for the range of data to fetch. Line 124-126: Fetch rows.  Execution results are as follows.\nList.4 Results\nget by Time Range Sat Jul 02 09:00:00 EDT 2016 weather_station_1 75.0 Sat Jul 02 12:00:00 EDT 2016 weather_station_1 80.0 Sat Jul 02 15:00:00 EDT 2016 weather_station_1 75.0\nRelative Time You can fetch rows with timestamps are earlier or later than specified timestamp with the TimeOperator enum.\nTable 1 TimeOperator Enum\nAcquisition method\nDescription\nTimeOperator.NEXT\nReturns the oldest among the Rows whose timestamp are identical with or later than the specified time.\nTimeOperator.NEXT_ONLY\nReturns the oldest among the Rows whose timestamp are later than the specified time.\nTimeOperator.PREVIOUS\nReturns the newest among the Rows whose timestamp are identical with or earlier than the specified time.\nTimeOperator.PREVIOUS_ONLY\nReturns the newest among the Rows whose timestamp are earlier than the specified time.\nFetching Rows with a Relative Time stamp (TimeSeriesRetrieve.java)\n// Specify the next time, including a specified time InstrumentLog log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;), TimeOperator.NEXT); System.out.println(\u0026ldquo;get by Next Time\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\n// Specify the next time log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;), TimeOperator.NEXT_ONLY); System.out.println(\u0026ldquo;get by NextOnly Time\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\n// Specify the previous time, including a specified time log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;), TimeOperator.PREVIOUS); System.out.println(\u0026ldquo;get by Previous Time\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\n// Specify the previous time log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;), TimeOperator.PREVIOUS_ONLY); System.out.println(\u0026ldquo;get by PreviousOnly Time\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture));\n Line 144: you get to specify the TimeOperator.NEXT. Line 150: you get to specify the TimeOperator.NEXT_ONLY. Line 156: you get to specify the TimeOperator.PREVIOUS. Line 162: you get to specify the TimeOperator.PREVIOUS_ONLY.  Serving as a reference time has designated all \u0026quot; 2016/07/02 12:00 \u0026quot;.\nExecution results are as follows.\nList.6 Results that the constant time was on the basis of\nget by Next Time Sat Jul 02 12:00:00 EDT 2016 weather_station_1 80.0 get by NextOnly Time Sat Jul 02 15:00:00 EDT 2016 weather_station_1 75.0 get by Previous Time Sat Jul 02 12:00:00 EDT 2016 weather_station_1 80.0 get by PreviousOnly Time Sat Jul 02 09:00:00 EDT 2016 weather_station_1 75.0\nAggregatation Time series container is able to aggregate the data for the specified time period.\nList.7 Aggregate Average\n// Average Temperature Date start = format.parse(\u0026ldquo;2016/07/01 12:00\u0026rdquo;); Date end = format.parse(\u0026ldquo;2016/07/02 9:00\u0026rdquo;); AggregationResult aggrResult = logTs.aggregate(start, end, \u0026ldquo;temperture\u0026rdquo;, Aggregation.AVERAGE); System.out.println(\u0026ldquo;Average Temperature:\u0026rdquo; + aggrResult.getDouble() + \u0026ldquo;\\n\u0026rdquo;);\n 181-182 line: TimeSeries.aggregate (Date, Date, String, Aggregation) method You have a summary to find the average value of the temperature.  Execution results are as follows.\nList.8 Aggregate Results\nAverage Temperature: 67.5\nThe different types of Aggregation methods are as follows:\nTable 2 Aggregation types\nAggregation method\nDescription\nAggregation.AVERAGE\nObtain the average value.\nAggregation.COUNT\nObtain the number of samples.\nAggregation.MAXIMUM\nObtain the maximum value.\nAggregation.MINIMUM\nObtain the minimum value.\nAggregation.STANDARD_DEVIATION\nObtain the standard deviation.\nAggregation.TOTAL\nObtain the total value (sum).\nAggregation.VARIANCE\nObtain the variance within the rows.\nAggregation.WEIGHTED_AVERAGE\nObtain the weighted average.\nFor more information, please refer to the GridDB API Reference\nComplete source code Complete source code used in this sample can be downloaded from the following.\nDownload: timeseries-retrieve.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-14_timeseries-delete/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.14 TimeSeries Data Deletion Overview This section describes deleting data in a TimeSeries container.\nData deletion To delete the data (row) in a TimeSeries container, specify the timestamp of the rows you want to delete.\nList.1 Delete TimeSeries Data (TimeSeriesDeleteRow.java)\nSimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/dd HH:mm\u0026rdquo;, Locale.US);\nString containerName = \u0026ldquo;weather_station_1\u0026rdquo;; // Get TimeSeries Container TimeSeries logTs = store.getTimeSeries(containerName, InstrumentLog.class); Date deleteTime = format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;);\nSystem.out.println(containerName + \u0026ldquo; ################\u0026rdquo;); System.out.println(\u0026ldquo;Timestamp\\t\\t\\tWeatherStation ID\\tTemperture\u0026rdquo;);\n// Delete the Instrument log of the specified time boolean deleteSuccessed = logTs.remove(deleteTime); System.out.println(\u0026ldquo;Delete Result:\u0026rdquo; + deleteSuccessed);\nInstrumentLog log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;)); if (log == null) { System.out.println(\u0026ldquo;Deleted log at 2016/07/02 12:00\u0026rdquo;); }\n L.41: Specify the timestamps of the data to remove using the TimeSeries.remove(Date) method. Here the time stamp is 2016/07/02 12:00. If the deletion is successful, it will return is true.   Execution results are as follows.\nList.2 Result\nDelete Result: true Deleted log at 2016/07/02 12:00\nYou can check that the specified data has been deleted.\nComplete source code Complete source code used in this sample can be downloaded from the following.\nDownload: timeseries-delete.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-15_timeseries-modify/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.15 TimeSeries Container Modification Overview This chapter describes the changing the schema of a TimeSeries container. Changing the index is same as changing the index as with a collection, please refer the Collection Modification chapter for details on that process.\nSchema Modification Like you can with Collections, you can add or remove columns after creating a TimeSeriescontainer. Create a new class with the modified columns. The following AnotherInstrumentLog class shows an example of sch a change.\nList.1 Modified class (AnotherInstrumentLog.java)\npackage sample.row;\nimport java.sql.Blob; import java.util.Date;\nimport com.toshiba.mwcloud.gs.RowKey;\npublic class AnotherInstrumentLog { / ** * Timestamp of log * / @RowKey public Date timestamp;\n/ ** * ID of WeatherStation * / public String weatherStationId;\n/ ** * Temperature of the measurement result * / public float temperture;\n/ ** * Image data obtained by photographing the sky * / public Blob liveImage;\n/ ** * Added field to InstrumentLog * / public String description; }\n L.33: The new field added to the InstrumentLog class.  While normally GridStore.putTimeSeries (String, Class) would be used but with the modified schema, we need to change the pparameters and use GridStore.putTimeSeries (String, Class, TimeSeriesProperties, boolean) method.\nList.2 putTimeSeries Parameters\n≪ R \u0026gt; TimeSeries \u0026lt; R \u0026gt; putTimeSeries (java.lang.String name, java.lang.Class \u0026lt; R \u0026gt; rowType, TimeSeriesProperties props, boolean modifiable) throws GSException\nThe descriptions of each parameter are the same as when used with collections, please refer to the modifying collections chapter for more details.\nList 3.Creating a TimeSeries Container with the new Column (TimeSeriesModify.java)\n// Modify another schema TimeSeries Container TimeSeriesProperties timeProp=new TimeSeriesProperties (); TimeSeries \u0026lt; AnotherInstrumentLog \u0026gt; anotherTs=store.putTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, AnotherInstrumentLog.class, timeProp, true );\n L.37: In order to use the putTimeSeries (String, Class, TimeSeriesProperties, boolean) method with the new class, you have to create a new instance of TimeSeriesProperties. See the meta-information section for more information about TimeSeriesProperties. L.39: Specify true for the modifiable parameter.   Complete source code Complete source code used in this sample can be downloaded from the following.\nDownload: timeseries-modify.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-16_tql/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.16 TQL Overview This chapter covers the TQL query language for accessing GridDB. Only the SELECT statement is supported in TQL so no changes can be made to the database or schema. Please refer to the GridDB API reference for more information.\nConditional Search The TQL search functionality is the same as SQL.\nList.1 Conditional Search(TqlSeach.java)\n// Get TimeSeries Container TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, InstrumentLog.class);\n// Seach by temperture String tql = String.format(\u0026ldquo;SELECT * WHERE temperture \u0026gt; %s\u0026rdquo;, temperture); Query query = logTs.query(tql, InstrumentLog.class); RowSet rows = query.fetch();\n Line 103: Use Container.query class to input the TQL statement Line 104: Fetch results from TQL statement  The result of the TQL statement is stored in a RowSet and can be retrieved as follows:\nList.2 Get the results of the Conditional Search(TqlSeach.java)\nwhile (rows.hasNext()) { InstrumentLog log = rows.next(); System.out.println(\u0026ldquo;Timestamp\\t\\t\\tWeatherStation ID\\tTemperture\u0026rdquo;); System.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture)); }\n Line 38: Use RowSet.hasNext() to see if there any more results Line 39: Use RowSet.next() to retrieve the next row.  The execution results follow:\nList.3 Results of the Conditional Search(TqlSeach.java)\nTQL:SELECT * WHERE temperture \u0026gt; 70.0 Timestamp WeatherStation ID Temperture Fri Jul 01 09:00:00 JST 2016 weather_station_1 75.0 Fri Jul 01 12:00:00 JST 2016 weather_station_1 80.0 Fri Jul 01 15:00:00 JST 2016 weather_station_1 75.0 Sat Jul 02 09:00:00 JST 2016 weather_station_1 75.0 Sat Jul 02 12:00:00 JST 2016 weather_station_1 80.0 Sat Jul 02 15:00:00 JST 2016 weather_station_1 75.0\nLike Search You can use the LIKE keyword search like SQL.\nList.4 Like Search(TqlSeach.java)\n// Get Collection CollectionweatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\n// Like Search String tql = \u0026ldquo;SELECT * WHERE name LIKE \u0026lsquo;%\u0026rdquo; + name + \u0026ldquo;%\u0026rsquo;\u0026rdquo;; // Search By TQL Query query = weatherStationCol.query(tql, WeatherStation.class); RowSet rows = query.fetch();\n Line 120: Specifies LIKE in the query string using patterns. %: Matches a string of one or more characters. _: Matches a single character.  Retrieving the results from the RowSet does not change.\nList.5 Get the results from the Like Query(TqlSeach.java)\n// Search WeatherStation by name RowSet wsRows = searchByName(store, \u0026ldquo;kyo\u0026rdquo;);\n// Show search WeatherStation results System.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); while (wsRows.hasNext()) { WeatherStation weatherStation = wsRows.next(); System.out.println(String.format(\u0026ldquo;%-3s\\t%-20s\\t%-10s\\t%-10s\\t%-5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); (snip) }\nExecution results are as follows:\nList.6 Result(TqlSeach.java)\nTQL:SELECT * WHERE name LIKE \u0026lsquo;%kyo%\u0026rsquo; ID Name Longitude Latitude Camera 13 Tokyo-Shinjuku 35.68944 139.69167 true\nTime specified Search In the case of a TimeSeries container, TimeSeries.query() can be used with TQL.\nList.7 TQL and TimeSeries(TqlSeach.java)\n// Get TimeSeries Container TimeSeries logTs; logTs = store.getTimeSeries(\u0026ldquo;weather_station_\u0026rdquo; + weatherStationId, InstrumentLog.class);\nQuery query = logTs.query(String.format( \u0026ldquo;SELECT * WHERE TIMESTAMP(\u0026lsquo;%s\u0026rsquo;) \u0026gt;= timestamp AND timestamp \u0026lt;= TIMESTAMP(\u0026lsquo;%s\u0026rsquo;)\u0026ldquo;, TimestampUtils.format(start), TimestampUtils.format(end))); RowSet rows = query.fetch();\n Line 144-146: Use TimestampUtils.date to specify the time with in the TQL search string.   List.8 Get results from a TimeSeries TQL Query(TqlSeach.java)\n// Search WeatherStation by name RowSet wsRows = searchByName(store, \u0026ldquo;kyo\u0026rdquo;);\n// Show search WeatherStation results System.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); while (wsRows.hasNext()) { WeatherStation weatherStation = wsRows.next(); System.out.println(String.format(\u0026ldquo;%-3s\\t%-20s\\t%-10s\\t%-10s\\t%-5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); (snip) }\nList.9 Results (TqlSeach.java)\nTQL:SELECT * WHERE TIMESTAMP(\u0026lsquo;2016-07-01T21:00:00.000Z\u0026rsquo;) \u0026lt;= timestamp AND timestamp \u0026lt;= TIMESTAMP(\u0026lsquo;2016-07-02T03:00:00.000Z\u0026rsquo;) Timestamp WeatherStation ID Temperture Sat Jul 02 06:00:00 JST 2016 weather_station_13 70.0 Sat Jul 02 09:00:00 JST 2016 weather_station_13 75.0 Sat Jul 02 12:00:00 JST 2016 weather_station_13 80.0\nSearch with forUpdate It is possible to update rows retrieved using TQL.\nList.10 Search with forUpdate(TqlForUpdate.java)\n// Get TimeSeries CollectionweatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\n// When using forUpdate, there is a need to disable the Auto Commit weatherStationCol.setAutoCommit(false);\n// In the case of boolean columns, NOT means false. String tql = \u0026ldquo;SELECT * WHERE NOT hasCamera\u0026rdquo;; // Search By TQL Query query = weatherStationCol.query(tql, WeatherStation.class); // Using forUpdate option. RowSet rows = query.fetch(true);\n// Install a camera in all of WeatherStation. while (rows.hasNext()) { System.out.println(\u0026ldquo;not exists camera\u0026rdquo;); WeatherStation weatherStation = rows.next(); System.out.println(\u0026ldquo;ID:\u0026rdquo; + weatherStation.id); weatherStation.hasCamera = true; // update row of WeatherStation rows.update(weatherStation); } // Commit weatherStationCol.commit();\nTQL EXPLAIN and ANALYZE Like SQL, it is possible to use the EXPLAIN and ANALYZE keywords to get execution plans for TQL statemntes.\nList.11 TQL Execution plans(TqlExplain.java)\n// Get InstrumentLog TimeSeries TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, InstrumentLog.class);\n// Analyzing the execution plan of the TQL String tql = \u0026ldquo;EXPLAIN ANALYZE SELECT * WHERE 50.0 \u0026lt;= temperture AND temperture \u0026lt;= 70.0\u0026rdquo; + \u0026ldquo; AND TIMESTAMP(\u0026lsquo;2016-07-01T06:00:00Z\u0026rsquo;) \u0026lt;= timestamp\u0026rdquo;; Query query = logTs.query(tql, QueryAnalysisEntry.class); RowSet rows = query.fetch();\n// output the execution plan of the TQL System.out.println(\u0026ldquo;ID\\tDepth\\tType\\t\\t\\t\\tValueType\\tValue\\t\\t\\tStatement\u0026rdquo;); while (rows.hasNext()) { QueryAnalysisEntry analysis = rows.next(); System.out.println(String.format(\u0026ldquo;%s\\t%s\\t%-24s\\t%-10s\\t%-20s\\t%s\u0026rdquo;, analysis.getId(), analysis.getDepth(), analysis.getType(), analysis.getValueType(), analysis.getValue(), analysis.getStatement())); }\n Line 31-32: Add EXPLAIN ANALYSZE to the start of the TQL statement. Line 33: Run the query with QueryAnalysisEntry.class as a parameter   List.12 Result(TqlExplain.java)\nID Depth Type ValueType Value Statement 0 1 TIMESERIES_EXPIRE TIMESTAMP 1970-01-01T00:00:00Z 1 0 QUERY_LOOP_NUMBER INTEGER 0 2 1 NOT_INDEX_USABLE COLUMN temperture 3 1 NOT_INDEX_USABLE COLUMN temperture 4 1 INDEX_FOUND INDEX_TYPE ROWKEY timestamp 5 1 SET_INDEX_KEY STRING START_KEY 2016-07-01T06:00:00.000Z 6 1 SEARCH_EXECUTE MAP_TYPE BTREE 7 2 SEARCH_MAP STRING TIME_SERIES_ROW_MAP 8 1 SEARCH_RESULT_ROWS INTEGER 6 9 0 QUERY_EXECUTE_RESULT_ROWS INTEGER 6 10 0 QUERY_RESULT_TYPE STRING RESULT_ROW_ID_SET 11 0 QUERY_RESULT_ROWS INTEGER 6\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload: tql.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-17_trigger-rest/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.17 Trigger (REST) Overview This chapter describes how to notify events by REST method in trigger function.\nTrigger function A trigger function is an automatic notification function when an operation (add/update or delete) is carried out on the row data of a container. Event notifications can be received without the need to poll and monitor the database by application.\nThere are 2 ways of notifying events to the application.\n REST Java Messaging Service (JMS)  Here we describe how to notify the events by REST method.\nSetting the Trigger Following information should be set to notify events by REST method.\nTable.1 Trigger settings (REST)\nItem\nDescription\nName\nTrigger name.\nTrigger type\nTriggerInfo.Type.REST\nNotification condition\nTriggerInfo.EventType.PUT or TriggerInfo.EventType.DELETE\nNotification destination URI\nIt should be described in the following format. (method name)://(host name):(port number)/(path)\nList.1 Setting the Trigger (TriggerRest.java)\n// Create Trigger Settings TriggerInfo trigger = new TriggerInfo(); trigger.setName(\u0026ldquo;InstrumentLogRESTTrigger\u0026rdquo;); trigger.setType(Type.REST); trigger.setTargetEvents(EnumSet.of(EventType.PUT)); trigger.setURI(URI.create(\u0026ldquo;http://127.0.0.1/api\u0026quot;));\n// Get TimeSeries Container TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, InstrumentLog.class);\nlogTs.createTrigger(trigger);\n// Update Data for call Trigger SimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/dd HH:mm\u0026rdquo;, Locale.US); InstrumentLog log = logTs.get(format.parse(\u0026ldquo;2016/07/02 12:00\u0026rdquo;)); log.temperture = 90; ogTs.put(log);\n L.33: Instantiate the TriggerInfo which represents the trigger setting. L.34−37: Set trigger properties in Table.1 L.43: Set TriggerInfo into Timeseries Container. Not only Timeseries Container, but also Collection could be used for trigger function. L.46-49: Update the Timeseries Container to enable trigger notification.   REST server startup You need to prepare the REST server to receive notifications. In this sample, we will show you how to run the simple Web server in Python which can be used to display the logs.\nList.2 REST Server Script (restserver.py)\nimport sys import json from BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\nclass JsonResponseHandler(BaseHTTPRequestHandler):\ndef do_POST(self): content_len = int(self.headers.get('content-length')) body = self.rfile.read(content_len).decode('UTF-8') jsonBody = json.loads(body) print(json.dumps(jsonBody, indent=4)) self.send_response(200) self.send_header('Content-type', 'text/json') self.end_headers()  server = HTTPServer((\u0026ldquo;, 80), JsonResponseHandler) server.serve_forever()\nThis REST server can be run by executing the command below.\nList.3 Run REST Server Script\n$ sudo python restserver.py\nPlease note that you should run the REST server on the machine which represents notification destination URI.\nResult of the Trigger execution You can see the result below on REST server when TriggerRest.java is executed.\nList.4 Result of Update the Row\n{ \u0026ldquo;container\u0026rdquo;: \u0026ldquo;weather_station_1\u0026rdquo;, \u0026ldquo;event\u0026rdquo;: \u0026ldquo;put\u0026rdquo; } gsnode2.griddb_default - - [13/Sep/2016 10:18:01] \u0026ldquo;POST /api HTTP/1.1\u0026rdquo; 200 -\nIn this case, Container weather_station_1 exists in gsnode2.griddb_default and you can see the the notification content.\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload: trigger-rest.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-18_trigger-jms/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.18 Trigger (JMS) Overview This section covers how to notify an application with JMS using Trigger functions. For an overview of the trigger function, please refer to the Trigger (rest) seciton.\nSetting the Trigger List.1 Setting the Trigger(TriggerJms.java)\n// Create Connection store = gridLogic.createGridStore();\n// Create Trigger Settings TriggerInfo trigger = new TriggerInfo(); trigger.setName(\u0026ldquo;InstrumentLogJMSTrigger\u0026rdquo;); trigger.setType(Type.JMS); trigger.setJMSDestinationType(\u0026ldquo;queue\u0026rdquo;); trigger.setJMSDestinationName(\u0026ldquo;jms/griddb\u0026rdquo;); trigger.setTargetEvents(EnumSet.of(EventType.PUT)); trigger.setUser(\u0026ldquo;admin\u0026rdquo;); trigger.setPassword(\u0026ldquo;admin\u0026rdquo;); trigger.setURI(URI.create(\u0026ldquo;http://127.0.0.1:7676/\u0026quot;));\n// Get TimeSeries Container TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, InstrumentLog.class);\nlogTs.createTrigger(trigger);\n Line 33: Create a new Trigger with the TriggerInfo() class Line 34-41: Set the parameters for the trigger as necessary Line 47: Create the trigger Line 50-53: The a notification will be triggered when a row is updated.   Installation of the JMS server Note: The installation procedure is complicated so please refer to the documentation of your chosen JMS server. The necessary configuration information specific to the sample environment is as follows:\nTable 1 JMS Server Configuration\nSetting Item\nSetting Value\nNotification URI\nhttp://127.0.0.1:7676/\nDestination type\nqueue\nDestination namejms/griddb\nThe following is the output of the JMS server when above code triggers an event\nList.4 Result\nThe execution result of the list second row update TriggerJms.java the contents of the standard output of the JMS server when you run is as follows. { \u0026ldquo;Container\u0026rdquo; : \u0026ldquo;Weather_station_1\u0026rdquo; , \u0026ldquo;Event\u0026rdquo; : \u0026ldquo;Put\u0026rdquo; } Gsnode2 . Griddb_default - - [ 13 / Sep / 2016 10 : 18 : 01 ] \u0026ldquo;POST / HTTP / 1.1\u0026rdquo; 200 -\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload: trigger-jms.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-19_multiput/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.19 Multi-Put Overview This section describes Multi-put, a method of registering batches of data at one time. It is described further in the GridDB Technical Reference.\nBatch Processing When data is sent or received in a one by one manner it is possible to reach the upper limit of network bandwidth and throughput will peak, the following GridDB API\u0026rsquo;s offer a solution to this problem with methods to work with large sets of data in an efficient manner:\n Multi-put Multi-get Multi-query   Create Row for registration List.1 Create Row of WeatherStation(MultiPut.java)\nprivate static Row createWeatherStationRow(GridStore store, String id, String name, double latitude, double longitude, boolean hasCamera) throws GSException { // Create WeatherStation Row ContainerInfo wsContainerInfo = store.getContainerInfo(\u0026ldquo;weather_station\u0026rdquo;); Row wsRow = store.createRow(wsContainerInfo);\n// Set by specifying the index of the order of definition of the WeatherStation class // ID wsRow.setString(0, id); // Name wsRow.setString(1, name); // Latitude wsRow.setDouble(2, latitude); // Longitude wsRow.setDouble(3, longitude); // hasCamera wsRow.setBool(4, hasCamera); return wsRow;  }\nList.2 Create Row of Intrument Log(MultiPut.java)\nprivate static Row createIntrumentLogRow(GridStore store, Date timestamp, String weatherStationId, float temperture, Blob liveImage) throws GSException, ParseException, SerialException, SQLException { // Create IntrumentLogRow Row ContainerInfo logContainerInfo = store.getContainerInfo(\u0026ldquo;weather_station_99\u0026rdquo;); Row logRow = store.createRow(logContainerInfo);\n// Set by specifying the index of the order of definition of the InstrumentLog class // Timestamp logRow.setTimestamp(0, timestamp); // ID of WeatherStation logRow.setString(1, weatherStationId); // Temperture logRow.setFloat(2, temperture); // Live Image data logRow.setBlob(3, liveImage); return logRow;  }\nMulti-Put execution List.3 Multi-Put execution(MultiPut.java)\n// Create Connection store = gridLogic.createGridStore();\nSimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/dd HH:mm\u0026rdquo;, Locale.US);\n// multiput Rows Map\u0026gt; containerRowsMap = new HashMap\u0026lt;\u0026gt;();\n// Create WeatherStation Row Row wsRow = createWeatherStationRow(store, \u0026ldquo;99\u0026rdquo;, \u0026ldquo;new WeaterStation\u0026rdquo;, 45.26, 75.42, true);\n// Add multiput value List wsRowList = new ArrayList\u0026lt;\u0026gt;(); wsRowList.add(wsRow); containerRowsMap.put(\u0026ldquo;weather_station\u0026rdquo;, wsRowList);\n// Create InstrumentLog Container and Row store.putTimeSeries(\u0026ldquo;weather_station_99\u0026rdquo;, InstrumentLog.class);\n// Create WeatherStation Row Row logRow = createIntrumentLogRow(store, format.parse(\u0026ldquo;2016/07/03 12:00:00\u0026rdquo;), \u0026ldquo;weather_station_99\u0026rdquo;, 40.5f, new SerialBlob(new byte[] {0x10, 0x11, 0x12, 0x13, 0x14, 0x15}));\n// Add multiput value List logRowList = new ArrayList\u0026lt;\u0026gt;(); logRowList.add(logRow); containerRowsMap.put(\u0026ldquo;weather_station_99\u0026rdquo;, logRowList);\n// Register by multiput store.multiPut(containerRowsMap);\nResult of Multi-Get execution List.4 Obtain of Multi-Put results(WeatherStation)(MultiPut.java)\nSystem.out.println(\u0026ldquo;##### WeatherStation:\u0026ldquo;); System.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); // Get WeatherStation CollectionweatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class); WeatherStation weatherStation = weatherStationCol.get(rowKey);\nSystem.out.println(String.format(\u0026ldquo;%-3s\\t%-20s\\t%-10s\\t%-10s\\t%-5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera));\nList.5 Result of Multi-Get execution(WeatherStation)\n##### WeatherStation: ID Name Longitude Latitude Camera 99 new WeatherStation 45.26 75.42 true\nList.6 Obtain of Multi-Put results(InstrumentLog)(MultiPut.java)\nSystem.out.println(\u0026ldquo;##### InstrumentLog:\u0026ldquo;); System.out.println(\u0026ldquo;Timestamp\\t\\t\\tWeatherStation ID\\tTemperture\u0026rdquo;); TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_99\u0026rdquo;, InstrumentLog.class); InstrumentLog log = logTs.get(rowKey); // Make a displayable byte string String byteText = InstrumentLogLogic.makeByteString(log.liveImage);\nSystem.out.println(String.format(\u0026ldquo;%s\\t%-20s\\t%-10s\\t%s\u0026rdquo;, log.timestamp, log.weatherStationId, log.temperture, byteText));\nList.7 Result of Multi-Get execution(InstrumentLog)\n##### InstrumentLog: Timestamp WeatherStation ID Temperature Live Image Sun Jul 03 12:00:00 EDT 2016 weather_station_99 40.5\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload:multi-put.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-1_introduction/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.1 Introduction Overview This document introduces the basic programming using GridDB Java APIs. For more information about the GridDB API, please refer to the GridDB API Reference.\nAbout the Sample application The sample application is a Weather Record Storage System, the contents of the \u0026ldquo;delivery system of weather observation data\u0026rdquo; are as follows.\nWeather Record Storage System The weather data that the measuring instrument observes is written at regular intervals to the measurement log.\nThe system has the following data fields:\n Measuring Instrument  Data held  Measuring Instrument ID  To be used as the row key.  Name  The geographic name of where the Measuring Instrument is installed.  Latitude  Latitude of the Measuring Instrument.  Longitude  Longitude of the Measuring Instrument.  Camera Available  Boolean value for if a camera feed is available at that location.    Measurement Log  Records measuring instrument data.  Data held  Timestamp  To be used as the row key.  Measuring instrument ID  ID of instrument which records a log  Temperature  Measured temperature.  Live image  Image data.      The data schema used in the sample application is shown below.\nFigure 1 Schema Definition※ PK indicates a Primary Key.\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-20_multiquery/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.20 Multi-Query What is Multi-Query Note: The concept of Multi-Query is described in our GridDB_TechnicalReference (Section 4.7.2)\nCreate query List.1 Create query(MultiQuery.java)\nprivate static List\u0026gt; createQueries(TimeSeries logTs) throws ParseException, GSException { // Set TimeSeries conditions Date start = TimestampUtils.getFormat().parse(\u0026ldquo;2016-07-01T06:00:00Z\u0026rdquo;); Date end = TimestampUtils.getFormat().parse(\u0026ldquo;2016-07-01T18:00:00Z\u0026rdquo;);\nList\u0026lt;Query\u0026lt;?\u0026gt;\u0026gt; queries = new ArrayList\u0026lt;\u0026gt;(); // Get Max Temperture String maxTempertureTql = String.format( \u0026quot;SELECT MAX(temperture) WHERE\u0026quot; \\+ \u0026quot; TIMESTAMP('%s') \u0026lt; timestamp AND timestamp \u0026lt; TIMESTAMP('%s')\u0026quot;, TimestampUtils.format(start), TimestampUtils.format(end)); Query\u0026lt;AggregationResult\u0026gt; maxTempertureQuery = logTs.query(maxTempertureTql, AggregationResult.class); queries.add(maxTempertureQuery); // Get Min Temperture String minTempertureTql = String.format( \u0026quot;SELECT MIN(temperture) WHERE\u0026quot; \\+ \u0026quot; TIMESTAMP('%s') \u0026lt; timestamp AND timestamp \u0026lt; TIMESTAMP('%s')\u0026quot;, TimestampUtils.format(start), TimestampUtils.format(end)); Query\u0026lt;AggregationResult\u0026gt; minTempertureQuery = logTs.query(minTempertureTql, AggregationResult.class); queries.add(minTempertureQuery); // Get Average String avgTempertureTql = String.format( \u0026quot;SELECT AVG(temperture) WHERE\u0026quot; \\+ \u0026quot; TIMESTAMP('%s') \u0026lt; timestamp AND timestamp \u0026lt; TIMESTAMP('%s')\u0026quot;, TimestampUtils.format(start), TimestampUtils.format(end)); Query\u0026lt;AggregationResult\u0026gt; avgTempertureQuery = logTs.query(avgTempertureTql, AggregationResult.class); queries.add(avgTempertureQuery); // Retrieve by time range Query\u0026lt;InstrumentLog\u0026gt; timeRangeQuery = logTs.query(start, end); queries.add(timeRangeQuery); return queries;  }\nMulti-Query execution List.2 Multi-Query execution(MultiQuery.java)\n// Create Connection store = gridLogic.createGridStore();\n// Get InstrumentLog TimeSeries logTs = store.getTimeSeries(\u0026ldquo;weather_station_1\u0026rdquo;, InstrumentLog.class);\n// Create query list List\u0026gt; queries = createQueries(logTs);\n// Execute Multi Query store.fetchAll(queries);\nResult of Multi-Query execution List.3 Obtain of Multi-Query results(MultiQuery.java)\n// Retrieve reulsts for (Query\u0026lt;?\u0026gt; query : queries) { RowSet\u0026lt;?\u0026gt; rowSet = query.getRowSet(); while (rowSet.hasNext()) { Object rowObj = rowSet.next(); if (rowObj instanceof AggregationResult) { // When retrieve AggregationResult AggregationResult aggregation = (AggregationResult) rowObj; System.out.println(\u0026ldquo;AggregationResult:\u0026rdquo; + aggregation.getDouble());\n } else if (rowObj instanceof InstrumentLog) { // When retrieve InstrumentLog InstrumentLog log = (InstrumentLog) rowObj; System.out.println(String.format(\u0026quot;%s\\\\t%-20s\\\\t%-10s\u0026quot;, log.timestamp, log.weatherStationId, log.temperture)); } else { // Do not reach in this sample System.out.println(rowObj); } }  }\nList.4 Result of Multi-Get execution\nAggregationResult:70.0 AggregationResult:50.0 AggregationResult:60.0 Fri Jul 01 15:00:00 EDT 2016 weather_station_1 75.0 Fri Jul 01 18:00:00 EDT 2016 weather_station_1 70.0 Fri Jul 01 21:00:00 EDT 2016 weather_station_1 60.0 Sat Jul 02 00:00:00 EDT 2016 weather_station_1 50.0 Sat Jul 02 03:00:00 EDT 2016 weather_station_1 60.0 AggregationResult:70.0 AggregationResult:50.0 AggregationResult:60.0 Fri Jul 01 15:00:00 EDT 2016 weather_station_2 75.0 Fri Jul 01 18:00:00 EDT 2016 weather_station_2 70.0 Fri Jul 01 21:00:00 EDT 2016 weather_station_2 60.0 Sat Jul 02 00:00:00 EDT 2016 weather_station_2 50.0 Sat Jul 02 03:00:00 EDT 2016 weather_station_2 60.0\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload:multi-query.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-21_multiget/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.21 Multi-Get What is Multi-Get Note: The concept of Multi-Get is described in our GridDB_TechnicalReference (Section 4.7.2)\nCreate the acquisition conditions List.1 Create the acquisition conditions(MultiGet.java)\nprivate static Map\u0026gt; createMultiGetCondition( CollectionweatherStationCol) throws GSException, ParseException { SimpleDateFormat format = new SimpleDateFormat(\u0026ldquo;yyyy/MM/dd HH:mm\u0026rdquo;, Locale.US);\n // Create search condition of WeatherStation RowKeyPredicate\u0026lt;String\u0026gt; wsRowKeys = RowKeyPredicate.create(String.class); // Create multiget condition Map\u0026lt;String, RowKeyPredicate\u0026lt;?\u0026gt;\u0026gt; containerPredicateMap = new HashMap\u0026lt;\u0026gt;(); for (int i = 0; i \u0026lt; 2; i++) { // Get WeatherStation WeatherStation weatherStation = weatherStationCol.get(String.valueOf(i + 1)); wsRowKeys.add(weatherStation.id); // Create search condition of InstrumentLog RowKeyPredicate\u0026lt;Date\u0026gt; logRowKeys = RowKeyPredicate.create(Date.class); // Set TimeSeries Rows Timestamp logRowKeys.setStart(format.parse(\u0026quot;2016/07/02 6:00\u0026quot;)); logRowKeys.setFinish(format.parse(\u0026quot;2016/07/02 12:00\u0026quot;)); // Add ContainerName and RowKeyPredicate String logContainerName = \u0026quot;weather\\_station\\_\u0026quot; + weatherStation.id; // Put multiget condition containerPredicateMap.put(logContainerName, logRowKeys); } // Put multiget condition String wsContainerName = \u0026quot;weather_station\u0026quot;; containerPredicateMap.put(wsContainerName, wsRowKeys); return containerPredicateMap;  }\nMulti-Get execution List.2 Multi-Get execution(MultiGet.java)\n// Create Connection store = gridLogic.createGridStore();\n// Get Collection CollectionweatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\n// Create MultiGet parameters Map\u0026gt; containerPredicateMap = careteMultiGetCondition(weatherStationCol);\n// Get by multiget Map\u0026gt; multiGetResults = store.multiGet(containerPredicateMap);\nResult of Multi-Get execution List.3 Obtain of Multi-Get results(MultiGet.java)\n// Retrieve results for (Entry\u0026gt; multiGetResult : multiGetResults.entrySet()) { // Container Name String containerName = multiGetResult.getKey(); System.out.println(containerName + \u0026ldquo; ################\u0026rdquo;);\nif (\u0026quot;weather_station\u0026quot;.equals(containerName)) { // Retrieve WeatherStation Rows retieveWeatherStationRows(multiGetResult); } else { // Retrieve InstrumentLog Rows retrieveInstrumentLogRows(multiGetResult); }  }\nList.4 Result of Multi-Get execution\nweather_station_2 ################ Timestamp WeatherStation ID Temperature Live Image Sat Jul 02 06:00:00 JST 2016 weather_station_2 70.0 None Sat Jul 02 09:00:00 JST 2016 weather_station_2 75.0 None Sat Jul 02 12:00:00 JST 2016 weather_station_2 80.0 None weather_station ################ ID Name Longitude Latitude Camera 1 Hokkaido-Sapporo 43.06417 141.34694 true 2 Aomori-Aomori 40.82444 140.74 true weather_station_1 ################ Timestamp WeatherStation ID Temperature Live Image Sat Jul 02 06:00:00 JST 2016 weather_station_1 70.0 None Sat Jul 02 09:00:00 JST 2016 weather_station_1 75.0 None Sat Jul 02 12:00:00 JST 2016 weather_station_1 90.0 None\nSource Code Complete source code used in this sample can be downloaded from the following.\nDownload:multi-get.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-2_griddb-basics/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.2 GridDB Basics Overview We will introduce a series of basic operations in this chapter to help better understand the overall of GridDB API. It should be noted that not all of specification information about specific GridDB API\u0026rsquo;s will be introduced. Please refer to the individual chapter for further details. In addition, this chapter uses the data described in the Introduction.\nApplication Workflow The basic workflow of a GridDB application is as follows:\n Connect  Open a connection from the client program to the GridDB server.  Create a container  Create a container to store the data in GridDB/  Register data  Register the data in the container.  Get the data  Get the data from the container.  Close  Disconnect the client program from the server to free up resources.   The flow of the process is shown below.\nFigure 1 GridDB basic operation sequence\nFirstGridDB in the figure refers to the client program. For more information on classes and each processing will be described below.\nClient program Let\u0026rsquo;s take a look at the basic workflow of a GridDB application. Since the description of the individual parts in the process will be described later, we focus on the overall workflow of the program.\nList.1 client program (FirstGridDB.java)\npackage sample;\nimport java.util.Properties;\nimport com.toshiba.mwcloud.gs.Collection; import com.toshiba.mwcloud.gs.GSException; import com.toshiba.mwcloud.gs.GridStore; import com.toshiba.mwcloud.gs.GridStoreFactory;\nimport sample.row.WeatherStation;\npublic class FirstGridDB {\n public static void main(String\\[\\] args) throws GSException { // Set the Connection parameter for GridDB Properties props = new Properties(); props.setProperty(\u0026quot;host\u0026quot;, \u0026quot;127.0.0.1\u0026quot;); props.setProperty(\u0026quot;port\u0026quot;, \u0026quot;10001\u0026quot;); props.setProperty(\u0026quot;clusterName\u0026quot;, \u0026quot;GSCLUSTER\u0026quot;); props.setProperty(\u0026quot;database\u0026quot;, \u0026quot;public\u0026quot;); props.setProperty(\u0026quot;user\u0026quot;, \u0026quot;admin\u0026quot;); props.setProperty(\u0026quot;password\u0026quot;, \u0026quot;admin\u0026quot;); GridStore store = GridStoreFactory.getInstance().getGridStore(props); // Create Collection Collection weatherStationCol = store.putCollection(\u0026quot;weather_station\u0026quot;, WeatherStation.class); // Set the value to Row data WeatherStation weatherStation1 = new WeatherStation(); weatherStation1.id = \u0026quot;1\u0026quot;; weatherStation1.name = \u0026quot;WeatherStation 01\u0026quot;; weatherStation1.latitude = 35.68944; weatherStation1.longitude = 139.69167; weatherStation1.hasCamera = true; WeatherStation weatherStation2 = new WeatherStation(); weatherStation2.id = \u0026quot;2\u0026quot;; weatherStation2.name = \u0026quot;WeatherStation 02\u0026quot;; weatherStation2.latitude = 35.02139; weatherStation2.longitude = 135.75556; weatherStation2.hasCamera = false; // Register Collection weatherStationCol.put(weatherStation1); weatherStationCol.put(weatherStation2); // Retrieve Collection System.out.println(\u0026quot;get by key\u0026quot;); System.out.println(\u0026quot;ID\\\\tName\\\\t\\\\t\\\\tLongitude\\\\tLatitude\\\\tCamera\u0026quot;); weatherStationCol = store.getCollection(\u0026quot;weather_station\u0026quot;, WeatherStation.class); for (int i = 0; i \u0026lt; 2; i++) { WeatherStation weatherStation = weatherStationCol.get(String.valueOf(i + 1)); System.out.println(String.format(\u0026quot;%-3s\\\\t%-20s\\\\t%-10s\\\\t%-10s\\\\t%-5s\u0026quot;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); } // Close Connection store.close(); }  }\n1. Connection Let\u0026rsquo;s connect to the GridDB.\nList.2 connection process (FirstGridDB.java)\n// Set the Connection parameter for GridDB Properties props = new Properties (); props.setProperty(\u0026ldquo;host\u0026rdquo;, \u0026ldquo;127.0.0.1\u0026rdquo;); props.setProperty(\u0026ldquo;port\u0026rdquo;, \u0026ldquo;10001\u0026rdquo;); props.setProperty(\u0026ldquo;clusterName\u0026rdquo;, \u0026ldquo;GSCLUSTER\u0026rdquo;); props.setProperty(\u0026ldquo;database\u0026rdquo;, \u0026ldquo;public\u0026rdquo;); props.setProperty(\u0026ldquo;user\u0026rdquo;, \u0026ldquo;admin\u0026rdquo;); props.setProperty(\u0026ldquo;password\u0026rdquo;, \u0026ldquo;admin\u0026rdquo;); GridStore store = GridStoreFactory.getInstance().getGridStore(props);\n L.16-22: Set the connection poperties. L.23: From a GridStoreFactory instance, get a store object that will be later used to specify the connection information.  For more information and further details about connecting to GridDB, please see the [Connection chapter] (../sample_app_prepare_connection).\n2. Creating a container Containers can be made easily by defining the data as a class. It is possible to specify the data type dynamically in the program as well. For more information is available in the Container Create/Delete and also the meta-information chapters.\nList.3 measuring instrument class (WeatherStation.java)\npackage sample.row;\nimport com.toshiba.mwcloud.gs.RowKey;\n/** * Class that represents the definition of the schema. */ public class WeatherStation { /** * ID of WeatherStation */ @RowKey public String id;\n/\\*\\* \\* Name of WeatherStation */ public String name; /\\*\\* \\* Installation Latitude */ public double latitude; /\\*\\* \\* Installation Longitude */ public double longitude; /\\*\\* \\* Camera exists or not */ public boolean hasCamera;  }\n L.12-13: The @RowKey keyword specifies which variable becomes the key of the container.   List.4 container creation process (FirstGridDB.java)\n// Create Collection Collection weatherStationCol = store.putCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\n L.26-27: Creates the container with name and collection class type.   3. Register the data Ready access to GridDB in the above process is now ready. Let\u0026rsquo;s try to register the data in GridDB.\nList.5 data registration processing (FirstGridDB.java)\n// Set the value to Row data WeatherStation weatherStation1 = new WeatherStation(); weatherStation1.id = \u0026ldquo;1\u0026rdquo;; weatherStation1.name = \u0026ldquo;WeatherStation 01\u0026rdquo;; weatherStation1.latitude = 35.68944; weatherStation1.longitude = 139.69167; weatherStation1.hasCamera = true;\nWeatherStation weatherStation2 = new WeatherStation(); weatherStation2.id = \u0026ldquo;2\u0026rdquo;; weatherStation2.name = \u0026ldquo;WeatherStation 02\u0026rdquo;; weatherStation2.latitude = 35.02139; weatherStation2.longitude = 135.75556; weatherStation2.hasCamera = false;\n// Register Collection weatherStationCol.put(weatherStation1); weatherStationCol.put(weatherStation2);\n L.30-42: Set the values of the data to be registered. L.45-46: Pack data and register it in the container.  For more information on data registration, please see that chapter. There is also further information about TimeSeries data registration in that chapter.\n4. Data Retrieval Now the just registered data can be retrieved from the GridDB server.\nList.6 data acquisition process (FirstGridDB.java)\n// Retrieve Collection System.out.println(\u0026ldquo;get by key\u0026rdquo;); System.out.println(\u0026ldquo;ID\\tName\\t\\t\\tLongitude\\tLatitude\\tCamera\u0026rdquo;); weatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class);\nfor (int i = 0; i \u0026lt; 2; i++) { WeatherStation weatherStation = weatherStationCol.get(String.valueOf(i + 1)); System.out.println(String.format(\u0026ldquo;%-3s\\t%-20s\\t%-10s\\t%-10s\\t%-5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); }\n L.51: First get the container by specifying the container name and class. L.54: Then get row data by specifying the key.  Here is the output:\nList.7 data acquisition result\nget by key ID Name Longitude Latitude Camera 1 WeatherStation 01 35.68944 139.69167 true 2 WeatherStation 02 35.02139 135.75556 false\nFurther details are explained in the Data retrieval chapter. Also refer to TimeSeries data retrieval.\n5. Closing the Connection Finally, cut the client program and GridDB.\nList.8 data acquisition process (FirstGridDB.java)\n// Close Connection store.close ();\n L.61: Release the used resources on both the client and the server.   This chapter provided a brief overview of the GridDB workflow and should give an understanding of GridDB\u0026rsquo;s basic functions. Image of the program to access the GridDB is I think I was able to understand. There are many more functions in the GridDB API that are not described here. Please refer to each specific for more details.\nSource code Complete source code used in this chapter can be downloaded from the following.\nDownload: griddb-first.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-3_prepare-server/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.3 Preparation: Environment (server) Overview This chapter covers setting up a GridDB server to use while running the sample application.\nComponents The environment will be configured as follows:\nFigure 1 block diagram\nTable 2 middleware version\nmiddleware\nversion\nGridDB Community Edition\n2.8.0\nDocker\n1.12.0\nDocker Compose\n1.8.0\nThe environment in which you run GridDB will be built in a container using Docker but you can apply the same concepts to other VMs or a native OS to suit your environment. We will install Docker to the host machine\u0026rsquo;s OS (Centos 7), then manually build a single node GridDB server and then a two node GridDB cluster using docker-compose.\nInstall Docker Install Docker:\nList.1 Docker installation of\n$ sudo curl -fsSL https://get.docker.com/ | sh $ sudo usermod -aG docker vagrant $ sudo systemctl enable docker $ sudo systemctl start docker\n L.1: Download and run the Docker installation script. L.2: Create a user who runs Docker. L.3: Enable boot time start up of Docker. L.4: Start Docker.   Create the Docker container These steps create the container where you\u0026rsquo;ll run GridDB:\nList.2 Docker container creation\n$ sudo docker pull centos: 6.7 $ sudo docker network create \u0026ndash;subnet=192.168.11.0/24 griddb_default $ sudo docker run -it \u0026ndash;name gsnode1 -h gsnode1 \u0026ndash;net=griddb_default \u0026ndash;ip=192.168.11.11 -d centos: 6.7\n L.1: Download the CentOS image. L.2: Create a network for the container to run GridDB. L.3: Create a container from the CentOS image. This will use the network settings from the previously created network.   Install GriddDB in Docker container. With the Docker container configured , you can install and configure GridDB.\nList.3 container login\n$ sudo docker exec -it griddb_griddb1_1 bash\nLine 1: Run a shell on the GridDB container and log in.\nList.4 network settings\n# hostname -i 192.168.11.11 # cat /etc/sysconfig/network NETWORKING=yes HOSTNAME=localhost.localdomain # vi /etc/sysconfig/network HOSTNAME= 192.168.11.11 (to change to 192.168.11.11)\n L.1-2: Make sure the IP address has been set for the hostname. L.6-7: Set the IP address in the HOSTNAME of/etc/sysconfig/network.\n  List.5 GridDB installation\n# curl -L -O https://github.com/griddb/griddb_nosql/releases/download/v2.8.0/griddb_nosql-2.8.0-1.el6.x86_64.rpm # rpm -Uvh griddb_nosql-2.8.0-1.el6.x86_64.rpm\n L.1: Fetch the GridDB RPM file from Github/ L.2: Install the RPM.  Then perform the initial configuration of GridDB.\nList.6 gs_cluster.json\n# cat /var/lib/gridstore/conf/gs_cluster.json (Snip) \u0026ldquo;Cluster\u0026rdquo;: { \u0026ldquo;ClusterName\u0026rdquo;: \u0026ldquo;\u0026rdquo;, (Snip) \u0026ldquo;LoadbalanceCheckInterval\u0026rdquo;: \u0026ldquo;180s\u0026rdquo; (Snip) # vi /var/lib/gridstore/conf/gs_cluster.json (Snip) \u0026ldquo;Cluster\u0026rdquo;: { \u0026ldquo;ClusterName\u0026rdquo;: \u0026ldquo; GSCLUSTER \u0026ldquo;, (to set the cluster name clusterName) (Snip) \u0026ldquo;LoadbalanceCheckInterval\u0026rdquo;: \u0026ldquo;180s\u0026rdquo;, \u0026ldquo;NotificationMember\u0026rdquo;: [ { \u0026ldquo;cluster\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;192.168.11.11\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: 10010}, \u0026ldquo;cync\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;192.168.11.11\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: 10020}, \u0026ldquo;system\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;192.168.11.11\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: 10040}, \u0026ldquo;sransaction\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;192.168.11.11\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: 10001}, \u0026ldquo;sql\u0026rdquo;: { \u0026ldquo;address\u0026rdquo;: \u0026ldquo;192.168.11.11\u0026rdquo;, \u0026ldquo;port\u0026rdquo;: 20001} } ] (Snip)\n L.4: The initial clusterName will be empty(\u0026ldquo;\u0026rdquo;) and GridDB will fail to start. L.9: Set clusterName to GSCLUSTER. This setting will be used by applications to connect later. L.14-22: Contains address and port information using GridDB\u0026rsquo;s fixed list format. If you want to configure a cluster of multiple nodes, all nodes information must be listed here.   List.7 Set default user password\n# su - gsadm $ gs_passwd admin Password: ** admin ** (enter the password) Retype password: ** admin ** (and then re-enter the password)\n L.1: Internal GridDB commands need to be run as the gsadm user. L.2 to 4: Sets the admin user\u0026rsquo;s password, which is the default user on GridDB.\n(Note) This admin user is not a user of the OS. The GridDB user name and password will be required for the sample application to connect later.  Now GridDB can be started.\nList.8 Start GridDB\n# su - gsadm $ gs_startnode -u ** admin/admin ** -w 0 \u0026hellip;\u0026hellip;. It started node. $ gs_joincluster -c ** GSCLUSTER ** -u ** admin/admin ** -w 0 . Joined node\n L.2: Start the GridDB service. Wait until the start-up in the -w 0 option. You will need to specify the user name and password with the -u flag, such as -u /. L.5: Either join or create a cluster. Specify the cluster name with the -c option that was set in Listing 6.  Setting of one of GridDB node is now complete.\nDockerfile The Dockerfile contains all the previous work and is able to perform the above configuration steps automatically. It is useful for creating many nodes.\nList 9 Dockerfile\nFROM centos: 6.7\nRUN set -x \u0026amp;\u0026amp;\nHOST=`hostname` \u0026amp;\u0026amp;\nsed -i \u0026ldquo;s/^ \\ (HOSTNAME=\\). */\\ 1 $ HOST /\u0026ldquo;/etc/sysconfig/network \u0026amp;\u0026amp;\ncurl -L -O https://github.com/griddb/griddb_nosql/releases/download/v2.8.0/griddb_nosql-2.8.0-1.el6.x86_64.rpm \u0026amp;\u0026amp;\nrpm -Uvh griddb_nosql-2.8.0-1.el6.x86_64.rpm\nARG CLUSTER_CONF RUN echo $CLUSTER_CONF ADD $CLUSTER_CONF /var/lib/gridstore/conf/\nRUN set -x \u0026amp;\u0026amp;\nchown gsadm: gridstore/var/lib/gridstore/conf/$ CLUSTER_CONF \u0026amp;\u0026amp;\nchmod 640/var/lib/gridstore/conf/$ CLUSTER_CONF \u0026amp;\u0026amp;\nsu - gsadm -c \u0026ldquo;gs_passwd admin -p admin\u0026rdquo;\nCMD su - gsadm -c \u0026ldquo;gs_startnode -u admin/admin -w 0 \u0026amp;\u0026amp;gs_joincluster -c GSCLUSTER -n 2 -u admin/admin\u0026rdquo; \u0026amp;\u0026amp;\ntail -f/dev/null\nIt summarizes the settings that were set manually in the above.\n L.9: Allows the gs_cluster.conf input to be specified for use with docker-compose.yml L.19: Too build a cluster of multiple nodes, the gs_joincluster command needs the -n that specifies the number of nodes.   Install Docker Compose If you create more than one similar Docker container, it is worth while to use Docker Compose which can automatically create and boot multiple containers.\nList.10 Install Docker Compose\n$ curl -L https://github.com/docker/compose/releases/download/1.8.0/docker-compose-`uname -s`-`uname -m` -O $ chmod + x docker-compose-* $ sudo mv docker-compose-* /usr/local/bin/docker-compose\n L.1: Get the Docker Compose for your system binary from Github. L.2: Grant executable permissions to the binary. L.3: Move the binary to /usr/local/bin, renaming it docker-compose  The following is the contents of Docker Compose\u0026rsquo;s configuration file, docker-compose.yml:\nList.11 Docker Compose configuration file\nversion: \u0026lsquo;2\u0026rsquo; services: griddb1: build: context . args: CLUSTER_CONF: \u0026ldquo;gs_cluster.json\u0026rdquo; container_name: gsnode1 networks: default: ipv4_address: 192.168.11.11 ports: - \u0026ldquo;10001:10001\u0026rdquo; - \u0026ldquo;20001:20001\u0026rdquo; expose: - \u0026ldquo;10010\u0026rdquo; - \u0026ldquo;10020\u0026rdquo; - \u0026ldquo;10040\u0026rdquo; - \u0026ldquo;10001\u0026rdquo; - \u0026ldquo;20001\u0026rdquo; griddb2: build: context . args: CLUSTER_CONF: \u0026ldquo;gs_cluster.json\u0026rdquo; container_name: gsnode2 networks: default: ipv4_address: 192.168.11.12 ports: - \u0026ldquo;12001:10001\u0026rdquo; - \u0026ldquo;22001:20001\u0026rdquo; expose: - \u0026ldquo;10010\u0026rdquo; - \u0026ldquo;10020\u0026rdquo; - \u0026ldquo;10040\u0026rdquo; - \u0026ldquo;10001\u0026rdquo; - \u0026ldquo;20001\u0026rdquo;\nnetworks: default: ipam: config: - subnet: 192.168.11.0/24\nIn the Docker compose file, you have to specify the parameters of the container that had been specified. It also specifies the respective configuration files which need to represent configured values.\nThe files that will be used in the Docker Compose are placed in a directory as follows:\nList.12 Docker Compose configuration file configuration\ngriddb / - Docker-compose.yml - Dockerfile - gs_cluster_1.json - gs_cluster_2.json\nFrom that directory, run the commands that will create the Container, installer GridDB, and then start the container.\nList.13 Docker Compose configuration file placement\n$ cd griddb $ sudo /usr/local/bin/docker-compose up -d $ sudo /usr/local/bin/docker-compose ps Name Command State Ports -------------------------------------------------- -------------------------------------------------- -------------------------------------- gsnode1/bin/sh -c su - gsadm -c \u0026ldquo;\u0026hellip; Up 127.0.0.1:10001-\u0026gt;10001/tcp, 10010/tcp, 10020/tcp, 10040/tcp, 127.0.0.1:20001-\u0026gt;20001/tcp gsnode2/bin/sh -c su - gsadm -c \u0026ldquo;\u0026hellip; Up 0.0.0.0:12001-\u0026gt;10001/tcp, 10010/tcp, 10020/tcp, 10040/tcp, 0.0.0.0:22001-\u0026gt;20001/tcp\n L.1: Change directory to where the above configuration files were created. L.2: In accordance with the docker-compose command in the configuration file, if Docker container has not yet been created then create and start a container. If the container has already been created, it will only be started. L.3 to 7: it displays the status of the container that was started in the docker-compose.   Files The set of files used in this chapter can be downloaded from: griddb-server-conf.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-4_prepare-client/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.4 Preparation: Environment (client) Overview This chapter shows the procedure to build an environment where you can build and execute GridDB Java applications. These instructions are specifically use Windows 10 (64 bit) as a development environment and CentOS 7.2 (64 bit) as an execution environment but can be adapted for other operating systems as well.\nObtaining the files First download the following the files to your build environment, the tested versions and filenames are listed in Table 1.\nTable 1 file available to\nSoftware\nVersion\nFile name\nSite to get\nJava(Java SE Development Kit) (Windows)\n8u102\njdk-8u101-windows-x64.exe\nJava SE Development Kit 8\nJava(Java SE Development Kit) (Linux)\n8u102\njdk-8u101-linux-x64.rpm\nJava SE Development Kit 8\nEclipse(Eclipse IDE for Java Developers)\nNeon\neclipse-java-neon-R-win32-x86_64.zip\nEclipse IDE for Java Developers\nConstruction of Build Environment Java Installation Table 1 there run the Java installer (jdk-8u102-windows-x64.exe), and install it. If you do not want to change the setting, it will be installed in C:\\ Program Files \\ Java \\ jdk1.8.0_102.\nSetting of environment variable First set Java\u0026rsquo;s path in the Windows environment variable registry.\nControl Panel \u0026gt; system and security \u0026gt; system \u0026gt; Advanced System Settings and click , Open the properties of the system.\nFigure 1 system Advanced\nClick on the Advanced tab of environment variable (N) ... button.\nFigure 2 environment variable\nAdd the following to the environment variable to the system environment variables:\n Variable name: JAVA_HOME Variable value: C:\\Program Files\\Java\\jdk1.8.0_102  Figure 3 Java environment variable\nIn addition, the system path must be changed by first selecting \u0026lsquo;Path\u0026rsquo; in the list, clicking the \u0026lsquo;Edit\u0026rsquo; button, and then then inserting \u0026ldquo;%JAVA_HOME%;\u0026rdquo; to the beginning of the text field.\nFigure 4 Edit Path\nAfter you change the above environment variables click the \u0026lsquo;OK\u0026rsquo; button.\nInstall Eclipse Follow Eclipse\u0026rsquo;s instructions on how to run the Eclipse. No specific options are required for GridDB development.\nImport of source code After Eclipse is installed you can start it and create a workspace in any location.\nDownload the source code for the first GridDB sample presented in GridDB Basics here: griddb_first.zip.\nFigure 5 Creatde a Workspace\nIn the Eclipse Package Exploer click the \u0026lsquo;Import\u0026rsquo; function:\n6 project import of\nSelect General \u0026gt; Existing Projects and click the Next button.\n7 import method selection\nSelect the Zip file that you downloaded and click the Finish button.\n8 import method selection\nBuild of jar file Use the Ant build.xml that will compile and generates a jar file.\nIn the Package Explorer build \u0026gt; to select build.xml to display the context menu, Click on the 1 Ant Build Run As \u0026gt;.\n9 build run\nIf the build is successful, jar file in the folder out is output. When you click the Refresh in the context menu of the Package Explorer, It shows.\nFigure 10 build success\nConstruction of execution environment Then, in gshost that you created in the Environment (server)] (../ sample_app_prepare_server) Install the Java in order to run the sample application.\nInstallation of Java Place Table 1 of Java of rpm files (jdk-8u101-linux-x64.rpm) to any location on the gshost, Please install the following procedure.\n$ cd /tmp $ sudo rpm -Uvh /tmp/jdk-8u101-linux-x64.rpm $ sudo alternatives \u0026ndash;config java\nThere are 2 programs which provide \u0026lsquo;java\u0026rsquo;.\n Selection Command ----------------------------------------------- * + 1 /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.111-2.6.7.2.el7_2.x86_64/jre/bin/java 2 /usr/java/jdk1.8.0_102/jre/bin/java\nPress Enter to keep the current selection [+], or type the selection number you wish too use. In this case, we want to use 2 . Now java returns corresponds with the correct version.\n$ java -version java version \u0026ldquo;1.8.0_102\u0026rdquo; Java \u0026trade; SE Runtime Environment (build 1.8.0_102-b14) Java HotSpot \u0026trade; 64-Bit Server VM (build 25.102-b14, mixed mode)\nPlacement of the jar file Create a sample directory and copy the files from the built Eclipse project as follows:\nsample / ├─lib \u0026hellip; Directly copy the lib directory of the Eclipse project. │ gridstore-conf.jar │ gridstore.jar │ opencsv-3.8.jar ├─data \u0026hellip; Directly copy the data directory in the Eclipse project. │ liveimage1.jpg │ liveimage2.jpg │ instrument_log.csv │ weather_station.csv └─ \u0026lt; jar file \u0026gt; \u0026hellip; Copy the jar file from the output directory of the Eclipse project.\nYou can run the sample application as follows:\n$ java -jar \u0026lt; jar file \u0026gt;\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-5_connection/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.5 Preparation: Connecting to GridDB Overview This section describes the connecting to and disconnecting from a GridDB server.\nConnecting to GridDB In order to connect with the GridDB server, we use a Property class to set the connnection details which will be used by GridStoreFactory to open a connection. Refer to Environment (server) on what these settings are configured as in your envrionment.\nList.1 Connecting to a GridDB server (GridDBLogic.java)\n// Set the Connection parameter for GridDB Properties props=new Properties (); props.setProperty(\u0026ldquo;host\u0026rdquo;, \u0026ldquo;127.0.0.1\u0026rdquo;); props.setProperty(\u0026ldquo;port\u0026rdquo;, \u0026ldquo;10001\u0026rdquo;); props.setProperty(\u0026ldquo;clusterName\u0026rdquo;, \u0026ldquo;GSCLUSTER\u0026rdquo;); props.setProperty(\u0026ldquo;database\u0026rdquo;, \u0026ldquo;public\u0026rdquo;); props.setProperty(\u0026ldquo;user\u0026rdquo;, \u0026ldquo;admin\u0026rdquo;); props.setProperty(\u0026ldquo;password\u0026rdquo;, \u0026ldquo;admin\u0026rdquo;); GridStore store=GridStoreFactory.getInstance().getGridStore(props);\nParameters of GridDB connection are as follows.\nL.24: host specifies the destination host name or IP address (IPV4 only).\nL.25: port specifies the destination port number.\nL.26: clusterName specifies the destination cluster name.\nL.27: database specifies the database name of the destination. This value is the default can be omitted is the \u0026ldquo;public\u0026rdquo;.\nL.28: user Specifies the user name of GridDB server.\nL.29: password specifies the password of GridDB server.\nFor more information on the parameters other than the above, refer to the GridDB API Reference.\nCut from GridDB After the application\u0026rsquo;s GridDB tasks have completed, the connection should be closed.\nList.2 cut from GridDB(CollectionCreate.java)\nGridStore store=null; try { // Processing to GridDB (Snip) } finally { // Close Connection if (store!=null) { store.close (); } }\nL.8: Using close() within a try/finally block.\nThe following classes have a close() method that can terminate the connection with GridDB.\n GridStoreFactory GridStore Container Query RowSet   Source code Complete source code used in this sample can be downloaded here:\nDownload: container-create-drop.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-6_container-schema/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.6 Schema definition Overview This chapter describes the schema definition and data types.\nSchema definition Data of the measuring instruments and instrument log that will be handled in this sample application are defined in the container as follows.\nTable.1 Container Information\nData\nContainer type\nQuantity\nContainer name\nRow key\nMeasuring instrument\nCollection\n1\nweather_station\nInstrument ID\nInstrument log\nTimeSeries Container\n1 per instrument\nweather_station_\n(Example:weather_station_1)\nLogging date and time\nOne collection will be created with rows corresponding to different measuring instruments..\nOne TimeSeries container will be created as the instrument log for each measuring instrument.\nThe instrument ID to will be included in the instrument log\u0026rsquo;s container name.\nThere are two ways to use Container in your programs.\nOne is to pre-define the class statically which represents a row of the container.\nThe other is to create a Container dynamically without preparing the pre-defined class.\nHere we describe only the static method, so please refer to Meta-information for more details about dynamic method.\nList.1 Measuring instrument class(WeatherStation.java)\npackage sample.row;\nimport com.toshiba.mwcloud.gs.RowKey;\n/** * Class that represents the definition of the schema. */ public class WeatherStation { /** * ID of WeatherStation */ @RowKey public String id;\n/\\*\\* \\* Name of WeatherStation */ public String name; /\\*\\* \\* Installation Latitude */ public double latitude; /\\*\\* \\* Installation Longitude */ public double longitude; /\\*\\* \\* Camera exists or not */ public boolean hasCamera;  }\n L.12: The @RowKey annotates the field to be used as a row key.   List.2 Instrument log class(InstrumentLog.java)\npackage sample.row;\nimport java.sql.Blob; import java.util.Date;\nimport com.toshiba.mwcloud.gs.RowKey;\n/** * Class that represents the definition of the Row of Instrument log. */ public class InstrumentLog { /** * Timestamp of log */ @RowKey public Date timestamp;\n/\\*\\* \\* ID of Weather Station */ public String weatherStationId; /\\*\\* \\* Temperature of the measurement result */ public float temperture; /\\*\\* \\* Image data obtained by photographing the sky */ public Blob liveImage;  }\n L.15: The @RowKey annotates the field to be used as a row key.   Column Data Types The following types of data can be used in a GridDB column.\nTable.2 Available column type\nColumn Type\nType in Program\nDescription\nBOOL\nboolean or Boolean\nTRUE or FALSE.\nSTRING\nString\nZero or more Unicode characters excluding the NULL character (U+0000).\nBYTE\nByte or byte\nInteger value in the following range.(-27 to 27-1)\nSHORT\nShort or short\nInteger value in the following range.(-215 to 215-1)\nINTEGER\nInteger or int\nInteger value in the following range.(-231 to 231-1)\nLONG\nLong or long\nInteger value in the following range.(-263 to 263-1)\nFLOAT\nFloat or float\nSingle-precision floating point number.\nDOUBLE\nDouble or double\nDouble-precision floating point number.\nTIMESTAMP\njava.util.Date\nCombination of a date and time consisting of year, month, day, hour, minute and second.\nBLOB\njava.sql.Blob\nBinary data.\nAll types except BLOB can be stored as an array.\nThis chapter demonstrated how to define a schema and use it in your GridDB applications.\nPlease refer to the GridDB Programming Tutorial for more details on schema definitions.\nComplete source code The complete source code used in this sample can be downloaded from the following:\nDownload: container-schema.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-7_container-create-drop/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.7 Container Creation and Deletion Overview This chapter describes how to create/delete GridDB containers.\nCollection and Timeseries Container There are 2 container types, a timeseries container and a collection.\n Timeseries container  A container that specializes for data whcih is consisted of the pair of time and value, such as sensor data. The TIMESTAMP data can be specified in a row key. It is possible to acquire the data in a specified period of time or to aggregate the data. Value can be linearly interpolated when register or retreive the data.  Collection  Collection is a general purpose container. Collection does not have any functions related to timeseries data, however it can be used with any data type in a key and it can be used in the same way as the other key-value database.   Here are some programming samples to create or delete each type of containers.\nHow to create a Collection Collection can be created as follows.\nList.1 Creating a Collection(WeatherStationLogic.java)\n// Create Collection CollectionweatherStationCol = store.putCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class); return weatherStationCol;\n L.34-35: Use GridStore.putCollection(String, Class) method to create a Collection.\nA container name should be specified to String, and WeatherStation class which can be created in chapter Schema definition should be set to Class.\nA created Collection will obtained as an instance of com.toshiba.mwcloud.gs.Collection class.   List.2 Releasing a Collection(CollectionCreate.java)\n} finally { // Close Connection weatherStationCol.close(); }\n L.183: Use Collection.close() method to release associated resources by closing the created Collection.   How to create a Timeseries Container Timeseries Container can be created as follows.\nList.3 Creating a Timeseries Container(InstrumentLogLogic.java)\nfor (int i = 0; i \u0026lt; WeatherStationLogic.JP_PREFECTURE; i++) { // Create TimeSeries Container TimeSeries ts = store.putTimeSeries(\u0026ldquo;weather_station_\u0026rdquo; + (i + 1), InstrumentLog.class);\n(snip)  }\n L.137-138: Use GridStore.putTimeSeries(String, Class) method to create a Timeseries Container.\nA Timeseries container name should be specified to String, and InstrumentLog class which can be created in chapter Schema definition should be set to Class.\nA created Timeseries Container will obtained as an instance of com.toshiba.mwcloud.gs.TimeSeries class.   List.4 Releasing a Timeseries Container(InstrumentLogLogic.java)\n// Close TimeSeries Container ts.close();\n L.56: In the same manner as Collection, use Collection.close() method to release associated resources by closing the created Timeseries Container.   How to delete a Container List.5 Deleting Collection(WeatherStationLogic.java)\npublic void dropCollection(GridStore store) throws GSException { // Drop Collection store.dropCollection(\u0026ldquo;weather_station\u0026rdquo;); }\n L.47: Use GridStore.dropCollection(String) method to delete a Collection.   List.6 Deleting Timeseries Container(SampleTimeSeries.java)\npublic void dropTimeSeries(GridStore store, String name) throws GSException { // Drop TimeSeries Container store.dropTimeSeries(name); }\n L.69: Use GridStore.dropTimeSeries(String) method to delete a Timeseries Container.  This chapter provided how to create or delete 2 types of Container, a Collection and a Timeseries Container.\nContainer can be easily created or deleted by simply preparing a container name and a class that represents the rows.\nHere we describe only the static method of preparing a class that represents a row in advance, but there is a way to dynamically create a Container without providing a class.\nPlease refer to Meta-information for more details.\nComplete source code Complete source code used in this sample can be downloaded from the following.\nDownload: container-create-drop.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-8_collection-register/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.8 Collection Registration Overview This chapter describes registering data in a GridDB collection.\nData to be registered Create registration data in a CSV file with the contents as follows:\n Measuring instrument CSV file  File name  Weather_station.csv  Retention data  Row 1: measuring instrument ID Row 2: The name Row 3: installation coordinates (latitude) Row 4: installation coordinates (longitude) Row 5: camera presence     List.1 Data File (weather_station.csv)\n1,Hokkaido-Sapporo,43.06417,141.34694,true 2,Aomori-Aomori-City,40.82444,140.74,false 3,Iwate-Morioka,39.70361,141.1525,true 4,Miyagi-Sendai,38.26889,140.87194,false 5,Akita-Akita,39.71861,140.1025,true (Snip)\nData registration The following code demonstrates registering data:\nList.2 Simple Data Registration (CollectionRegister.java)\n// Read WeatherStation data from csv List weatherStationList=wsLogic.readCsv (); // Register Collection wsLogic.registerCollection (store, weatherStationList);\n L.26: Read the CSV to get a list of measuring instruments. L.28: Register the list of measuring instruments in a GridDB collection.   List.3 Read CSV (WeatherStationLogic.java)\npublic List readCsv() throws IOException { List weatherStationList = new ArrayList(); // Read CSV file CSVReader reader = new CSVReader(new FileReader(\u0026ldquo;data/weather_station.csv\u0026rdquo;));\ntry { String\\[\\] line = null; WeatherStation weatherStation = null; // Read all line of CSV while ((line = reader.readNext()) != null) { // Set the value to Row of WeatherStation weatherStation = new WeatherStation(); weatherStation.id = line\\[0\\]; weatherStation.name = line\\[1\\]; weatherStation.latitude = Double.valueOf(line\\[2\\]); weatherStation.longitude = Double.valueOf(line\\[3\\]); weatherStation.hasCamera = Boolean.valueOf(line\\[4\\]); // Add Collection weatherStationList.add(weatherStation); } } finally { // Close CSV reader.close(); } return weatherStationList;  }\nRead CSV  L.31: Read a CSV file weather_station.csv that contains the measuring instrument\u0026rsquo;s data. The operation of reading the CSV file uses the CSVReader class of which is an open source software opencsv. L.38-45: Read the contents of the CSV and set the values to an instance of WeatherStation.   Register Collection List.4 Register Data (WeatherStationLogic.java)\npublic void registerCollection(GridStore store, List weatherStationList) throws GSException { // Get Collection CollectionweatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class); try { // Disable Auto Commit weatherStationCol.setAutoCommit(false);\n boolean isSuccess = true; for (WeatherStation weatherStation : weatherStationList) { if (weatherStation != null) { // Add Collection weatherStationCol.put(weatherStation.id, weatherStation); } else { // if Row Data is null, abort register weatherStationCol.abort(); System.out.println(\u0026quot;Register of the Collection is aborted.\u0026quot;); isSuccess = false; break; } } // Commit only when the registration was successful all if (isSuccess) { // Commit weatherStationCol.commit(); } } finally { // Close Connection weatherStationCol.close(); }  }\nGet Container  L.67-68: Get the subject of container to be registered.   Auto-commit  L.71: Disable auto-commit.  Auto-commit is set by Collection.setAutoCommit () method. Auto-commit by default in GridDB is enabled.\nWhen you register the data collectively, to disable auto-commit makes a reduction in the number of times of the inquiry, therefore increasing the throughput is expectable.\nAdd Data  L.78: Add the value read from CSV to the collection.  In this code auto-commit is disabled, but If auto-commit is enabled, the data will be registered here.\nCommit  L.91: Commit to a collection.  Commit is executed by Collection.commit () method. If auto-commit is disabled, the data is registered here.\nRoll back  L.81: Execute rollback to collection.  Rollback is executed by Collection.abort () method.\nRollback must be executed only when an exception occurs other than GSException or problems are detected. Otherwise operation of the pre-commit will be canceled if an exception GSException occurs when auto-commit is disabled.\nIn this sample, if the data of the measuring instrument read from the CSV is NULL, execute rollback assuming that a problem occurs.\nComplete source code Complete source code used in this sample can be downloaded from the following.\nDownload: collection-register.zip\n"
},
{
	"uri": "http://example.org/sample-applications/basics/5-1-9_collection-retrieve/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Table of Contents\n5.1.9 Data Retrieval Overview This chapter covers retrieving data using the GridDB API.\nRetrieve data Retrieve data in a collection with specifying a Row key.\nList.1 Data Read (WeatherStationLogic.java)\n// Get Collection Collection weatherStationCol = store.getCollection(\u0026ldquo;weather_station\u0026rdquo;, WeatherStation.class); }\n L.24-25: Retrieve a collection by specifying a container name  List.2 Data Read by Row Key(CollectionRetrieve.java)\ntry { System.out.println(\u0026ldquo;ID \\tName \\t \\t \\tLongitude \\tLatitude \\tCamera\u0026rdquo;); for (int i=0; i \u0026lt; WeatherStationLogic.JP_PREFECTURE; i ++) { // Retrieve row by key WeatherStation weatherStation=weatherStationCol.get (String.valueOf (i + 1)); System.out.println (String.format(\u0026ldquo;% - 3s \\t% -20s \\t% -10s \\t% -10s \\t% -5s\u0026rdquo;, weatherStation.id, weatherStation.name, weatherStation.latitude, weatherStation.longitude, weatherStation.hasCamera)); } } Finally { // Close Connection weatherStationCol.close (); }\n L.31: Retrieve a Row with specifying the measuring instrument ID of the key.   Execution results are as follows.\nList.3 Result\nID Name Longitude Latitude Camera 1 Hokkaido-Sapporo 43.06417 141.34694 true 2 Aomori-Aomori 40.82444 140.74 false 3 Iwate-Morioka 39.70361 141.1525 true 4 Miyagi-Sendai 38.26889 140.87194 false 5 Akita-Akita 39.71861 140.1025 true (Snip)\nComplete source code Complete source code used in this sample can be downloaded from the following.\nDownload: collection-retrieve.zip\n"
},
{
	"uri": "http://example.org/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/",
	"title": "GridDB",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://example.org/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]